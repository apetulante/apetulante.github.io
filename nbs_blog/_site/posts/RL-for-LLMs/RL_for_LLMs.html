<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abbie Petulante">
<meta name="dcterms.date" content="2025-02-25">
<meta name="description" content="A guide to reinforcement learning as it applies to modern large language models">

<title>RLHF for LLMs – Abbie’s Jupyter Tutorials</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Abbie’s Jupyter Tutorials</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/apetulante"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/apetulante/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">RLHF for LLMs</h1>
                  <div>
        <div class="description">
          A guide to reinforcement learning as it applies to modern large language models
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">machine-learning</div>
                <div class="quarto-category">reinforcement-learning</div>
                <div class="quarto-category">LLMs</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abbie Petulante </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 25, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><a target="_blank" href="https://colab.research.google.com/github/apetulante/Tutorials/blob/master/Training/RL_for_LLMs.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="reinforcement-learning-for-llms" class="level1">
<h1>Reinforcement Learning for LLM’s</h1>
<blockquote class="blockquote">
<p>In this notebook, we’ll dive deep into how LLM’s are trained using reinforcement learning, exploring how we can implement this type of training on an existing, small model to better guide our desired responses!</p>
</blockquote>
<section id="what-is-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="what-is-reinforcement-learning">What is reinforcement learning?</h3>
<p>In a typical LLM training paradigm, models are first trained in an <strong>self-supervised</strong> manner to predict a masked-out word. This makes them great at understanding how language is constructed, and helps us to make giant datasets very easily.</p>
<p>However, models trained to simply predict what word comes next are not very helpful to talk to. If I ask a model trained in this manner <em>“What kind of tree is a cedar?”</em> It may respond <em>“I would really like to know about cedar trees.”</em> because it thinks it should be continuing the thought.</p>
<p>The next step, then, in LLM training, is typically <strong>instruction fine tuning (IFT)</strong>. In this step, models are trained to respond and follow instructions. In this step, they come to recognize when they are being asked a question, and the language produced thereafter becomes a <em>response</em> not simply a <em>continuation</em>. After IFT, a model might respond: <em>“coniferous”</em></p>
<p>The model now answers the question (hopefully correctly.) But it may not be friendly, or be too terse, or not structure its answers particularly well, or even be mean and harmful. So, the last step in training is typically <strong>reinforcement learning from human feedback (RLHF, or just RL)</strong>. In this step, the model is told what responses of its are better and encouraged to respond in that type of way. While this can adjust the responses in many ways, one example is that it may encourage more information or detail, or generally more wordiness. So, a model trained with RL may respond <em>“A cedar is a type of coniferous tree in the genus Cedrus. Cedars are evergreen trees known for their aromatic wood, needle-like leaves, and cones. They belong to the family Pinaceae.”</em></p>
<blockquote class="blockquote">
<p>📌 <font color="#996565"><strong>Note</strong>: A great textbook on reinforcement learning is <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement Learning: An Introduction</a>. I highly reccommend referencing it if, after or during this tutorial, you find yourself wanting an even deeper dive into some of these concepts.</font></p>
</blockquote>
</section>
</section>
<section id="setup-and-introduction" class="level1">
<h1>0. Setup and Introduction</h1>
<p>Before we get any deeper and start writing code, let’s install our necessary packages and import dependencies:</p>
<section id="installations-and-imports" class="level2">
<h2 class="anchored" data-anchor-id="installations-and-imports">Installations and Imports</h2>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install required packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q <span class="op">--</span>upgrade transformers datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q trl<span class="op">==</span><span class="fl">0.10.1</span> <span class="co">#install downgraded version because it's easier to use!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>(If the below imports fail, you may need to restart the kernel for those installations to take effect).</p>
<div id="cell-7" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic imports</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trl <span class="im">import</span> AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Tuple</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you have access to it, a GPU runtime will make this code run smoother (or might make it possible to run at all!)</p>
<p>The below code will confirm if you’re on a GPU. You want to see <code>CUDA available: True</code></p>
<p>It’s not <em>required</em>, but preferred. A stronger CPU might be required.</p>
<div id="cell-9" class="cell" data-outputid="aa078af0-1fb1-469f-c49a-ffa74f6b3d88">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check PyTorch version and CUDA availability</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"CUDA device: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_name(<span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Available GPU memory: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_properties(<span class="dv">0</span>)<span class="sc">.</span>total_memory <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">3</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 2.5.1+cu124
CUDA available: False</code></pre>
</div>
</div>
<section id="optional-hugging-face-authentication" class="level3">
<h3 class="anchored" data-anchor-id="optional-hugging-face-authentication">(OPTIONAL) Hugging Face Authentication</h3>
<p>Later in this notebook, we’ll be grabbing a model off of huggingface. While the one we use here doesn’t require a token, you can add your token here if you want to experiment with swapping GPT-2 for a different model that needs authentication.</p>
<div id="cell-11" class="cell" data-outputid="2022b73e-bb69-4561-cbf7-3b2294af2c89">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> login</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> getpass</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>token <span class="op">=</span> getpass.getpass(<span class="st">"Enter your Hugging Face token: "</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify login</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Login status: Authenticated with Hugging Face"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Login status: Authenticated with Hugging Face</code></pre>
</div>
</div>
</section>
</section>
<section id="an-introduction-from-traditional-training-to-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="an-introduction-from-traditional-training-to-reinforcement-learning">An Introduction: From Traditional Training to Reinforcement Learning</h2>
<p>Before we get started, let’s talk about what exactly the goals of RL are, how it differs from “traditional” training, and get a basic understanding of the RL pipeline.</p>
<section id="traditional-training-a-quick-review" class="level3">
<h3 class="anchored" data-anchor-id="traditional-training-a-quick-review">Traditional Training: A Quick Review</h3>
<p>In “traditional”, supervised training of a transformer or neural network, which you’re likely familiar with, the process looks like this:</p>
<ol type="1">
<li>You have training data with inputs and known correct outputs</li>
<li>The model makes predictions</li>
<li>You calculate a loss function (like MSE or cross-entropy) that measures how wrong the predictions are</li>
<li>Backpropagation updates the weights to minimize this loss</li>
<li>Repeat until the model gets good at predicting correct outputs</li>
</ol>
<p>The key here is that for every input, you know exactly what the correct output should be.</p>
<p>But what if you don’t know the exact right answer? What if you just know when answers are “better” or “worse”? This is where reinforcement learning comes in.</p>
<p>Consider training an LLM to be helpful and truthful. There’s no single “correct” response to a prompt - there might be many good responses and many bad ones. We can’t use traditional supervised learning because: - We don’t have examples of perfect responses - Multiple very different responses might be equally good - We care about abstract qualities (helpfulness, truthfulness) more than exact word matches</p>
</section>
<section id="enter-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="enter-reinforcement-learning">Enter Reinforcement Learning</h3>
<p>RL approaches this differently:</p>
<ol type="1">
<li>Instead of a loss function that measures “wrongness”, we use a reward function that measures “goodness”</li>
<li>Instead of comparing to correct answers, we try different outputs and see which get higher rewards</li>
<li>Instead of direct supervision, the model learns through trial and error</li>
</ol>
<p>RL requires, at a high level:</p>
<p><strong>1. Generation Phase</strong> - Model receives a prompt - Model generates multiple different possible responses - This is called “exploring” the space of possible outputs</p>
<p><strong>2. Evaluation Phase</strong> - Each generated response gets a reward score - Better responses = higher rewards - This tells us which outputs we want to encourage</p>
<p><strong>3. Learning Phase</strong> - Model is updated to make high-reward outputs more likely - <em>But</em>, it doesn’t memorize specific outputs - Instead, it learns patterns that tend to lead to high rewards</p>
<p><strong>4. Repeat</strong> - Generate new responses - Evaluate them - Learn from the results - Over time, the model gets better at generating high-reward outputs</p>
</section>
<section id="some-key-differences-from-traditional-training" class="level3">
<h3 class="anchored" data-anchor-id="some-key-differences-from-traditional-training">Some Key Differences from Traditional Training</h3>
<p><strong>1. Exploration vs Exploitation</strong> - The model needs to try new things (<strong>explore</strong>) to find better strategies - But it also needs to use what it knows works (<strong>exploit</strong>) - This “exploration-exploitation tradeoff” doesn’t exist in traditional training</p>
<p><strong>2. Delayed Rewards</strong> - Sometimes we don’t know if an output was good until several steps later - The model needs to learn which actions led to good outcomes - This is very different from immediate feedback in traditional training</p>
<p><strong>3. Moving Targets</strong> - As the model improves, it generates different outputs - These new outputs might get different rewards - So, the learning process is more dynamic than traditional fixed-dataset training</p>
</section>
</section>
</section>
<section id="the-reward-function" class="level1">
<h1>1. The Reward Function</h1>
<p>We’ve just talked a lot about a reward function, which is the cornerstone of reinforcement learning. Now, let’s dive into exactly what a reward function looks like and implement one ourselves.</p>
<section id="defining-the-reward-function" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-reward-function">Defining the Reward Function</h2>
<p>So, what is a reward function? In reality, it can take many forms. But in general, a reward function needs to be:</p>
<ul>
<li><strong>Clear</strong>: It should have a well-defined relationship between output quality and some numerical score</li>
<li><strong>Consistent</strong>: Similar outputs should get similar rewards</li>
<li><strong>Meaningful</strong>: Higher rewards should genuinely represent better outputs</li>
<li><strong>Computationally Feasible</strong>: As we need to calculate rewards for many outputs quickly</li>
</ul>
<p>Reward functions can also incorportate <em>negative rewards</em> for behaviors that the model wants to explicitly avoid.</p>
</section>
<section id="a-very-simple-example" class="level2">
<h2 class="anchored" data-anchor-id="a-very-simple-example">A (Very) Simple Example</h2>
<p>Let’s just start with a very basic example to illustrate what a simple reward function <em>could</em> be. Below, we’ll write a “positive sentiment” reward function that counts how many positive and negative words were used in a response, giving positive rewards for positive words, and negative rewards for negative words.</p>
<div id="cell-19" class="cell" data-outputid="7cd3d4c6-9e5b-46ec-f6a6-762590ab0e54">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple reward function</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sentiment_reward(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    positive_words <span class="op">=</span> [<span class="st">'good'</span>, <span class="st">'great'</span>, <span class="st">'excellent'</span>, <span class="st">'wonderful'</span>, <span class="st">'amazing'</span>]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    negative_words <span class="op">=</span> [<span class="st">'bad'</span>, <span class="st">'awful'</span>, <span class="st">'terrible'</span>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> text.lower().split()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    positive_count <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">in</span> positive_words)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    negative_count <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">in</span> negative_words)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> positive_count <span class="op">-</span> negative_count</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Test our reward function with some example sentences</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>test_texts <span class="op">=</span> [</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This is a good and great day"</span>,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Nothing particularily special happened today, but I was still satisfied"</span>,</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later"</span>,</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Good wow great amazing excellent stuff, wow great and good and great"</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Testing reward function:"</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> test_texts:</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reward: </span><span class="sc">{</span>sentiment_reward(text)<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Testing reward function:

Text: This is a good and great day
Reward: 2.00

Text: Nothing particularily special happened today, but I was still satisfied
Reward: 0.00

Text: Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later
Reward: -1.00

Text: Good wow great amazing excellent stuff, wow great and good and great
Reward: 7.00</code></pre>
</div>
</div>
<p>So, we can see that the above, while likely far too simple to actually be useful, counts as a reward function, as it meets our criteria and provides some way to understand “better” responses.</p>
<p>This reward function clearly meets the requirements of: 1. <strong>Clear relationship</strong>: More positive words = higher score 2. <strong>Fast to compute</strong>: Simple word counting is very efficient 3. <strong>Easy to understand</strong>: The logic is straightforward</p>
<p>However, we can point out some clear reasons that this would be an <em>unuseful</em> reward function in practice: 1. <strong>Easy to game</strong>: Model could just repeat positive words 2. <strong>Misses context</strong>: “Not good” counts as positive 3. <strong>Ignores quality</strong>: Well-written neutral text scores lower than poorly written positive text (see second example vs last)</p>
<p>So, a <em>good</em> reward function should take more into account when deciding if a whole response is good or not.</p>
<section id="one-step-better-adding-context" class="level3">
<h3 class="anchored" data-anchor-id="one-step-better-adding-context">One Step Better: Adding Context</h3>
<p>Let’s improve our reward function by considering context. We’ll: 1. Account for negations 2. Consider word positioning 3. Add penalties for repetition</p>
<p>(We’ll also, just for simplicity sake, move this to be positive-detecting only, no negative rewards)</p>
<div id="cell-22" class="cell" data-outputid="a3f7fc0a-c5e0-4eed-b818-ea196aa521f7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Improved reward function with context awareness</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positivity_reward(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We'll keep the same (extremely incomplete) word list</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    positive_words <span class="op">=</span> [<span class="st">'good'</span>, <span class="st">'great'</span>, <span class="st">'excellent'</span>, <span class="st">'wonderful'</span>, <span class="st">'amazing'</span>]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> text.lower().split()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check for negations (looking at pairs of words)</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words)):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> words[i] <span class="kw">in</span> positive_words:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check if previous word is a negation</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> words[i<span class="op">-</span><span class="dv">1</span>] <span class="kw">in</span> {<span class="st">'not'</span>, <span class="st">'never'</span>, <span class="st">"don't"</span>, <span class="st">'no'</span>}:</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>                score <span class="op">-=</span> <span class="fl">0.5</span>  <span class="co"># Penalty for negated positive words</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>                score <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Penalty for repetition</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    unique_words <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(words))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    repetition_penalty <span class="op">=</span> unique_words <span class="op">/</span> <span class="bu">max</span>(<span class="bu">len</span>(words), <span class="dv">1</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate final score with penalties</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    final_score <span class="op">=</span> (score <span class="op">/</span> <span class="bu">max</span>(<span class="bu">len</span>(words), <span class="dv">1</span>)) <span class="op">*</span> repetition_penalty</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Clip to range [0, 1]</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(<span class="bu">min</span>(final_score, <span class="fl">1.0</span>), <span class="fl">0.0</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the improved function</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>test_texts <span class="op">=</span> [</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This is good and helpful."</span>,</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This is not good at all."</span>,</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">"good good good good good"</span>,</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The explanation was clear and helpful, making it incredibly beneficial."</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing our improved reward function:"</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> test_texts:</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    reward <span class="op">=</span> positivity_reward(text)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reward: </span><span class="sc">{</span>reward<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Testing our improved reward function:

Text: This is good and helpful.
Reward: 0.200

Text: This is not good at all.
Reward: 0.000

Text: good good good good good
Reward: 0.200

Text: The explanation was clear and helpful, making it incredibly beneficial.
Reward: 0.000</code></pre>
</div>
</div>
<p>So, we’ve definitely improved things! But you can see how easy pitfalls are. The last example was definitely positive in tone, but that’s not accounted for here. We could add “clear”, “helpful” and “beneficial” to our positive words list, but then how many words would need to be added just because sometimes, in the right context, they are positive? Perhaps “incredibly” is the thing giving this sentence a positive connotation, but then that could also be used to say something is “incredibly” negative.</p>
<p>You can see how reward functions we use in practice require some careful considerations.</p>
</section>
</section>
<section id="real-world-reward-functions" class="level2">
<h2 class="anchored" data-anchor-id="real-world-reward-functions">Real-World Reward Functions</h2>
<p>In practice, good reward functions for LLMs often combine multiple components, as there are a lot of factors to consider when deeming a response as “good”:</p>
<p><strong>1. Quality Metrics</strong> measure for things like grammar and fluency, relevance of the response to the given prompt, factual accuracy, and overall coherence and structure.</p>
<p><strong>2. Task-Specific Metrics</strong> measure for things like format adherence, style matching, any domain-specific requirements, and length constraints</p>
<p><strong>3. Safety and Alignment Metrics</strong> include toxicity detection and bias measurements, and check for things like truthfulness and helpfulness.</p>
<section id="a-more-practical-reward-function" class="level3">
<h3 class="anchored" data-anchor-id="a-more-practical-reward-function">A More Practical Reward Function</h3>
<p>Let’s see how we can implement <strong>some</strong> of those metrics above and build a (still very basic, but more realistic) reward function that would be more useful in practice than our “positive words” detection.</p>
<div id="cell-26" class="cell" data-outputid="8fdc1291-24ca-464d-f199-fbd3b1c02419">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_rewards(prompt: <span class="bu">str</span>, response: <span class="bu">str</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, <span class="bu">float</span>]:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate multiple reward components for a given response.</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns a dictionary of different reward aspects.</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> {}</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Response Length Reward</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encourage responses between 50 and 500 words</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> response.split()</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    word_count <span class="op">=</span> <span class="bu">len</span>(words)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    length_reward <span class="op">=</span> <span class="bu">min</span>(<span class="fl">1.0</span>, word_count <span class="op">/</span> <span class="fl">50.0</span>) <span class="cf">if</span> word_count <span class="op">&lt;</span> <span class="dv">50</span> <span class="cf">else</span> <span class="op">\</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>                   <span class="fl">1.0</span> <span class="cf">if</span> <span class="dv">50</span> <span class="op">&lt;=</span> word_count <span class="op">&lt;=</span> <span class="dv">500</span> <span class="cf">else</span> <span class="op">\</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>                   <span class="bu">max</span>(<span class="fl">0.0</span>, <span class="fl">1.0</span> <span class="op">-</span> (word_count <span class="op">-</span> <span class="dv">500</span>) <span class="op">/</span> <span class="dv">500</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    rewards[<span class="st">'length'</span>] <span class="op">=</span> length_reward</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Prompt Relevance Reward</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if response uses key terms from prompt</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    prompt_words <span class="op">=</span> <span class="bu">set</span>(prompt.lower().split())</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    response_words <span class="op">=</span> <span class="bu">set</span>(response.lower().split())</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    overlap <span class="op">=</span> <span class="bu">len</span>(prompt_words.intersection(response_words))</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    relevance_reward <span class="op">=</span> <span class="bu">min</span>(<span class="fl">1.0</span>, overlap <span class="op">/</span> <span class="bu">max</span>(<span class="bu">len</span>(prompt_words), <span class="dv">1</span>))</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    rewards[<span class="st">'relevance'</span>] <span class="op">=</span> relevance_reward</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Format Quality Reward</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check for good formatting practices</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    format_scores <span class="op">=</span> []</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Has paragraphs</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    format_scores.append(<span class="fl">1.0</span> <span class="cf">if</span> response.count(<span class="st">'</span><span class="ch">\n\n</span><span class="st">'</span>) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Uses punctuation properly</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    format_scores.append(<span class="fl">1.0</span> <span class="cf">if</span> re.search(<span class="vs">r'[.!?]\s+[A-Z]'</span>, response) <span class="cf">else</span> <span class="fl">0.0</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Proper capitalization</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    format_scores.append(<span class="fl">1.0</span> <span class="cf">if</span> re.search(<span class="vs">r'^[A-Z]'</span>, response) <span class="cf">else</span> <span class="fl">0.0</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    rewards[<span class="st">'formatting'</span>] <span class="op">=</span> <span class="bu">sum</span>(format_scores) <span class="op">/</span> <span class="bu">len</span>(format_scores)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Calculate Final Combined Reward</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> {<span class="st">'length'</span>: <span class="fl">0.2</span>, <span class="st">'relevance'</span>: <span class="fl">0.5</span>, <span class="st">'formatting'</span>: <span class="fl">0.3</span>}</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    final_reward <span class="op">=</span> <span class="bu">sum</span>(rewards[k] <span class="op">*</span> weights[k] <span class="cf">for</span> k <span class="kw">in</span> weights)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    rewards[<span class="st">'final'</span>] <span class="op">=</span> final_reward</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rewards</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's test our practical reward function</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>test_cases <span class="op">=</span> [</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">"prompt"</span>: <span class="st">"Explain how photosynthesis works."</span>,</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"response"</span>: <span class="st">"""</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="st">Photosynthesis is the process by which plants convert sunlight into energy.</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a><span class="st">The process involves chlorophyll in the leaves capturing sunlight. This energy</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="st">is used to convert water and carbon dioxide into glucose and oxygen.</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a><span class="st">Plants use glucose for energy and release oxygen as a byproduct.</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>        <span class="st">"prompt"</span>: <span class="st">"Explain how photosynthesis works."</span>,</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>        <span class="st">"response"</span>: <span class="st">"plants make food from sun"</span></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> case <span class="kw">in</span> test_cases:</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> calculate_rewards(case[<span class="st">"prompt"</span>], case[<span class="st">"response"</span>])</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Prompt: </span><span class="sc">{</span>case[<span class="st">'prompt'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Response: </span><span class="sc">{</span>case[<span class="st">'response'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Rewards:"</span>)</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> aspect, score <span class="kw">in</span> rewards.items():</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>aspect<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Prompt: Explain how photosynthesis works.
Response: 
Photosynthesis is the process by which plants convert sunlight into energy.

The process involves chlorophyll in the leaves capturing sunlight. This energy
is used to convert water and carbon dioxide into glucose and oxygen.

Plants use glucose for energy and release oxygen as a byproduct.


Rewards:
length: 0.900
relevance: 0.250
formatting: 0.667
final: 0.505

Prompt: Explain how photosynthesis works.
Response: plants make food from sun

Rewards:
length: 0.100
relevance: 0.000
formatting: 0.000
final: 0.020</code></pre>
</div>
</div>
</section>
</section>
<section id="key-takeaways-about-reward-functions" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways-about-reward-functions">Key Takeaways About Reward Functions</h2>
<p>So, when making practical reward functions, there are multiple ways to make a reward function better, including (but not necessarily limited to):</p>
<ul>
<li>adding multiple reward components help avoid gaming the system</li>
<li>using weighted combinations allow prioritizing different target metrics</li>
<li>defining clear relationships between quality and reward score</li>
</ul>
<p>Some very common pitfalls to keep in mind: - <em>Reward Hacking:</em> some rewards are easily maximized by doing something very simple, not actually increasing quality - <em>Unclear Signaling:</em> it can be difficult to make rewards that very clearly, concretely determine “goodness” - <em>Computational Overhead:</em> Complex rewards can obviously be helpful in targeting rich, high-quality results, BUT can slow down training significantly - <em>Inconsistent Scaling:</em> Be sure to normalize and weight any scores that are combined appropriately.</p>
</section>
</section>
<section id="proximal-policy-optimization-ppo" class="level1">
<h1>2. Proximal Policy Optimization (PPO)</h1>
<p>So, we’ve defined a reward function that we want to use to inform the model how to answer. But how do we use it?</p>
<p>The simplest thing we could think to do might be to just turn that somehow into a loss to minimize and give examples to our model, like traditional training.</p>
<p>But this wouldn’t work! A lot more care and nuance is needed for something like reinforcement learning.</p>
<section id="the-challenge-with-rl-for-llms" class="level3">
<h3 class="anchored" data-anchor-id="the-challenge-with-rl-for-llms">The Challenge with RL for LLMs</h3>
<p>When training an LLM with RL, we face several unique challenges that mean we must make our updates carefully.</p>
<ol type="1">
<li><p>Catastrophic Updates: If we make too large a change to the model based on rewards, we risk losing the model’s existing knowledge or ability to generate coherent text, and can easily get stuck in local optima (like repeating high-reward words)</p></li>
<li><p>Discrete Actions: During text generation, an LLM model repeatedly chooses from a vocabulary of discrete tokens. As such, a single “action” (generating text) involves many individual, discrete actions (choosing the next token), making it harder to know which “choices” led to good or bad outcomes.</p></li>
</ol>
</section>
<section id="enter-ppo" class="level3">
<h3 class="anchored" data-anchor-id="enter-ppo">Enter PPO</h3>
<p>PPO (Proximal Policy Optimization) is a technique for training the model in a way that moves towards maximizing our reward, but slowly and carefully.</p>
<p>In PPO, a ‘policy’ refers to the model’s strategy for generating outputs—in an LLM’s case, how it predicts and constructs sentences token by token (basically, the model itself). An <strong>‘old policy’</strong> is the model as it currently functions, and a <strong>‘new policy’</strong> is an updated version that tries to generate outputs more aligned with the reward signal. PPO carefully balances exploration (trying new strategies) with stability by limiting how far the new policy is allowed to deviate from the old one. This ensures gradual, meaningful improvements without destabilizing the models ability to generate coherent text.</p>
</section>
<section id="key-components-of-ppo" class="level2">
<h2 class="anchored" data-anchor-id="key-components-of-ppo">Key Components of PPO</h2>
<ol type="1">
<li><strong>The Policy (π):</strong> In our case, this is our language model. It has two key behaviors:</li>
</ol>
<ul>
<li>Acting: Generating text given a prompt</li>
<li>Calculating Probabilities: Telling us how likely it would be to generate specific text</li>
</ul>
<ol start="2" type="1">
<li><strong>The Clipped Objective:</strong> This is PPO’s real metric for improvement. It includes:</li>
</ol>
<ul>
<li>Probability Ratio (r): How much more/less likely is the new policy to generate certain text compared to the old policy?</li>
<li>Clipping: We put boundaries on this ratio (typically between 0.8 and 1.2)</li>
<li>Advantage (A): How much better/worse was the outcome than expected?</li>
</ul>
<p>For LLM’s, PPO is implemented on a <strong>token-by-token basis</strong>, so each token gets the opportunity to be better or worse for the outcome. In this way, PPO is not exactly a loss function but uses a surrogate objective (a type of loss function) to guide updates. It incorporates the reward by comparing the new policy’s performance (via probabilities of actions, i.e., token predictions) to the old policy’s performance, scaled by the reward.</p>
</section>
<section id="the-ppo-mathematically" class="level2">
<h2 class="anchored" data-anchor-id="the-ppo-mathematically">The PPO Mathematically</h2>
<p>Of course, how the PPO is implemented in practice is through that clipped objective function.</p>
<p>The PPO objective function <span class="math inline">\((L^{CLIP})\)</span> is defined as: <span class="math display">\[
L^{CLIP}(\theta) = E[  min(r_t(\theta)A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\theta\)</span> represents the policy parameters (our LLM weights)</li>
<li><span class="math inline">\(r_t(\theta)\)</span> is the probability ratio, <span class="math inline">\(\pi_{\theta}(a_t|s_t)\)</span> / <span class="math inline">\(\pi_{\theta_{old}}(a_t|s_t)\)</span></li>
<li><span class="math inline">\(A_t\)</span> is the advantage</li>
<li><span class="math inline">\(\epsilon\)</span> is the clipping parameter (typically 0.2)</li>
</ul>
<hr>
<ol type="1">
<li>For an LLM, the policy <span class="math inline">\(\pi_{\theta}(a_t|s_t)\)</span> represents the probability of choosing token <span class="math inline">\(a_t\)</span> given context <span class="math inline">\(s_t\)</span>:</li>
</ol>
<ul>
<li><span class="math inline">\(s_t\)</span> is the current context (prompt + generated tokens so far)</li>
<li><span class="math inline">\(a_t\)</span> is the next token to generate</li>
</ul>
<ol start="2" type="1">
<li><p>The probability ratio, <span class="math inline">\(r_t(\theta) = \pi_{\theta}(a_t|s_t)\)</span> / <span class="math inline">\(\pi_{\theta_{old}}(a_t|s_t)\)</span> is the ratio in probability of the new policy (<span class="math inline">\(\theta\)</span>) choosing token <span class="math inline">\(a_t\)</span> over the probability from the old policy (<span class="math inline">\(\theta_{old}\)</span>) choosing token <span class="math inline">\(a_t\)</span>. (i.e <span class="math inline">\(r_t(\theta)\)</span> = 2 means it’s twice as likely to choose <span class="math inline">\(a_t\)</span> in the new policy)</p></li>
<li><p>The clipping function, <span class="math inline">\(clip(r_t(\theta), 1-\epsilon, 1+\epsilon)\)</span> ensures this probability ratio is reasonably bounded, keeping updates small.</p></li>
<li><p>Then for each token, we calculate both the unclipped objective: <span class="math inline">\(r_t(θ)A_t\)</span> and the clipped objective <span class="math inline">\(clip(r_t(θ), 1-ε, 1+ε)A_t\)</span>, and take the minimum of both, again ensuring smaller, more stable updates.</p></li>
<li><p>Where (<span class="math inline">\(A(t)\)</span>) is the “advantage” (<span class="math inline">\(A(t)\)</span>) is a measurement of how much better or worse an action was compared to what we expected, it <em>incorporates</em>, but isn’t exactly the reward…</p></li>
</ol>
<section id="the-advantage-a_t" class="level3">
<h3 class="anchored" data-anchor-id="the-advantage-a_t">…The “Advantage” <span class="math inline">\((A_t)\)</span></h3>
<p>Ok, so we spent all of that time talking about the reward, just for it to end up wrapped in something called the “advantage”, so let’s break down the advantage a bit more.</p>
<p>Advantage <span class="math inline">\((A_t)\)</span> measures how much better or worse an action was compared to what we expected. It’s calculated as: <span class="math display">\[A_t = R_t - V(s_t)\]</span></p>
<p>where: - <span class="math inline">\(R_t\)</span> is the actual reward received - <span class="math inline">\(V(s_t)\)</span> is the expected value (what we thought we’d get)</p>
<p>In the simplest example, imagine you’re a language model deciding what word to generate:</p>
<ol type="1">
<li><p>You generate “excellent” with an expected reward of 0.5. Then, for an actual reward of 0.8: <span class="math display">\[Advantage = 0.8 - 0.5 = +0.3\]</span> This was better than expected.</p></li>
<li><p>You generate “okay” with an expected reward of 0.5. Then, for an actual reward of 0.3: <span class="math display">\[Advantage = 0.3 - 0.5 = -0.2\]</span> Negative advantage means this was worse than expected.</p></li>
</ol>
<p>Advantage is used over raw reward because it allows harder prompts to expect harder results. For example, easy prompts could be very easy to get a reward of 0.8, but hard prompts hardly ever get even 0.4. Hard vs easy prompts having their own expected reward can accommodate this difference in their “typical” reward.</p>
<p>Ok, so reward for a given word, we’ve covered. But <em>expected</em> reward, what we “thought we’d get”… you might have guessed that that’s a bit more complicated.</p>
</section>
<section id="the-expected-value-vs_t" class="level3">
<h3 class="anchored" data-anchor-id="the-expected-value-vs_t">The “Expected Value” (<span class="math inline">\(V(s_t)\)</span>)</h3>
<p>The expected value is what you <em>think</em> your reward should be. This is naturally difficult to define, so you can really get this in a lot of ways. Some very simple ways could be: - defining different expected values for different categories of prompts. i.e:</p>
<pre><code>expected_values = {
    "math": 0.6,        # Math problems are harder
    "greeting": 0.9,    # Greetings are easy
    "general": 0.7      # General conversation
}
expected_value = expected_values[get_prompt_type(prompt)]</code></pre>
<ul>
<li>keeping track of rewards you’ve already given different types of prompts:</li>
</ul>
<pre><code>rewards_history = {
    "math_questions": [0.6, 0.7, 0.5, 0.8],  # Previous rewards for math
    "greeting": [0.9, 0.95, 0.85, 0.9]       # Previous rewards for greetings
}
expected_value = average(rewards_history[prompt_type])</code></pre>
<p>But in practice, calculating the expected reward tends to be more sophisticated. It can be done with:</p>
<ul>
<li><p><strong>Value Networks</strong>: these are neural networks who are trained to predict rewards. It can handle more nuanced contexts about the prompts, and more easily adapt to things like prompt lengths, topics covered, and depth of explanation asked for.</p></li>
<li><p><strong>Temporal Differences</strong>: A way to consider future rewards, not just immediate ones. Instead of just looking at the current reward (R), we also consider what rewards we expect in the future (<span class="math inline">\(\lambda\)</span> * <span class="math inline">\(V_{next}\)</span>, where <span class="math inline">\(\lambda\)</span> is a discount factor). This helps when your early actions lead to better outcomes later.</p></li>
<li><p><strong>Generalized Advantage Estimation (GAE)</strong>: A method that balances between immediate and future rewards when calculating advantage. It uses a weighted average of rewards over different time spans, helping to reduce variance in our advantage estimates while still maintaining useful learning signals.</p></li>
</ul>
<p>In this notebook, we’ll stick to some of these very simple expected value definitions. But it’s important to know that this is a choice to be made in any RL implementation! And, that <strong>the value function used is also something that can be updated</strong>. So, during training, a value function can <em>learn</em> to generate better expected values: we can use loss functions to update <em>both</em> <span class="math inline">\(\pi_\theta\)</span> and (<span class="math inline">\(V(s_t)\)</span>). We’ll discuss this in more detail below!</p>
<p><a id="expectation_operation"></a></p>
</section>
<section id="the-expectation-operation" class="level3">
<h3 class="anchored" data-anchor-id="the-expectation-operation">The Expectation Operation</h3>
<p>I promise there will soon be code. But there’s one more thing to clear up. The PPO objective function <span class="math inline">\((L^{CLIP})\)</span>: <span class="math display">\[
L^{CLIP}(\theta) = E[  min(r_t(\theta)A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]
\]</span></p>
<p>has that “<span class="math inline">\(E\)</span>” - what is that?</p>
<p>This is the expectation operation. That’s mathematically simple enough - it’s really just a weighted average: the mean of the possible values that a variable can take. That’s all!</p>
<p>But what are “all possible values”? For LLM’s, this is a <strong>sample of all possible experiences (trajectories)</strong> collected from the policy. So really, we average over a <em>sample</em> of all token paths that you could have generated in response to all prompt examples. Why a sample? For a bit more detailed explanation, see <a href="#A_1">Appendix 1</a>.</p>
<p>We’ll touch on this again in a bit, but for now, let’s jump into an example.</p>
</section>
</section>
<section id="a-simple-ppo-example" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-ppo-example">A Simple PPO Example</h2>
<p>Let’s see an actual implementation of the PPO calculation, to observe how, in practice, this is calculated over trajectories that an LLM could generate.</p>
<section id="for-a-single-token" class="level3">
<h3 class="anchored" data-anchor-id="for-a-single-token">For a Single Token</h3>
<p>Below, we’ll see how the PPO function would calculate its objective for a single new potential token.</p>
Consider in this example that we have generated so far:
<h5 class="anchored">
<center>
“I am feeling great and happy today, it’s”
</center>
</h5>
And we are tasked with determining if the new word that we could generate:
<h5 class="anchored">
<center>
“excellent”
</center></h5>

<p>is advantageous or not.</p>
<p>The current policy (model) generates “excellent” with probability 0.3, and the new proposed policy we’re evaluating generates “excellent” with probability 0.6.</p>
<p>Below, we’ll calculate the PPO Objective <span class="math inline">\((L^{CLIP})\)</span> that would result from the production of that token. We’ll use our super simple <code>sentiment_reward</code> function that we defined above to determine our reward.</p>
<div id="cell-38" class="cell" data-outputid="e69fe142-fe88-4276-ce33-46802eed1ab7">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppo_update(old_prob, new_prob, advantage, epsilon<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the probability ratio</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    prob_ratio <span class="op">=</span> new_prob <span class="op">/</span> old_prob</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the unclipped and clipped objectives</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    unclipped_objective <span class="op">=</span> prob_ratio <span class="op">*</span> advantage</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    clipped_objective <span class="op">=</span> np.clip(prob_ratio, <span class="dv">1</span> <span class="op">-</span> epsilon, <span class="dv">1</span> <span class="op">+</span> epsilon) <span class="op">*</span> advantage</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PPO's objective is the minimum of the unclipped and clipped objectives</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    ppo_objective <span class="op">=</span> <span class="bu">min</span>(unclipped_objective, clipped_objective)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ppo_objective</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># new word to generate that we are testing the update for</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>new_word <span class="op">=</span> <span class="st">"excellent"</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's say, the old policy generated this word with 30% chance, and the new one generates it with 60% chance</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>old_prob <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>new_prob <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated response from the agent so far</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> <span class="st">"I am feeling great and happy today, it's"</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the reward based on our very simple positive word reward from before</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co"># this calculates over the full current response with this new next token</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>reward <span class="op">=</span> sentiment_reward(response <span class="op">+</span> new_word)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's say in this case, the expected behavior is that the new word is neutral,</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="co"># so we expect it to not *add* any reward over what we had</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>expected_reward <span class="op">=</span> sentiment_reward(response)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the advantage that this new word has given us</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>advantage <span class="op">=</span> reward <span class="op">-</span> expected_reward</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform PPO update</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>ppo_objective <span class="op">=</span> ppo_update(old_prob, new_prob, advantage)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Response: </span><span class="sc">{</span>response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Positivity Reward: </span><span class="sc">{</span>reward<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Advantage: </span><span class="sc">{</span>advantage<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PPO Objective: </span><span class="sc">{</span>ppo_objective<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Response: I am feeling great and happy today, it's
Positivity Reward: 1
Advantage: 0
PPO Objective: 0.0</code></pre>
</div>
</div>
<p>We can see here, that adding excellent resulted in <strong>positive</strong> advantage: it was <strong>good</strong> to our reward function to add that word.</p>
<p>We can also see when we step through the math, that our objective got clipped: Our probability ratio was 2.0, so: - unclipped objective was <span class="math inline">\(2.0 * 1.0 = 2.0\)</span>. - clipped objective (what’s used) was <span class="math inline">\(1.1 * 1.0 = 1.1\)</span></p>
<p>This is a key action of PPO: ensuring that policy updates remain relatively small.</p>
<blockquote class="blockquote">
<p>📌 <font color="#996565"><strong>Note</strong>: you may be able to see here, that if our advantage term had been large, even with clipping, the update can be substantial, potentially leading to instability. To mitigate this, it’s common practice to normalize or scale rewards, thereby controlling the magnitude of the advantage. This normalization helps maintain stable and consistent updates.</font></p>
</blockquote>
</section>
<section id="simple-ppo-example-over-a-trajectory" class="level3">
<h3 class="anchored" data-anchor-id="simple-ppo-example-over-a-trajectory">Simple PPO Example Over a Trajectory</h3>
<p>In practice, any given “response” which is a series of token selections is a <strong>trajectory</strong> that the model could have generated.</p>
For each word in:
<h5 class="anchored">
<center>
“I am feeling great and happy today, it’s”
</center>
</h5>
<p>there is an associated old probability, new probability, and reward for generating that token.</p>
<p>We can calculate <span class="math inline">\(L_{CLIP(\text{trajectory})}\)</span> over a given trajectory as: <span class="math display">\[
L_{CLIP(\text{trajectory})} =  \sum_t^T L_{CLIP(t)}
\]</span></p>
<p>for a sequence of length T, comprised of tokens t</p>
<div id="cell-41" class="cell" data-outputid="a808d2eb-1fb3-4f40-d877-27855dac6eb8">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for a given trajectory of tokens, each generated with some probability</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>response_tokens <span class="op">=</span> [<span class="st">"I"</span>, <span class="st">"am"</span>, <span class="st">"feeling"</span>, <span class="st">"great"</span>, <span class="st">"and"</span>, <span class="st">"happy"</span>, <span class="st">"today,"</span>, <span class="st">"it's"</span>, <span class="st">"excellent"</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>old_probs <span class="op">=</span> [<span class="fl">.9</span>, <span class="fl">.95</span>, <span class="fl">.4</span>, <span class="fl">.25</span>, <span class="fl">.33</span>, <span class="fl">.45</span>, <span class="fl">.4</span>, <span class="fl">.15</span>, <span class="fl">.3</span>] <span class="co"># dummy probabilities for each word in the sequence, in old policy</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>new_probs <span class="op">=</span> [<span class="fl">.9</span>, <span class="fl">.95</span>, <span class="fl">.6</span>, <span class="fl">.55</span>, <span class="fl">.5</span>, <span class="fl">.65</span>, <span class="fl">.3</span>, <span class="fl">.2</span>, <span class="fl">.6</span>] <span class="co"># dummy probabilities for each word in the sequence, in new policy</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>expected_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute PPO objectives for each token</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>ppo_objectives <span class="op">=</span> []</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, token <span class="kw">in</span> <span class="bu">enumerate</span>(response_tokens):</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    reward <span class="op">=</span> sentiment_reward(token)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    advantage <span class="op">=</span> reward <span class="op">-</span> expected_reward  <span class="co"># Advantage function</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    ppo_obj <span class="op">=</span> ppo_update(old_probs[i], new_probs[i], advantage)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    ppo_objectives.append(ppo_obj)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token: </span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reward: </span><span class="sc">{</span>reward<span class="sc">:.2f}</span><span class="ss">, Advantage: </span><span class="sc">{</span>advantage<span class="sc">:.2f}</span><span class="ss">, PPO Objective: </span><span class="sc">{</span>ppo_obj<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Trajectory PPO objective (sum over tokens in this trajectory)</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>traj_ppo_objective <span class="op">=</span> <span class="bu">sum</span>(ppo_objectives)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total PPO Objective for this trajectory: </span><span class="sc">{</span>traj_ppo_objective<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Token: I
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: am
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: feeling
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: great
Reward: 1.00, Advantage: 1.00, PPO Objective: 1.10

Token: and
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: happy
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: today,
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: it's
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: excellent
Reward: 1.00, Advantage: 1.00, PPO Objective: 1.10

Total PPO Objective for this trajectory: 2.20</code></pre>
</div>
</div>
</section>
<section id="simple-ppo-example-over-multiple-trajectories" class="level3">
<h3 class="anchored" data-anchor-id="simple-ppo-example-over-multiple-trajectories">Simple PPO Example Over Multiple Trajectories</h3>
<p>So, the last step here is to consider the multiple trajectories that a model could have reasonably taken during genration, of which we sample some number.</p>
<p>We can calculate our final <span class="math inline">\(L_{CLIP(\text{total})}\)</span> as: <span class="math display">\[
L_{CLIP(\text{total})} =  \frac{1}{N} * \sum_i^N L_{CLIP(i)}
\]</span></p>
<p>over all N trajectories we sampled.</p>
<p>So, let’s consider in this case, we sampled a second trajectory which we want to consider in our final <span class="math inline">\(L_{CLIP(\text{total})}\)</span> calculation.</p>
<div id="cell-43" class="cell" data-outputid="ac31d727-054c-4c3e-abac-51526b541cfd">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for a different trajectory of tokens, each generated with some probability</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>response_2_tokens <span class="op">=</span> [<span class="st">"I"</span>, <span class="st">"am"</span>, <span class="st">"angry"</span>, <span class="st">"it's"</span>, <span class="st">"awful"</span>]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>old_probs <span class="op">=</span> [<span class="fl">.9</span>, <span class="fl">.95</span>, <span class="fl">.3</span>, <span class="fl">.25</span>, <span class="fl">.2</span>] <span class="co"># dummy probabilities for each word in the sequence, in old policy</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>new_probs <span class="op">=</span> [<span class="fl">.9</span>, <span class="fl">.95</span>, <span class="fl">.66</span>, <span class="fl">.55</span>, <span class="fl">.75</span>] <span class="co"># dummy probabilities for each word in the sequence, in new policy</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>expected_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute PPO objectives for each token</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>ppo_objectives <span class="op">=</span> []</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, token <span class="kw">in</span> <span class="bu">enumerate</span>(response_2_tokens):</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    reward <span class="op">=</span> sentiment_reward(token)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    advantage <span class="op">=</span> reward <span class="op">-</span> expected_reward  <span class="co"># Advantage function</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    ppo_obj <span class="op">=</span> ppo_update(old_probs[i], new_probs[i], advantage)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    ppo_objectives.append(ppo_obj)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token: </span><span class="sc">{</span>token<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reward: </span><span class="sc">{</span>reward<span class="sc">:.2f}</span><span class="ss">, Advantage: </span><span class="sc">{</span>advantage<span class="sc">:.2f}</span><span class="ss">, PPO Objective: </span><span class="sc">{</span>ppo_obj<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Trajectory PPO objective (sum over tokens in this trajectory)</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>traj_ppo_objective2 <span class="op">=</span> <span class="bu">sum</span>(ppo_objectives)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total PPO Objective for this trajectory: </span><span class="sc">{</span>traj_ppo_objective2<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Final PPO objective (average over both trajectories)</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>total_ppo_objective <span class="op">=</span> np.mean([traj_ppo_objective, traj_ppo_objective2])</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\x1b</span><span class="st">[0;33;35m'</span> <span class="op">+</span> <span class="ss">f"Total PPO Objective for the full response: </span><span class="sc">{</span>total_ppo_objective<span class="sc">:.2f}</span><span class="ss">"</span> <span class="op">+</span> <span class="st">'</span><span class="ch">\x1b</span><span class="st">[0m'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Token: I
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: am
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: angry
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: it's
Reward: 0.00, Advantage: 0.00, PPO Objective: 0.00

Token: awful
Reward: -1.00, Advantage: -1.00, PPO Objective: -3.75

Total PPO Objective for this trajectory: -3.75

Total PPO Objective for the full response: -0.77</code></pre>
</div>
</div>
<p><a id="ppo_example"></a></p>
<p>So, our total PPO objective is calculated over <em>all</em> potential trajectories that we sampled. In this case, that was just 2. And we can see that, even though our first trajectory seemed to look good and aligned with the behavior that we wanted, we also had a trajectory that was <strong>bad</strong>. This is why it’s important to take a sample of possible trajectories that the model could produce!</p>
<blockquote class="blockquote">
<p>📌 <font color="#996565"><strong>Note</strong>: You might also notice here that our second response got a strongly negative PPO objective. Negative rewards, leading to negative advantages, mean that the clipping <em>won’t do anything</em>. Since we always take the <strong>minimum</strong> of the clipped and unclipped objective, <code>advantage * unclipped probability</code> will always be <code>&lt;= advantage * clipped probability</code>. See the note in <a href="#A_2">Appendix 2</a> about this for more details about how negative rewards are used in practice! For now, we’ll roll with it.)</font></p>
</blockquote>
</section>
</section>
<section id="kl-divergence-in-rlhf" class="level2">
<h2 class="anchored" data-anchor-id="kl-divergence-in-rlhf">KL Divergence in RLHF</h2>
<p>One more concept to touch on is something called the <strong>Kullback-Leibler (KL) Divergence</strong>. When fine-tuning LLMs with reinforcement learning, we want to improve the model’s behavior while preventing it from deviating too drastically from its original training.</p>
<p>KL divergence measures how much one probability distribution differs from another. In the context of LLMs, it quantifies the difference between the token probability distributions of two models - typically our current policy model and a reference model.</p>
<p>Mathematically, for two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, KL divergence is defined as:</p>
<p><span class="math display">\[
D_{KL}(P || Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
\]</span></p>
<p>For language models, this becomes: <span class="math display">\[
D_{KL}(\pi_{\text{new}} || \pi_{\text{ref}}) = \sum_{t} \pi_{\text{new}}(t|c) \log\frac{\pi_{\text{new}}(t|c)}{\pi_{\text{ref}}(t|c)}
\]</span> Where: - <span class="math inline">\(\pi_{\text{new}}\)</span> is our current policy model - <span class="math inline">\(\pi_{\text{ref}}\)</span> is the reference model (usually the initial model before RL training) - <span class="math inline">\(t\)</span> represents tokens, generated given - <span class="math inline">\(c\)</span> the context/prompt</p>
<p>Frequently, KL divergence is included as an explicit penalty in our reward function, preventing the model from:</p>
<ol type="1">
<li>Forgetting its pre-trained knowledge</li>
<li>Adopting degenerate patterns to maximize reward</li>
<li>Straying too far from human-like text generation</li>
</ol>
<p>In practice, we typically add a KL penalty term to our reward: <span class="math display">\[
r_{\text{total}} = r_{\text{original}} - \beta \cdot D_{KL}(\pi_{\text{new}} || \pi_{\text{ref}})
\]</span></p>
<p>Where <span class="math inline">\(\beta\)</span> controls the strength of the penalty. This creates a balance between optimizing for rewards and maintaining the model’s original behavior.</p>
<p><a id="ppo_note"></a> ## 📌 One Final Note on PPO</p>
<p>You should now understand how the PPO is calculated and implemented fairly well. And while PPO <strong>is</strong> a very popular algorithm for reinforcement learning, and what we’ll go on to use in this notebook, you should know that <em>it’s far from the only one</em>. There are many other choices of update strategy that can be used, and while it’s out of the scope of this notebook to go into all of them, I’ll leave a few links here for some other popular choices, that you may be interested to look into now that you know about PPO.</p>
<p><a href="https://arxiv.org/pdf/1502.05477">Trust Region Policy Optimization (TRPO)</a></p>
<p><a href="https://arxiv.org/pdf/2305.18290">Direct Preference Optimization (DPO)</a></p>
</section>
</section>
<section id="building-an-rl-pipeline" class="level1">
<h1>3. Building an RL Pipeline</h1>
<p>Now that we have all the background, we can get to adding reinforcement learning into a real training pipeline of an LLM!</p>
<p>RLHF works as an iterative process, where the LLM is trained not just to predict the next token but to optimize for desired behavior based on reward signals.</p>
<p>The high-level steps:</p>
<ol type="1">
<li>Response Generation → The model generates multiple responses to a prompt.</li>
<li>Reward Assignment → Responses are evaluated (human or automated).</li>
<li>PPO Training → The model updates itself to favor high-reward responses.</li>
<li>Iterate → Repeat the process across more data and updates.</li>
</ol>
<section id="how-ppo-updates-a-model-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="how-ppo-updates-a-model-in-practice">How PPO Updates a Model in Practice</h2>
<p>So, we know how the PPO Objective is calculated. But how exactly is that implemented during training to update the weights of the model and change its behavior?</p>
<p>Well, it’s simple! The PPO objective is <strong>directly applied as the loss function</strong>, so gradient descent directly optimizes this function by computing</p>
<p><span class="math display">\[
\frac{\partial L_{CLIP}}{\partial\theta}
\]</span></p>
<p>during backpropogation to update the model’s weights.</p>
<p><a id="defining_policy"></a></p>
<section id="what-defines-the-old-and-new-policy" class="level3">
<h3 class="anchored" data-anchor-id="what-defines-the-old-and-new-policy">What Defines the Old and New Policy?</h3>
<p>As we already discussed, PPO relies on comparing an old policy with a new policy and determining if the new policy is favorable.</p>
<p>The old policy (<span class="math inline">\(\pi_{\theta(old)}\)</span>) is the model before applying the PPO update. The new policy (<span class="math inline">\(\pi_\theta\)</span>) is the model after we’ve updated it using PPO.</p>
<p>But you might be asking, how do we <em>get</em> old and new policies? What stages of training to they correspond to? The new policy is the one that we just got via gradient descent. And the old policy is from the previous update.</p>
<p>Consider going through an epoch of updating a model using PPO. The typical PPO “iteration” (a high-level loop): - Set Old Policy: Copy your current model weights as “old policy.” - Gather Data: Roll out the environment using the old policy. - Compute Rewards &amp; Advantages: Based on the data from step 2. - Run Multiple Mini-Batch Updates: Each update modifies the model from (<span class="math inline">\(\pi_{\theta(old)}\)</span>) to (<span class="math inline">\(\pi_{\theta}\)</span>)</p>
<p>After these updates finish, your “new policy” (<span class="math inline">\(\pi_{\theta}\)</span>) is typically quite different from (<span class="math inline">\(\pi_{\theta(old)}\)</span>).</p>
<p>Then at the start of the next iteration, you set old policy = new policy (the final model from last iteration), gather fresh data, do more updates, repeat.</p>
<p>In many PPO codebases, they’ll say “we do <span class="math inline">\(K\)</span> epochs per iteration.” Those “epochs” just mean <span class="math inline">\(K\)</span> passes of gradient descent on the same collected batch. Each pass changes the policy slightly, but it’s all within a single iteration.</p>
<blockquote class="blockquote">
<p>📌 <font color="#996565"><strong>Note</strong>: As you might have guessed, this is a little bit more nuanced at the <em>start</em> of an iteration, as we don’t really yet have a <span class="math inline">\(\pi_\theta\)</span> to act as the new policy yet! So for the first step of every iteration, <span class="math inline">\(\pi_\theta = \pi_{\theta(old)}\)</span> and it updates as soon as it’s seen some of the data. See <a href="#A_3">Appendix 3</a> for a much more in-depth discussion of this, including why <strong>mini-batches</strong> can improve learning with PPO over i.e full stochastic gradient descent.</font></p>
</blockquote>
<p><a id="value_functions"></a></p>
</section>
</section>
<section id="training-the-value-function" class="level2">
<h2 class="anchored" data-anchor-id="training-the-value-function">Training The Value Function</h2>
<p>We’ve talked about the advantage, which is calculated: <span class="math display">\[
A(t) = R(t) - V_s(t)
\]</span> And how <span class="math inline">\(V_s(t)\)</span>, the value function that determines the reward that the model “expects” to receive, can generally be quite complicated.</p>
<p>Well, in practice, it is often something that is trained along with adjustments to the model, because accurate <span class="math inline">\(V_s(t)\)</span> estimates lead to less noisy advantage estimates and more stable training.</p>
<p>The value function can be trained just like any regression model: - We want <span class="math inline">\(V_{\theta}(s_t)\)</span> to <em>match</em> the actual return <span class="math inline">\(R_t\)</span>. - So, we use i.e.&nbsp;a <strong>Mean Squared Error (MSE)</strong> loss: $ L_V() = ( V_(s_t) - R_t )^2 $ - And update it alongside PPO’s policy loss as a separate loss term.</p>
<p>So a real, full PPO loss function, including the value function then becomes something like: <span class="math display">\[
L_{\text{total}} = L_{\text{CLIP}} + c_1 L_V - c_2 H[\pi]
\]</span> where: - <span class="math inline">\(L_{\text{CLIP}}\)</span> = PPO policy loss. - <span class="math inline">\(L_V\)</span> = Value function loss (MSE between <span class="math inline">\(V(s)\)</span> and <span class="math inline">\(R_t\)</span>). - <span class="math inline">\(H[\pi]\)</span> = Optional entropy term sometimes added to encourage exploration. - <span class="math inline">\(c_1, c_2\)</span> = Tunable coefficients that control the relative weights of the loss components(e.g., <span class="math inline">\(c_1 = 0.5, c_2 = 0.01\)</span>).</p>
<p>That’s all that I’ll say about training the value function here, because while we <em>will</em> train it going forward, we’re going to keep it simple. But make sure to reference <a href="#A_4">Appendix 4</a> about value functions if you’d like to know more.</p>
</section>
<section id="an-example-in-code" class="level2">
<h2 class="anchored" data-anchor-id="an-example-in-code">An Example in Code</h2>
<p>Ok. We’re ready to put together a full simple pipeline of doing reinforcement learning on an LLM!</p>
<p>We’ll make use of the <a href="https://huggingface.co/docs/trl/v0.10.1/en/index"><code>trl</code></a> library to handle the PPO part. We’ll see below how this library handles all of the “difficult” parts of implementing reinforcement learning for us.</p>
<blockquote class="blockquote">
<p>📌 <font color="#996565"><strong>Note</strong>: In this tutorial, we are using a downgraded version of <code>trl</code>, 0.10.1. I found this version was easier to use and understand the code for. Make sure if you read the docs while going through this tutorial to switch them to that version, as more updated versions changed syntax somewhat drastically!</font></p>
</blockquote>
<section id="defining-the-reward" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-reward">Defining the Reward</h3>
<p>We’re going to use a very simple reward function for this example, similar to the <code>sentiment_reward</code> that we used earlier in this tutorial, but improve it slightly and make some changes to make it easier to learn.</p>
<p>The changes we’ll make: - Assign word-based values for postive-ness and negative-ness, so some words get higher rewards than others - Multiply our positive score by <strong>5</strong> to get stronger bias towards positive words - Add a small positive bias for all sentences (0.1) so our reward is always positive. This makes for more stable training generally.</p>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define improved sentiment reward function</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sentiment_reward(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    positive_words <span class="op">=</span> {</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">'good'</span>: <span class="dv">1</span>, <span class="st">'great'</span>: <span class="dv">2</span>, <span class="st">'excellent'</span>: <span class="dv">3</span>, <span class="st">'wonderful'</span>: <span class="dv">3</span>, <span class="st">'amazing'</span>: <span class="dv">3</span>,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'happy'</span>: <span class="dv">2</span>, <span class="st">'glad'</span>: <span class="dv">1</span>, <span class="st">'love'</span>: <span class="dv">3</span>, <span class="st">'like'</span>: <span class="dv">1</span>, <span class="st">"awesome"</span>: <span class="dv">2</span>,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"fun"</span>: <span class="dv">2</span>, <span class="st">"super"</span>: <span class="dv">2</span>, <span class="st">"incredible"</span>: <span class="dv">3</span>, <span class="st">'perfect'</span>: <span class="dv">3</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    negative_words <span class="op">=</span> {</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">'bad'</span>: <span class="dv">1</span>, <span class="st">'awful'</span>: <span class="dv">2</span>, <span class="st">'terrible'</span>: <span class="dv">3</span>, <span class="st">'angry'</span>: <span class="dv">2</span>, <span class="st">'horrible'</span>: <span class="dv">3</span>,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lame'</span>: <span class="dv">1</span>, <span class="st">'hate'</span>: <span class="dv">3</span>, <span class="st">"gross"</span>: <span class="dv">2</span>, <span class="st">'sad'</span>: <span class="dv">1</span>, <span class="st">'upset'</span>: <span class="dv">2</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> text.lower().split()</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    positive_score <span class="op">=</span> <span class="bu">sum</span>(positive_words.get(word, <span class="dv">0</span>) <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    negative_score <span class="op">=</span> <span class="bu">sum</span>(negative_words.get(word, <span class="dv">0</span>) <span class="cf">for</span> word <span class="kw">in</span> words)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simple calculation with positive bias</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    reward <span class="op">=</span> (positive_score <span class="op">*</span> <span class="dv">5</span>) <span class="op">-</span> negative_score</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(<span class="fl">0.1</span>, <span class="bu">float</span>(reward))  <span class="co"># Ensure minimum positive reward</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="loading-a-model" class="level3">
<h3 class="anchored" data-anchor-id="loading-a-model">Loading A Model</h3>
<p>Here, we’ll use GPT-2. This is a small LLM that generates text in a completion way (i.e it will finish sentences for you, not respond to what you say).</p>
<p>We will load this with <a href="https://huggingface.co/docs/trl/en/models#trl.AutoModelForCausalLMWithValueHead"><code>AutoModelForCausalLMWithValueHead</code></a>, which loads both an autoregressive model with a value head in addition to the language model head. The value head is then a trainable determination of the expected value of the model’s output.</p>
<p><code>trl</code> will also want a <code>ref_model</code>: A copy of the original model, used to compare the trained model’s outputs against a reference. This reference model is used to compute KL divergence to prevent the model from deviating too much from its initial behavior.</p>
<p>Finally, we load the tokenizer, which will turn our text into numbers the model can understand, and vice versa.</p>
<p>Then, we define some generation arguments. These parameters control some more fine-grained details about how text is generated during training, and how we explore different text options during generation. In particular, these arguments determine how the model chooses a response to a prompt: - <strong><code>top_k</code></strong>: 0 When set to 0, there’s no limit on how many possible next tokens the model considers. If set to a value like 50, the model would only consider the 50 most likely next tokens and ignore all others. With top_k: 0, all possible tokens remain candidates, even unlikely ones. - <strong><code>top_p</code></strong>: 1.0 This controls “nucleus sampling” (also called “cumulative probability truncation”). A value of 1.0 means the model considers all tokens whose cumulative probability adds up to 100%. If set to 0.9, the model would only consider tokens whose cumulative probability adds up to 90%, effectively filtering out the long tail of unlikely tokens. - <strong><code>do_sample</code></strong>: True This determines whether the model uses sampling (probabilistic selection) or greedy decoding: - When <code>True</code>: The model randomly selects the next token based on the probability distribution, allowing for creativity and variation - When <code>False</code>: The model always picks the single most likely token (greedy decoding), leading to more predictable but potentially repetitive output</p>
<p>These parameters allow the model to generate more diverse outputs for a given prompt, increasing the exploration.</p>
<div id="cell-58" class="cell" data-outputid="934bd4e7-1596-45dc-ef2f-e5476cb66f23">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. load a pretrained model - with clear device placement</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLMWithValueHead.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>ref_model <span class="op">=</span> AutoModelForCausalLMWithValueHead.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>ref_model.to(device)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>generation_kwargs <span class="op">=</span> {</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"min_length"</span>: <span class="op">-</span><span class="dv">1</span>,         <span class="co"># No minimum length constraint</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"top_k"</span>: <span class="dv">0</span>,               <span class="co"># No top-k filtering (consider all tokens)</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"top_p"</span>: <span class="fl">1.0</span>,             <span class="co"># No nucleus sampling (consider all tokens)</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"do_sample"</span>: <span class="va">True</span>,        <span class="co"># Use sampling rather than greedy decoding</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"pad_token_id"</span>: tokenizer.eos_token_id,  <span class="co"># Pad with EOS token</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_new_tokens"</span>: <span class="dv">15</span>,     <span class="co"># Generate at most 15 new tokens, will help speed up training</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6e736a007d14a89a987a883b29378ba","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"011a4d60c0f14a2a852dcbb0d9855f40","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4f9c6d956ec849d691f509b2a11f7ec0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c221ac6502d945c8b44d98d185d9f4c9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"51b142041fcc434c9911ef54dfe46217","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ad31c781d8854260b479702d651ebdc2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e73afe3d60f146cd9fc42705992225d7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9b49e4a919c941d09269a32606b9d1d7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="setting-up-the-ppo-configuration" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-ppo-configuration">Setting up the PPO Configuration</h3>
<p>The PPO Trainer also allows certain configuration parameters. We’ll set some simple ones, but mostly leave this to the defaults. The learning rate is important here - we choose something quite small to keep training stable.</p>
<p>Then, <a href="https://huggingface.co/docs/trl/v0.10.1/en/ppo_trainer"><code>PPOTrainer</code></a> just needs this config, the model, reference model, and tokenizer to eventually run the full PPO pipeline!</p>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. initialize trainer with minimal parameters</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>ppo_config <span class="op">=</span> {</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mini_batch_size"</span>: <span class="dv">1</span>,     <span class="co"># Process one example at a time</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: <span class="dv">1</span>,          <span class="co"># Total batch size for one optimization step</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="fl">5e-6</span>,    <span class="co"># Learning rate for optimizer</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"log_with"</span>: <span class="va">None</span>,         <span class="co"># No external logging</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> PPOConfig(<span class="op">**</span>ppo_config)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>ppo_trainer <span class="op">=</span> PPOTrainer(config, model, ref_model, tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="getting-a-starting-point" class="level3">
<h3 class="anchored" data-anchor-id="getting-a-starting-point">Getting a Starting Point</h3>
<p>Let’s start by defining some simple prompts that <em>should</em> prompt positive responses. We’ll see how GPT-2 performs out of the box on these prompts by checking the reward that we get from its default responses.</p>
<p>You’ll see below, GPT-2 isn’t exactly the most eloquent or coherent model. That’s ok! We’re really just trying to train it to give us a bunch of postive words anyway, which is simple enough of a task that it should learn it fine!</p>
<div id="cell-62" class="cell" data-outputid="a6127b61-e7fe-4a83-eaed-d8a421979f08">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training prompts</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> [</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I feel happy when"</span>,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The best part about this is"</span>,</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love how"</span>,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Today was great because"</span>,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Before training outputs</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Before Training Outputs ==="</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>orig_responses <span class="op">=</span> {}</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>orig_rewards <span class="op">=</span> []</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> prompt <span class="kw">in</span> prompts:</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    query_tensor <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(device)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use the base model for generation to avoid CUDA errors</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> model.generate(</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>            query_tensor,</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>generation_kwargs</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    response_txt <span class="op">=</span> tokenizer.decode(response[<span class="dv">0</span>])</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    orig_responses[prompt] <span class="op">=</span> response_txt</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>    reward_value <span class="op">=</span> get_sentiment_reward(response_txt)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    orig_rewards.append(reward_value)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Response: </span><span class="sc">{</span>response_txt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reward: </span><span class="sc">{</span>reward_value<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Average initial reward: </span><span class="sc">{</span>np<span class="sc">.</span>mean(orig_rewards)<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Before Training Outputs ===

Prompt: I feel happy when
Response: I feel happy when she is interested in contributing."

When asked about others calling out the
Reward: 10.0

Prompt: The best part about this is
Response: The best part about this is you can make it very nationalful. Again, it's all true.
Reward: 0.1

Prompt: I love how
Response: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their
Reward: 15.0

Prompt: Today was great because
Response: Today was great because we knew we could create more labor for people," Smith said. "And
Reward: 10.0

Average initial reward: 8.78</code></pre>
</div>
</div>
</section>
<section id="setting-up-the-training-loop" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-training-loop">Setting up the Training Loop</h3>
<p>This is the main loop where we’ll generate responses, calculate rewards, and update the model using PPO. The logic will be as follows:</p>
<p>For each epoch &gt; For each of our prompts &gt;&gt; Tokenize the prompt &gt;&gt; &gt;&gt; Generate a response from the current version of the model (new policy) &gt;&gt; &gt;&gt; Calculate the reward of that response &gt;&gt; &gt;&gt; <strong>Give the PPO trainer the prompt, response, and corresponding reward</strong></p>
<p>The magic really happens here: <strong>Give the PPO trainer the prompt, response, and corresponding reward</strong>. The <code>trl</code> library and <code>PPOTrainer</code> that we set up are aware of the model, reference model, and tokenizer. This step handles using the reward we passed in, as well as the prompt + response that corresponded to that reward to: - <strong>Calculate probabilities</strong>: Computes how likely the generated tokens were under both the current model (new policy) and reference model (old policy) - <strong>Compute advantage</strong>: Determines how much better or worse the generated response performed compared to what was expected - <strong>Apply the PPO objective</strong>: Uses the clipped PPO objective function to limit how much the model changes in a single step - <strong>Perform backpropagation</strong>: Updates the model weights to make high-reward responses more likely in the future - <strong>Update the value function</strong>: The value function (which is part of the model from <code>AutoModelForCausalLMWithValueHead</code>) estimates the expected reward for a given state is also updated. - <strong>Enforce KL divergence</strong>: Ensures the new policy doesn’t deviate too far from the old policy, maintaining coherent text generation - <strong>Return statistics</strong>: Provides metrics about the update like loss values, KL divergence, and entropy for monitoring the training process</p>
<p>The line: <code>stats = ppo_trainer.step([query[0]], [response[0]], [rewards]))</code> encapsulates the core RL algorithm that enables the model to learn from the reward signal. It’s where the model actually learns to adjust its probability distribution to favor token sequences that lead to higher sentiment rewards. And <code>trl</code> does all of the hard stuff in the background for us!</p>
<div id="cell-64" class="cell" data-outputid="9fd45d82-9612-4bdd-d5b1-7af91738e205">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Starting Training ==="</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare for tracking training statistics</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>epoch_stats <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>epoch_rewards <span class="op">=</span> []</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run for multiple epochs</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    epoch_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    epoch_responses <span class="op">=</span> []</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shuffle prompts each epoch for better generalization</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(prompts)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each prompt</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> prompt <span class="kw">in</span> prompts:</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Encode the prompt</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>          encoded_prompt <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(device)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Generate a response using the current model</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>          <span class="cf">with</span> torch.no_grad():</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>              response <span class="op">=</span> model.generate(</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>                  encoded_prompt,</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>                  <span class="op">**</span>generation_kwargs</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>              )</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Decode the response</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>          response_txt <span class="op">=</span> tokenizer.decode(response[<span class="dv">0</span>])</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Calculate reward for this response</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>          reward_value <span class="op">=</span> get_sentiment_reward(response_txt)</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>          rewards <span class="op">=</span> torch.tensor([reward_value], dtype<span class="op">=</span>torch.<span class="bu">float</span>, device<span class="op">=</span>device)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Store for reporting</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>          epoch_reward <span class="op">+=</span> reward_value</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>          epoch_responses.append((prompt, response_txt, reward_value))</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Prepare tensors for PPO step</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>          <span class="co"># PPO requires specific tensor shapes</span></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>          query <span class="op">=</span> encoded_prompt[<span class="dv">0</span>].unsqueeze(<span class="dv">0</span>)  <span class="co"># Reshape for PPO</span></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>          response <span class="op">=</span> response[<span class="dv">0</span>].unsqueeze(<span class="dv">0</span>)     <span class="co"># Reshape for PPO</span></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Train step - update model using PPO</span></span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>          stats <span class="op">=</span> ppo_trainer.step([query[<span class="dv">0</span>]], [response[<span class="dv">0</span>]], [rewards])</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Track training metrics</span></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>          <span class="cf">for</span> k, v <span class="kw">in</span> stats.items():</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>              <span class="cf">if</span> v <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>                  epoch_stats[k].append(v)</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate and report epoch statistics</span></span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch_responses:</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>        avg_reward <span class="op">=</span> epoch_reward <span class="op">/</span> <span class="bu">len</span>(epoch_responses)</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>        epoch_rewards.append(avg_reward)</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> Summary ---"</span>)</span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Average Reward: </span><span class="sc">{</span>avg_reward<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print reward trend</span></span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>            reward_change <span class="op">=</span> avg_reward <span class="op">-</span> epoch_rewards[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Reward Change: </span><span class="sc">{</span>reward_change<span class="sc">:+.2f}</span><span class="ss">"</span>)</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print sample responses</span></span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Sample responses from this epoch:"</span>)</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (prompt, response, reward) <span class="kw">in</span> <span class="bu">enumerate</span>(epoch_responses[:<span class="dv">2</span>]):</span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Response: </span><span class="sc">{</span>response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Reward: </span><span class="sc">{</span>reward<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"No successful training steps this epoch"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Starting Training ===

Epoch 1

--- Epoch 1 Summary ---
Average Reward: 11.25

Sample responses from this epoch:
Prompt: I love how
Response: I love how I don't think I'd rule it out because I'm very fortunate to
Reward: 15.00
Prompt: Today was great because
Response: Today was great because to see all the flames in the air. It was a whirlwind of congressional
Reward: 10.00

Epoch 2

--- Epoch 2 Summary ---
Average Reward: 13.78
Reward Change: +2.53

Sample responses from this epoch:
Prompt: I love how
Response: I love how tragic this was

I love it so much I feel like the tornado
Reward: 35.00
Prompt: The best part about this is
Response: The best part about this is that, from a human point of view, all models bear all of the
Reward: 0.10

Epoch 3

--- Epoch 3 Summary ---
Average Reward: 17.52
Reward Change: +3.75

Sample responses from this epoch:
Prompt: I feel happy when
Response: I feel happy when I keep seeing Lisp. I like it. It's a joy to work
Reward: 15.00
Prompt: I love how
Response: I love how they have tried. I love that they wore their mark on their hat with
Reward: 30.00

Epoch 4

--- Epoch 4 Summary ---
Average Reward: 12.53
Reward Change: -5.00

Sample responses from this epoch:
Prompt: I love how
Response: I love how you make method shots because I love orange funk and you've grown me up
Reward: 30.00
Prompt: I feel happy when
Response: I feel happy when I have another large cock in my hips

Latelose: Acting
Reward: 10.00

Epoch 5

--- Epoch 5 Summary ---
Average Reward: 12.53
Reward Change: +0.00

Sample responses from this epoch:
Prompt: I feel happy when
Response: I feel happy when I I I I I I I I I I I I I I I
Reward: 10.00
Prompt: Today was great because
Response: Today was great because they ate a little purple cabbage or rib eye of deind. Truly Complete
Reward: 10.00

Epoch 6

--- Epoch 6 Summary ---
Average Reward: 8.78
Reward Change: -3.75

Sample responses from this epoch:
Prompt: I feel happy when
Response: I feel happy when my rest rest becomes not difficult... But when my recovery growsnt so urgent
Reward: 10.00
Prompt: I love how
Response: I love how much multiplex are haunted areas. How pipe, wood, torches, etc
Reward: 15.00

Epoch 7

--- Epoch 7 Summary ---
Average Reward: 13.78
Reward Change: +5.00

Sample responses from this epoch:
Prompt: I love how
Response: I love how they work with us," Barzero argues with both hands. And they are
Reward: 15.00
Prompt: I feel happy when
Response: I feel happy when my sleep. I feel happy when I I I be happy uncertain uncertain uncertain
Reward: 30.00

Epoch 8

--- Epoch 8 Summary ---
Average Reward: 16.27
Reward Change: +2.50

Sample responses from this epoch:
Prompt: I love how
Response: I love how it. Ourld literally bind for a happy it to per chance or for
Reward: 25.00
Prompt: The best part about this is
Response: The best part about this is long hair:
Note:You can choose between option:UnlockedSince
Reward: 0.10

Epoch 9

--- Epoch 9 Summary ---
Average Reward: 8.78
Reward Change: -7.50

Sample responses from this epoch:
Prompt: Today was great because
Response: Today was great because we are currently loving and embracing being children. We are finding joy in understanding
Reward: 10.00
Prompt: I feel happy when
Response: I feel happy when we were even able to get close to our spiritual helper.

So
Reward: 10.00

Epoch 10

--- Epoch 10 Summary ---
Average Reward: 10.03
Reward Change: +1.25

Sample responses from this epoch:
Prompt: I love how
Response: I love how I look at how I look at grabbing at back grabbing at back grabbing at
Reward: 15.00
Prompt: I feel happy when
Response: I feel happy when you g m t,

J

acje

Yes
Reward: 10.00

Epoch 11

--- Epoch 11 Summary ---
Average Reward: 10.00
Reward Change: -0.03

Sample responses from this epoch:
Prompt: I love how
Response: I love how I I I I I I I I IIIIIII
Reward: 15.00
Prompt: Today was great because
Response: Today was great because malarkey women and men changed.The grateful ones had romantic view;
Reward: 10.00

Epoch 12

--- Epoch 12 Summary ---
Average Reward: 15.03
Reward Change: +5.03

Sample responses from this epoch:
Prompt: Today was great because
Response: Today was great because of the freshandfreshlight You've commanduedTheOfGoodTheAvoid
Reward: 10.00
Prompt: The best part about this is
Response: The best part about this is that on the gazelles – I used to use them – they
Reward: 0.10</code></pre>
</div>
</div>
</section>
<section id="seeing-how-we-did" class="level3">
<h3 class="anchored" data-anchor-id="seeing-how-we-did">Seeing How We Did</h3>
<p>Finally, we can look a bit deeper to see how well we did. Let’s investigate in more depth: - <strong>Before vs After Comparison</strong>: For each prompt, we’ll compare the original model’s response with our RL-trained model’s response. This direct comparison helps us visualize the specific changes in text generation. - <strong>Individual Reward Metrics</strong>: We calculate the sentiment reward for both the original and trained responses. The difference between these scores shows how much our model has improved at generating positive text. - <strong>Aggregate Improvement</strong>: By averaging rewards across all prompts, we can quantify the overall improvement from RL training. A positive change indicates successful optimization toward our sentiment objective.</p>
<div id="cell-66" class="cell" data-outputid="aad557ed-2293-4764-aa6e-a8883b048966">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare before/after</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== After Training Outputs ==="</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>final_rewards <span class="op">=</span> []</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> prompt <span class="kw">in</span> prompts:</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate using the standard method to avoid errors</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        encoded_prompt <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(device)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>            response <span class="op">=</span> model.generate(</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>                encoded_prompt,</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>                <span class="op">**</span>generation_kwargs</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        response_txt <span class="op">=</span> tokenizer.decode(response[<span class="dv">0</span>])</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        reward_value <span class="op">=</span> get_sentiment_reward(response_txt)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        final_rewards.append(reward_value)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compare with original</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        orig_reward <span class="op">=</span> get_sentiment_reward(orig_responses[prompt])</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Before: </span><span class="sc">{</span>orig_responses[prompt]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"After: </span><span class="sc">{</span>response_txt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Reward Before: </span><span class="sc">{</span>orig_reward<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Reward After: </span><span class="sc">{</span>reward_value<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Improvement: </span><span class="sc">{</span>reward_value <span class="op">-</span> orig_reward<span class="sc">:+.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Error evaluating prompt '</span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print final stats</span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> final_rewards:</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Training Results ==="</span>)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Starting Average Reward: </span><span class="sc">{</span>np<span class="sc">.</span>mean(orig_rewards)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Ending Average Reward: </span><span class="sc">{</span>np<span class="sc">.</span>mean(final_rewards)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Change: </span><span class="sc">{</span>np<span class="sc">.</span>mean(final_rewards) <span class="op">-</span> np<span class="sc">.</span>mean(orig_rewards)<span class="sc">:+.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== After Training Outputs ===

Prompt: I feel happy when
Before: I feel happy when she is interested in contributing."

When asked about others calling out the
After: I feel happy when ordinary people are free and all my woes are banished because of quite full of
Reward Before: 10.00
Reward After: 10.00
Improvement: +0.00

Prompt: The best part about this is
Before: The best part about this is you can make it very nationalful. Again, it's all true.
After: The best part about this is the paperwork. Write the cards up front, step by step, step by
Reward Before: 0.10
Reward After: 0.10
Improvement: +0.00

Prompt: I love how
Before: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their
After: I love how Snowman looks too when she looks like Rocky Simon Newell. I love
Reward Before: 15.00
Reward After: 35.00
Improvement: +20.00

Prompt: Today was great because
Before: Today was great because we knew we could create more labor for people," Smith said. "And
After: Today was great because of was amazing and appreciated and astounding were all the immense and unrelasibility
Reward Before: 10.00
Reward After: 25.00
Improvement: +15.00

=== Training Results ===
Starting Average Reward: 8.78
Ending Average Reward: 17.52
Change: +8.75</code></pre>
</div>
</div>
<p>And there you have it! We sucessfully trained GPT-2 to give us more positive words in its responses.</p>
<p>Now, of course, we used a super simple reward here, and not a particularly good model (no offense, GPT-2), so we can see a lot of repeated words contributing to that positive response. As we discussed, in reality, a reward function should be more complicated, and our prompts used for training should be much more diverse than just 4. Still, you now know how to set up a PPO training pipeline!</p>
</section>
</section>
</section>
<section id="from-algorithmic-rewards-to-human-feedback-rlhf" class="level1">
<h1>4. From Algorithmic Rewards to Human Feedback (RLHF)</h1>
<p>So far, we’ve implemented a reinforcement learning pipeline that uses a <strong>defined (algorithmic)</strong> reward. However, most modern LLM’s undergo a training portion of reinforcement learning <strong>from human feedback</strong>, where humans looking at responses and marking them as <em>good vs bad</em> or <em>better vs worse</em> provides direct feedback to the model.</p>
<p>Luckily, the same PPO framework we’ve already discussed is used to implement this sort of feedback loop, so this is really just a small extension to what we’ve already discussed!</p>
<section id="collecting-human-feedback" class="level2">
<h2 class="anchored" data-anchor-id="collecting-human-feedback">Collecting Human Feedback</h2>
<p>In RLHF, human judgments about model outputs are collected. Unlike our algorithmic reward function that automatically calculated a score, RLHF relies on actual human preferences.</p>
<p>Typically, a real RLHF pipeline will involve humans doing pairwise comparisons of responses. Rather than asking humans to provide absolute scores, RLHF typically uses comparative judgments where annotators choose which of two responses they prefer. This involves: - <strong>Prompt Selection</strong>: A diverse set of prompts is created to cover different topics, skills, and potential failure modes. - <strong>Response Generation</strong>: For each prompt, the model generates multiple responses using different sampling parameters. - <strong>Human Annotation</strong>: Human annotators are presented with a prompt and two model-generated responses, then asked to select which one is better according to specific criteria.</p>
<div id="cell-70" class="cell" data-outputid="c5d5c7c1-571f-4dcd-a9f4-9c731b0f068e">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple simulation of human preference collection interface</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collect_human_preference(prompt, response_a, response_b):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Response A:</span><span class="ch">\n</span><span class="sc">{</span>response_a<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Response B:</span><span class="ch">\n</span><span class="sc">{</span>response_b<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        choice <span class="op">=</span> <span class="bu">input</span>(<span class="st">"Which response do you prefer? (A/B/Tie): "</span>).upper()</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> choice <span class="kw">in</span> [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"TIE"</span>]:</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> choice</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Invalid input. Please enter A, B, or Tie."</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Example prompts and responses</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>examples <span class="op">=</span> [</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"prompt"</span>: <span class="st">"Explain the concept of reinforcement learning to a high school student."</span>,</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">"response_a"</span>: <span class="st">"Reinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes."</span>,</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">"response_b"</span>: <span class="st">"Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward."</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">"prompt"</span>: <span class="st">"What are some ways to reduce stress?"</span>,</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"response_a"</span>: <span class="st">"Reducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing."</span>,</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"response_b"</span>: <span class="st">"To reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system."</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the preference collection for demonstration</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>collected_preferences <span class="op">=</span> []</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, example <span class="kw">in</span> <span class="bu">enumerate</span>(examples):</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">===== Example </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> ====="</span>)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>    preference <span class="op">=</span> collect_human_preference(</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        example[<span class="st">"prompt"</span>],</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        example[<span class="st">"response_a"</span>],</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>        example[<span class="st">"response_b"</span>]</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>    collected_preferences.append({</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">"prompt"</span>: example[<span class="st">"prompt"</span>],</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">"chosen"</span>: <span class="st">"response_a"</span> <span class="cf">if</span> preference <span class="op">==</span> <span class="st">"A"</span> <span class="cf">else</span> <span class="st">"response_b"</span> <span class="cf">if</span> preference <span class="op">==</span> <span class="st">"B"</span> <span class="cf">else</span> <span class="st">"tie"</span>,</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">"rejected"</span>: <span class="st">"response_b"</span> <span class="cf">if</span> preference <span class="op">==</span> <span class="st">"A"</span> <span class="cf">else</span> <span class="st">"response_a"</span> <span class="cf">if</span> preference <span class="op">==</span> <span class="st">"B"</span> <span class="cf">else</span> <span class="st">"tie"</span></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"You preferred Response </span><span class="sc">{</span>preference<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
===== Example 1 =====
Prompt: Explain the concept of reinforcement learning to a high school student.

Response A:
Reinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes.

Response B:
Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward.

Which response do you prefer? (A/B/Tie): B
You preferred Response B

===== Example 2 =====
Prompt: What are some ways to reduce stress?

Response A:
Reducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing.

Response B:
To reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system.

Which response do you prefer? (A/B/Tie): B
You preferred Response B

===== Collected Preferences =====
Example 1: You chose the response_b over the response_a
Example 2: You chose the response_b over the response_a</code></pre>
</div>
</div>
<p>When collecting real human feedback, annotators are typically given specific criteria to evaluate, like:</p>
<ul>
<li>Helpfulness: How well does the response address the user’s request?</li>
<li>Truthfulness: Is the information accurate and factual?</li>
<li>Harmlessness: Does the response avoid harmful, offensive, or misleading content?</li>
<li>Clarity: Is the response clearly written and easy to understand?</li>
</ul>
</section>
<section id="reward-model-training" class="level2">
<h2 class="anchored" data-anchor-id="reward-model-training">Reward Model Training</h2>
<p>While you get direct feedback this way, it would be difficult to collect <em>real</em> human feedback over all responses that you could possibly reasonably expect. So instead, a <strong>reward model</strong> is often trained to <em>predict</em> what humans like better. Better responses usually have some things in common, they’re maybe: - wordier - friendlier - more factual</p>
<p>So these more generalized properties can be learned, and then applied to prompts en masse, rather than needs humans to look at hundreds of thousands of responses individually. The reward model bridges the gap between collected human judgments and automated rewards needed for reinforcement learning.</p>
<section id="converting-preferences-to-a-reward-model" class="level3">
<h3 class="anchored" data-anchor-id="converting-preferences-to-a-reward-model">Converting Preferences to a Reward Model</h3>
<p>The reward model is essentially a classifier trained to predict human preferences. It takes in a prompt and response, and outputs a scalar value representing the “quality” of the response according to human preferences.</p>
<p>If we define:</p>
<p><span class="math inline">\((x, y_w, y_l)\)</span> as a triplet where <span class="math inline">\(x\)</span> is the prompt, <span class="math inline">\(y_w\)</span> is the preferred (winning) response, and <span class="math inline">\(y_l\)</span> is the less preferred (losing) response,</p>
<p>and</p>
<p><span class="math inline">\(r_\theta(x, y)\)</span> as our reward model with parameters <span class="math inline">\(\theta\)</span> that outputs a scalar reward for prompt <span class="math inline">\(x\)</span> and response <span class="math inline">\(y\)</span>.</p>
<p>Then, we train the reward model to maximize the log probability of the human preferences: <span class="math display">\[
{L}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma(r_\theta(x, y_w) - r_\theta(x, y_l)) \right]
\]</span></p>
<ul>
<li><p><span class="math inline">\({L}(\theta)\)</span> is the loss function we’re trying to minimize, where <span class="math inline">\(\theta\)</span> represents all the parameters of our reward model.</p></li>
<li><p><span class="math inline">\(\mathbb{E}_{(x, y_w, y_l) \sim D}\)</span> is the expected value over all triplets sampled from our dataset D. In simpler terms, it means “the average across all our training examples.”</p></li>
<li><p><span class="math inline">\(r_\theta(x, y_w)\)</span> is the reward score our model assigns to the winning (preferred) response <span class="math inline">\(y_w\)</span> given prompt <span class="math inline">\(x\)</span>.</p></li>
<li><p><span class="math inline">\(r_\theta(x, y_l)\)</span> is the reward score our model assigns to the losing (less preferred) response <span class="math inline">\(y_l\)</span> given the same prompt <span class="math inline">\(x\)</span>.</p></li>
<li><p><span class="math inline">\(\sigma(z)\)</span> is the sigmoid function, defined as <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>, which maps any real number to a value between 0 and 1.</p></li>
</ul>
<p>The equation expresses a Bradley-Terry model, which is used for pairwise comparisons. For each pair of responses, we compute the difference in their rewards: <span class="math inline">\(r_\theta(x, y_w) - r_\theta(x, y_l)\)</span>. We pass this difference through a sigmoid function, which gives us the probability that the model correctly predicts the human preference. We want to maximize this probability, which is equivalent to minimizing the negative log probability (since loss functions want to be minimized). Then, we average this loss across all training examples.</p>
</section>
<section id="training-a-reward-model" class="level3">
<h3 class="anchored" data-anchor-id="training-a-reward-model">Training a Reward Model</h3>
<p>Then, we can train a reward model based on our triplets <span class="math inline">\((x, y_w, y_l)\)</span>.</p>
<p>Below is a very simplified example. Instead of a real neural network, we just use a function <code>simulate_reward_scores</code> to see how we could calculate the loss based on the preferences indicated in the last code cell. This function simply assigns a score to a response based purely on its length.</p>
<p>In reality, this calculated loss would help a model readjust its predicted rewards.</p>
<div id="cell-75" class="cell" data-outputid="4ce45382-f943-4d0c-ff15-ee49cd696157">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a reward model's outputs</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># In reality, these would come from a neural network</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_reward_scores(response_a, response_b):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simulate reward scores for demonstration purposes"""</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Just a simple length-based score for demonstration</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    score_a <span class="op">=</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="bu">len</span>(response_a)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    score_b <span class="op">=</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="bu">len</span>(response_b)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"response_a"</span>: score_a, <span class="st">"response_b"</span>: score_b}</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate reward model loss</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reward_model_loss(scores, chosen, rejected):</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate the loss based on preference pair and model scores"""</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    chosen_score <span class="op">=</span> scores[chosen]</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    rejected_score <span class="op">=</span> scores[rejected]</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print scores for demonstration</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Chosen response score: </span><span class="sc">{</span>chosen_score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Rejected response score: </span><span class="sc">{</span>rejected_score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Score difference (chosen - rejected): </span><span class="sc">{</span>chosen_score <span class="op">-</span> rejected_score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The core loss function: -log(sigmoid(chosen_score - rejected_score))</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This encourages the model to give the preferred response a higher score</span></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    sigmoid <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> math.exp(<span class="op">-</span>(chosen_score <span class="op">-</span> rejected_score)))</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>math.log(sigmoid)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Using data from our previously collected human preferences</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, preference <span class="kw">in</span> <span class="bu">enumerate</span>(collected_preferences):</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>    example <span class="op">=</span> examples[i]</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> example[<span class="st">"prompt"</span>]</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    response_a <span class="op">=</span> example[<span class="st">"response_a"</span>]</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    response_b <span class="op">=</span> example[<span class="st">"response_b"</span>]</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>    chosen <span class="op">=</span> preference[<span class="st">"chosen"</span>]</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>    rejected <span class="op">=</span> preference[<span class="st">"rejected"</span>]</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">===== Example </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss"> ====="</span>)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"You preferred: </span><span class="sc">{</span>chosen<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initial model scoring (before training)</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> simulate_reward_scores(response_a, response_b)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> reward_model_loss(scores, chosen, rejected)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
===== Example 1: Explain the concept of reinforcement learning to a high school student. =====
You preferred: response_b
Chosen response score: 2.8900
Rejected response score: 3.2500
Score difference (chosen - rejected): -0.3600
Loss: 0.8893

===== Example 2: What are some ways to reduce stress? =====
You preferred: response_b
Chosen response score: 2.8900
Rejected response score: 2.0000
Score difference (chosen - rejected): 0.8900
Loss: 0.3441</code></pre>
</div>
</div>
</section>
</section>
<section id="rlhf-pipeline-implementation" class="level2">
<h2 class="anchored" data-anchor-id="rlhf-pipeline-implementation">RLHF Pipeline Implementation</h2>
<p>Once trained, the reward model replaces the handcrafted reward function we used earlier. During RL training:</p>
<ol type="1">
<li>The model generates a response to a prompt</li>
<li>The reward model evaluates the response, producing a scalar reward</li>
<li>This reward is used to update the model via PPO, just as we did with our algorithmic reward</li>
</ol>
<p>So, where before we had</p>
<pre><code>reward = get_sentiment_reward(response_txt)  # From our sentiment detection function</code></pre>
<p>Now with RLHF:</p>
<pre><code>reward = reward_model(prompt, response_txt)  # From our trained reward model</code></pre>
<p>While this change may look simple, it fundamentally transforms how the system learns - from optimizing for predefined metrics to optimizing for learned human preferences.</p>
<p>So, the necessary steps for a complete RLHF pipeline include:</p>
<ol type="1">
<li>Initial LLM Training: Train or fine-tune a base LLM using standard methods</li>
<li>Human Preference Collection: Gather human judgments on model outputs</li>
<li>Reward Model Training: Train a reward model to predict human preferences</li>
<li>RL Fine-tuning: Use the reward model to guide policy optimization</li>
</ol>
</section>
<section id="practical-considerations-for-scaling-rlhf" class="level2">
<h2 class="anchored" data-anchor-id="practical-considerations-for-scaling-rlhf">Practical Considerations for Scaling RLHF</h2>
<p>Implementing RLHF at scale involves several important considerations:</p>
<p><strong>Quality of Human Feedback:</strong> Diverse annotator pools to avoid bias, clear guidelines to ensure consistency, and quality control measures to identify unreliable annotations can all enhance the quality of the data that trains the reward model.</p>
<p><strong>Computational Requirements:</strong> Training a reward model adds another large model to the pipeline, and PPO fine-tuning is more compute-intensive than supervised fine-tuning. Multiple runs may also be needed to find optimal hyperparameters.</p>
<p><strong>Reward Hacking:</strong> Models can also learn to exploit weaknesses in the reward model just like they can from an algorithmic model, so it’s important to regularly update the reward model with new human judgments. Adding KL penalties can help to prevent excessive deviation from the base model.</p>
<p><strong>Distribution Shift:</strong> As the policy model improves, it generates responses outside the reward model’s training distribution. Iterative approaches that collect new human feedback on improved model outputs help address this.</p>
<p><strong>Hybrid Approaches:</strong> Combining RLHF with rule-based rewards for certain constraints can improve overall quality. Multi-objective optimization can balance different desired qualities, and ensemble reward models are sometimes used to capture different aspects of human preferences.</p>
<p>In practice, RLHF is often implemented as an iterative process:</p>
<ol type="1">
<li>Train initial reward model from human preferences</li>
<li>Perform RL fine-tuning using this reward model</li>
<li>Generate new responses with the improved policy</li>
<li>Collect new human preferences on these responses</li>
<li>Retrain or update the reward model 6.Repeat the process</li>
</ol>
<p>This iterative approach helps address distribution shift and ensures the reward model keeps pace with policy improvements.</p>
</section>
</section>
<section id="appendix" class="level1">
<h1>APPENDIX</h1>
<p><a id="A_1"></a></p>
<section id="calculating-the-expectation-value-over-a-sample-of-trajectories" class="level3">
<h3 class="anchored" data-anchor-id="calculating-the-expectation-value-over-a-sample-of-trajectories">(1) Calculating the Expectation Value Over a Sample of Trajectories</h3>
<p>The expectation value in the PPO equation is the average over all possible outcomes.</p>
<p>But what is “all possible outcomes” for an LLM - and how do we average over <strong>all</strong> of them?</p>
<p>Consider starting with a prompt. You go to generate the next token, but you have many options for it (A,B,C). Then, for each of those, you have multiple different options for the next token, which are different depending on if you went with A, or B, or C for the first token.</p>
<pre><code>Start (prompt)
 ├── Token A (P = 0.4)
 │   ├── Token X (P = 0.5)
 │   ├── Token Y (P = 0.3)
 │   ├── Token Z (P = 0.2)
 │
 ├── Token B (P = 0.3)
 │   ├── Token M (P = 0.6)
 │   ├── Token N (P = 0.4)
 │
 ├── Token C (P = 0.3)
     ├── Token P (P = 0.7)
     ├── Token Q (P = 0.3)</code></pre>
<p>You get an exponentially increasing tree of valid options for what you could have generated in response to a particular prompt. This is far too many options to calculate the expectation over <strong>all</strong> of them. So, we take a subset. There are many different ways to select this subset, but generally, you want to: - not just choose the highest probability paths (encourage <em>exploration</em>) - take some sufficiently large sample that you have explored many different options (encourage <em>diversity</em>)</p>
<p>In practice, <em>how</em> this subset is taken must be implemented <strong>at each token generation step</strong>. After all, if it’s too many options to calculate PPO over all potential trajectories, it’s also too many options to follow all the potential paths in the first place.</p>
<p>The subset can taken in a few different ways, including: - Beam Search: This method explores a subset of the most promising trajectories by maintaining a fixed number of top candidates (beams) at each step, expanding only those beams in subsequent steps.</p>
<ul>
<li><p>Top-k Sampling: At each step, the agent samples from the top-k most probable actions, reducing the action space and focusing on more likely continuations.</p></li>
<li><p>Nucleus Sampling (Top-p Sampling): This approach samples from the smallest set of actions whose cumulative probability exceeds a threshold p, balancing exploration and exploitation.</p></li>
</ul>
<p><a href="#expectation_operation"><em>Take me back to the main tutorial</em></a></p>
<p><a id="A_2"></a></p>
</section>
<section id="more-about-negative-rewards-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="more-about-negative-rewards-in-practice">(2) More About Negative Rewards in Practice</h3>
<p>When implementing PPO for LLMs, handling negative rewards requires careful consideration. Let’s explore how negative rewards affect the training process and some practical approaches to addressing the challenges they present.</p>
<p>In our PPO implementation, we observed that negative rewards lead to a situation where clipping may not function as intended. To recap, the PPO objective is: <span class="math display">\[
L_{\text{CLIP}}(\theta) = \mathbb{E}\left[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)\right]
\]</span> When we have a negative advantage <span class="math inline">\(A_t\)</span> (derived from a negative reward), the relationship between the clipped and unclipped terms reverses:</p>
<ul>
<li><p>For positive advantages, clipping prevents the probability ratio from exceeding <span class="math inline">\(1+\epsilon\)</span>, limiting how much we reward “good” behaviors</p></li>
<li><p>For negative advantages, we’d want to limit how much we penalize “bad” behaviors, but the minimum operation still selects the more negative value.</p></li>
</ul>
<p>Specifically: - If <span class="math inline">\(r_t(\theta) &gt; 1+\epsilon\)</span> and <span class="math inline">\(A_t &lt; 0\)</span>, then <span class="math inline">\(r_t(\theta)A_t &lt; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t\)</span> - The min operation selects <span class="math inline">\(r_t(\theta)A_t\)</span>, effectively ignoring the clipping</p>
<p>This asymmetry can lead to instability in training, especially when strongly negative rewards are common.</p>
<p>Several techniques have been developed to address the challenges posed by negative rewards:</p>
<ol type="1">
<li><strong>Reward Normalization</strong>. One straightforward approach is to normalize rewards across a batch or trajectory. This centers the rewards around zero and scales them to a similar magnitude, which can improve training stability. However, it does change the relative scale between different batches or episodes.</li>
</ol>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> (rewards <span class="op">-</span> rewards.mean()) <span class="op">/</span> (rewards.std() <span class="op">+</span> <span class="fl">1e-8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li><strong>Advantage Clipping</strong>. In addition to clipping the probability ratio, some implementations also clip the advantage values, preventing extremely negative advantages from causing excessively large gradient updates.</li>
</ol>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>advantages <span class="op">=</span> torch.clamp(advantages, <span class="bu">min</span><span class="op">=-</span><span class="fl">10.0</span>, <span class="bu">max</span><span class="op">=</span><span class="fl">10.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>3.. <strong>Separate Clipping for Negative Advantages</strong>. A more advanced approach is to modify the PPO objective to clip differently depending on whether the advantage is positive or negative, creating symmetrical clipping behavior regardless of the advantage’s sign:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppo_loss(ratio, advantage, epsilon<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> advantage <span class="op">&gt;=</span> <span class="dv">0</span>:</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        clipped_ratio <span class="op">=</span> torch.clamp(ratio, <span class="dv">1</span><span class="op">-</span>epsilon, <span class="dv">1</span><span class="op">+</span>epsilon)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.<span class="bu">min</span>(ratio <span class="op">*</span> advantage, clipped_ratio <span class="op">*</span> advantage)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        clipped_ratio <span class="op">=</span> torch.clamp(ratio, <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>epsilon), <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>epsilon))</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.<span class="bu">max</span>(ratio <span class="op">*</span> advantage, clipped_ratio <span class="op">*</span> advantage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="4" type="1">
<li><strong>Positive-Only Rewards</strong> We can simply avoid negative rewards entirely by using a reward structure that’s always positive (as we did/will do in our sentiment reward example). This simplifies training at the cost of potentially less nuanced feedback.</li>
</ol>
<p>Negative rewards can have several effects on training stability, including:</p>
<ul>
<li><em>Larger Gradient Updates:</em> Strongly negative rewards can lead to larger gradient updates, potentially causing the policy to change too rapidly</li>
<li><em>Policy Collapse:</em> Excessive penalties can cause the policy to become overly conservative or even collapse</li>
<li><em>Exploration Challenges:</em> If the model is heavily penalized early in training, it may stop exploring and fail to discover better strategies</li>
</ul>
<p>The best approach to negative rewards depends on the specific application. For tasks where avoiding bad behaviors is critical, negative rewards may be essential, but for creative tasks where exploration is important, positive-only rewards often work better. For complex tasks that require more nuanced feedback, normalized rewards with careful clipping often provide the best balance.</p>
<p>In practice, many state-of-the-art RLHF implementations use reward normalization combined with conservative clipping to handle both positive and negative rewards while maintaining training stability.</p>
<p><a href="#ppo_example"><em>Take me back to the main tutorial</em></a></p>
<p><a id="A_3"></a></p>
</section>
<section id="ppo-updates-in-mini-batches" class="level3">
<h3 class="anchored" data-anchor-id="ppo-updates-in-mini-batches">(3) PPO Updates in Mini-Batches</h3>
<p>You may be wondering, on a deeper level, <strong>exactly</strong> when a policy updates to the “new” policy, and when that new policy becomes the “old” policy. Let’s break it down in a bit more detail.</p>
<p>Consider the absolute simplest example. You have 16 pieces of data, and you process them as <em>one</em> batch, so each epoch contains one iteration through the data. This is true <strong>stochastic gradient descent</strong>.</p>
<p>Then, the policy updates as follows:</p>
<p><img src="images/policy-updates-1.png" width="70%" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>Because you only do one step, the ratio <span class="math inline">\((r_t(\theta) = \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)})\)</span>, which relies on getting a “new” policy from updates, doesn’t get a chance to iteratively update, and only changes once you do the gradient step at the end of an epoch, but then starts anew. <em>In this way, PPO can be less effective without minibatches</em>.</p>
<p>Consider the same data, except we batched our set of 16 into two 8-data point mini batches. Then, our updates become:</p>
<section id="epoch-1-with-mini-batch-updates" class="level5">
<h5 class="anchored" data-anchor-id="epoch-1-with-mini-batch-updates">Epoch 1 with Mini-Batch Updates:</h5>
<p><img src="images/policy-updates-2.png" width="80%" style="display: block; margin-left: auto; margin-right: auto;"></p>
</section>
<section id="epoch-2-with-mini-batch-updates" class="level5">
<h5 class="anchored" data-anchor-id="epoch-2-with-mini-batch-updates">Epoch 2 with Mini-Batch Updates:</h5>
<p><img src="images/policy-updates-3.png" width="80%" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p><strong><em>Mini-batches allow intermediate updates within an epoch, where the “old” policy is held contstant to that at the start of the epoch, but the “new” policy can be updated via gradient descent to an intermittent state!</em></strong></p>
<p>What this means is that, while mini batching isn’t <em>critical</em> to PPO’s functionality, it certainly helps. Considering how large datasets are, processing in batches is almost always required computationally anyway. But in the case of PPO, there are these additional advantages to processing this way!</p>
<blockquote class="blockquote">
<p>📌 <font color="#996565"><strong>Note</strong>: This isn’t <em>just</em> for PPO. In RLHF, on-policy methods are the most common choice. For algorithms like PPO and TRPO, the idea of keeping a fixed “old” policy to compute importance sampling ratios makes techniques such as mini-batching particularly relevant in this way. But, off-policy methods of implementing rewards are possible, and <em>don’t</em> depend as directly on comparing an old versus a new policy, and therefore may not exhibit the same sensitivity to mini-batch versus full-batch SGD dynamics. So this aside will be implementation-dependent!</font></p>
</blockquote>
<p><a href="#defining_policy"><em>Take me back to the main tutorial</em></a></p>
<p><a id="A_4"></a></p>
</section>
</section>
<section id="value-functions-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="value-functions-in-practice">(4) Value Functions in Practice</h3>
<p>The value function plays a crucial role in modern reinforcement learning for LLMs, serving as a predictor of expected rewards. In practice, value heads are typically implemented as small neural networks attached to the base LLM:</p>
<ul>
<li>The value head usually consists of 1-2 fully connected layers (often with a <span class="math inline">\(\tanh\)</span> activation)</li>
<li>It takes the final hidden state representation from the LLM (often just the last token)</li>
<li>The output is a single scalar representing the expected reward</li>
</ul>
<p>Most implementations keep the majority of parameters shared between the policy and value function, only adding a small number of value-specific parameters.</p>
<p>The way the value function is trained can significantly impact performance:</p>
<p><strong>Joint Training</strong>: In this notebook, we use this simplified approach where the value function and policy are trained simultaneously with a combined loss. This works well for initial implementations but may lead to compromises in both policy and value accuracy.</p>
<p><strong>Alternating Updates</strong>: More sophisticated implementations alternate between policy and value updates, often performing multiple value updates for each policy update. This creates a more accurate value function at the cost of additional computation.</p>
<p><strong>Separate Networks</strong>: Some advanced systems maintain entirely separate networks for policy and value functions. While computationally expensive, this approach prevents interference between the two objectives and allows specialized architectures for each function.</p>
<p>Several practical issues dramatically affect value function implementation:</p>
<ol type="1">
<li><strong>Reward Scaling</strong>: Value functions must accurately predict rewards that might vary widely in scale. Common solutions include:</li>
</ol>
<ul>
<li>Normalizing rewards to zero mean and unit variance across batches</li>
<li>Using adaptive normalization that tracks running statistics</li>
<li>Employing specialized loss functions like Huber loss that are less sensitive to outliers</li>
</ul>
<ol start="2" type="1">
<li><strong>Initialization Strategy</strong>: The initial state of the value function can significantly impact early training:</li>
</ol>
<ul>
<li>Poor initialization can lead to large, destabilizing updates</li>
<li>Some implementations pre-train the value function on collected trajectories before starting RL</li>
<li>Others initialize with small weights to ensure initial predictions stay close to zero</li>
</ul>
<ol start="3" type="1">
<li><strong>Loss Balancing</strong>: Finding the right balance between policy and value losses is crucial:</li>
</ol>
<ul>
<li>The coefficient (often denoted as <span class="math inline">\(c_1\)</span>) typically ranges from 0.25 to 1.0</li>
<li>Too much weight on value loss can cause the model to focus on reward prediction at the expense of policy improvement</li>
<li>Too little weight can lead to inaccurate advantage estimates and unstable updates</li>
</ul>
<p>When implementing a PPO system in practice, carefully tuning these aspects of the value function can mean the difference between a stable, effective training process and one that fails to converge.</p>
<p><a href="#value_functions"><em>Take me back to the main tutorial</em></a></p>


</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"011a4d60c0f14a2a852dcbb0d9855f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_589f50bf46f442a3849364479e196306","IPY_MODEL_f04950e46b444fc3a2dae50b6548d766","IPY_MODEL_7183843ccb424af79d060374468a889b"],"layout":"IPY_MODEL_502ab8f674d54db6a1f071793921fcc4"}},"072098aebdde4310a97dbd42e0ae0aea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_830465030cc74e87b4f08533b60c427b","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e467fff767d64f7c922d7b9288cf9032","value":1355256}},"10ac7a59daf84b6088cff280505368c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"135afffc51e84a41ba02bc1f98f1d39b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_148d7f11eca14e3cb283310f0d0541ca","placeholder":"​","style":"IPY_MODEL_4d85f3eb4b544f948c281205443bb9f0","value":" 665/665 [00:00&lt;00:00, 33.8kB/s]"}},"148d7f11eca14e3cb283310f0d0541ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b0efdf45d1a41e2b328365507a30421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bc44bf7dffb48d3ac6ccba0939312ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1d938788f7b49cab9f1d17eccad6bb8","max":548118077,"min":0,"orientation":"horizontal","style":"IPY_MODEL_232aa2a8b0aa4479975905689073a0d5","value":548118077}},"1c5048f436e343cea76ad8bba544daff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"232aa2a8b0aa4479975905689073a0d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23b069219e94466789b2e35360300aab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cc66b8e2edf451bb9f13a133f5f6307":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f4d503d4d694ed1b1dc14dfccd14f6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61016acfc2874022a444b6bf3f6b4fdd","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4bc4e00d73234f4dad2bce1ca4bb7d8e","value":26}},"30a064b3481a41c5ab362dc7cc84d8e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3157d39e1c314258923b69cf4edd1c3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_364ebd4ae76941efa73f74a1539c299a","placeholder":"​","style":"IPY_MODEL_23b069219e94466789b2e35360300aab","value":"generation_config.json: 100%"}},"364ebd4ae76941efa73f74a1539c299a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36dbc47c5b5f4fafaf8922afc5ed3418":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91a0bcdf2edc4dd58c382421eb710cbe","placeholder":"​","style":"IPY_MODEL_b5f1ffea03934966b00670012ec4cd89","value":"tokenizer.json: 100%"}},"3a738ffb25e0492593d5bc3bc9476ea0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71c6b6255a22447582ad6671839892d7","placeholder":"​","style":"IPY_MODEL_ca4b07b98abd48a5adfe331b64b08570","value":" 26.0/26.0 [00:00&lt;00:00, 1.01kB/s]"}},"41091faf5d8b4646a62790fe1f933bc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42a2deb6e6e242c3be203107802ce629":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"448df611bfda44189d6878b2b450cc6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4aa264cc9bb74381b6216bf4cde8aff4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bc4e00d73234f4dad2bce1ca4bb7d8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d15c1c99be84959a1628daa83697a37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d85f3eb4b544f948c281205443bb9f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f9c6d956ec849d691f509b2a11f7ec0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3157d39e1c314258923b69cf4edd1c3e","IPY_MODEL_8dbc55ba4ccb45f9adb1a8af773e9e68","IPY_MODEL_a15475c9952940a488291f3c2fdd73e7"],"layout":"IPY_MODEL_688fd61b4358438299e6bee67679cd14"}},"502ab8f674d54db6a1f071793921fcc4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51b142041fcc434c9911ef54dfe46217":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91c941bf4ded47309cee78741ac34b4c","IPY_MODEL_2f4d503d4d694ed1b1dc14dfccd14f6f","IPY_MODEL_3a738ffb25e0492593d5bc3bc9476ea0"],"layout":"IPY_MODEL_556d95c9efc1433f92d3a38a60f3dc18"}},"556d95c9efc1433f92d3a38a60f3dc18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55ac1919916b4a6ab1d1d293bb336219":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"589f50bf46f442a3849364479e196306":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5d8dad78ba8411c9329c07a5db76da6","placeholder":"​","style":"IPY_MODEL_e2ccf65589904216bee51bfbf2b69e66","value":"model.safetensors: 100%"}},"59b5cb1b3ec64e90ac5b8f2b2035c8a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f54e5d9b4e249f9989c1d6da260ddf2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61016acfc2874022a444b6bf3f6b4fdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6307452f2d5b41ff852a224d4fea89c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67550cebca3942c998852e26d91fcf6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2ff20995d7e405fb8fd463309d3f688","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd20581cf357432dbbce32f88593c7b9","value":665}},"688fd61b4358438299e6bee67679cd14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7089597853d34398910fdf8539e009b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7183843ccb424af79d060374468a889b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42a2deb6e6e242c3be203107802ce629","placeholder":"​","style":"IPY_MODEL_c7d1e049d4fe46b0ab6fec49ab9fe001","value":" 548M/548M [00:03&lt;00:00, 173MB/s]"}},"71c6b6255a22447582ad6671839892d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f0b2dee9e142a69b7edf37a6028f1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75aed34c82424303a2aab98711cba7b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830465030cc74e87b4f08533b60c427b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83a8879dac254bdf9a997e65d39dd770":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75aed34c82424303a2aab98711cba7b8","placeholder":"​","style":"IPY_MODEL_10ac7a59daf84b6088cff280505368c1","value":"vocab.json: 100%"}},"8dbc55ba4ccb45f9adb1a8af773e9e68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b489f37cdc8d418a86f8a997d9037bd5","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d582a983318241cdbc5e14e6c5a4abc4","value":124}},"8ed948bfa8794b9f9b2641f15aaf70ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a72ec5f60eb04c1484a26c8ae061c19b","placeholder":"​","style":"IPY_MODEL_be1ff27bcfb14bfb9624bfc1c101dfdc","value":" 1.04M/1.04M [00:00&lt;00:00, 5.33MB/s]"}},"91388be0ffce4ebc935cfd96b6dab444":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91a0bcdf2edc4dd58c382421eb710cbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91c941bf4ded47309cee78741ac34b4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b69dc555d48848498b57957cc1d3ff01","placeholder":"​","style":"IPY_MODEL_448df611bfda44189d6878b2b450cc6a","value":"tokenizer_config.json: 100%"}},"9287186f54654e1588e484f436ae4330":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c7f1bf3ad544d77b01602a6fea12fe6","placeholder":"​","style":"IPY_MODEL_fea4d159e2b6472b8b4bcf40717be844","value":"config.json: 100%"}},"985f5f68aaac4edbb7ae919a3a71ac2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b49e4a919c941d09269a32606b9d1d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36dbc47c5b5f4fafaf8922afc5ed3418","IPY_MODEL_072098aebdde4310a97dbd42e0ae0aea","IPY_MODEL_f216bc067e91410e8f88a29d332c7a36"],"layout":"IPY_MODEL_91388be0ffce4ebc935cfd96b6dab444"}},"9c7f1bf3ad544d77b01602a6fea12fe6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a15475c9952940a488291f3c2fdd73e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cffba8ba6a6a4537bc07c7643dd18a1a","placeholder":"​","style":"IPY_MODEL_1b0efdf45d1a41e2b328365507a30421","value":" 124/124 [00:00&lt;00:00, 1.95kB/s]"}},"a23f951c12da4b4ebd6f1060d5e5cee6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a31fb728f159431380e308e30f4be1b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7089597853d34398910fdf8539e009b4","placeholder":"​","style":"IPY_MODEL_4d15c1c99be84959a1628daa83697a37","value":"merges.txt: 100%"}},"a72ec5f60eb04c1484a26c8ae061c19b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac0545465d1a4ec2b30f1cd414be0ad3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f54e5d9b4e249f9989c1d6da260ddf2","placeholder":"​","style":"IPY_MODEL_c85862c9fa454a0f89477905f4ac19c3","value":" 548M/548M [00:05&lt;00:00, 126MB/s]"}},"ad31c781d8854260b479702d651ebdc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83a8879dac254bdf9a997e65d39dd770","IPY_MODEL_bf42167b8f1b4dcea7231a3b2a0a87fc","IPY_MODEL_8ed948bfa8794b9f9b2641f15aaf70ec"],"layout":"IPY_MODEL_59b5cb1b3ec64e90ac5b8f2b2035c8a4"}},"b1d938788f7b49cab9f1d17eccad6bb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b489f37cdc8d418a86f8a997d9037bd5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5f1ffea03934966b00670012ec4cd89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b69dc555d48848498b57957cc1d3ff01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba826b79b397434d87ac5c95e3b0a46b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc5ee959769e4ef09cc7a73d10763764":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be1ff27bcfb14bfb9624bfc1c101dfdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf42167b8f1b4dcea7231a3b2a0a87fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc5ee959769e4ef09cc7a73d10763764","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6307452f2d5b41ff852a224d4fea89c9","value":1042301}},"c221ac6502d945c8b44d98d185d9f4c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9303d094f9145708557ad6a8b3cd6e7","IPY_MODEL_1bc44bf7dffb48d3ac6ccba0939312ca","IPY_MODEL_ac0545465d1a4ec2b30f1cd414be0ad3"],"layout":"IPY_MODEL_ee61fc06d431401395430327c1e2eee4"}},"c2ff20995d7e405fb8fd463309d3f688":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c597ac0fb0744580a1378f92966ae5bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41091faf5d8b4646a62790fe1f933bc7","placeholder":"​","style":"IPY_MODEL_74f0b2dee9e142a69b7edf37a6028f1e","value":" 456k/456k [00:00&lt;00:00, 7.73MB/s]"}},"c5d8dad78ba8411c9329c07a5db76da6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6e736a007d14a89a987a883b29378ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9287186f54654e1588e484f436ae4330","IPY_MODEL_67550cebca3942c998852e26d91fcf6d","IPY_MODEL_135afffc51e84a41ba02bc1f98f1d39b"],"layout":"IPY_MODEL_ec7b714a5fd642ab9c295aec45523551"}},"c7d1e049d4fe46b0ab6fec49ab9fe001":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c85862c9fa454a0f89477905f4ac19c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9303d094f9145708557ad6a8b3cd6e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8aa87623e3c4891aad21799821800ef","placeholder":"​","style":"IPY_MODEL_4aa264cc9bb74381b6216bf4cde8aff4","value":"pytorch_model.bin: 100%"}},"ca4b07b98abd48a5adfe331b64b08570":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cffba8ba6a6a4537bc07c7643dd18a1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d582a983318241cdbc5e14e6c5a4abc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2ccf65589904216bee51bfbf2b69e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e467fff767d64f7c922d7b9288cf9032":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e73afe3d60f146cd9fc42705992225d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a31fb728f159431380e308e30f4be1b6","IPY_MODEL_f8a9615313034af9849c328858fcb1c2","IPY_MODEL_c597ac0fb0744580a1378f92966ae5bf"],"layout":"IPY_MODEL_30a064b3481a41c5ab362dc7cc84d8e9"}},"e8aa87623e3c4891aad21799821800ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec7b714a5fd642ab9c295aec45523551":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee61fc06d431401395430327c1e2eee4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f04950e46b444fc3a2dae50b6548d766":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba826b79b397434d87ac5c95e3b0a46b","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2cc66b8e2edf451bb9f13a133f5f6307","value":548105171}},"f216bc067e91410e8f88a29d332c7a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c5048f436e343cea76ad8bba544daff","placeholder":"​","style":"IPY_MODEL_a23f951c12da4b4ebd6f1060d5e5cee6","value":" 1.36M/1.36M [00:00&lt;00:00, 5.10MB/s]"}},"f8a9615313034af9849c328858fcb1c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_985f5f68aaac4edbb7ae919a3a71ac2f","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55ac1919916b4a6ab1d1d293bb336219","value":456318}},"fd20581cf357432dbbce32f88593c7b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fea4d159e2b6472b8b4bcf40717be844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>