[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Abbie! I‚Äôm a Postdoctoral Research Fellow at Vanderbilt‚Äôs Data Science Institute. I got my Ph.D.¬†in Astrophysics in 2022, where my dissertation work focused on predicting the growth of dark matter structures in the universe by using various machine learning techniques. Now in my postdoc role, my research has expanded across modalities and methodologies. As such, the breadth of AI techniques I‚Äôve learned about has expanded. The Jupyter notebooks here include a selection of some of the concepts I‚Äôve explored more deeply."
  },
  {
    "objectID": "posts/RL_for_LLMs.html",
    "href": "posts/RL_for_LLMs.html",
    "title": "RLHF for LLMs",
    "section": "",
    "text": "In this notebook, we‚Äôll dive deep into how LLM‚Äôs are trained using reinforcement learning, exploring how we can implement this type of training on an existing, small model to better guide our desired responses!\n\n\n\nIn a typical LLM training paradigm, models are first trained in an self-supervised manner to predict a masked-out word. This makes them great at understanding how language is constructed, and helps us to make giant datasets very easily.\nHowever, models trained to simply predict what word comes next are not very helpful to talk to. If I ask a model trained in this manner ‚ÄúWhat kind of tree is a cedar?‚Äù It may respond ‚ÄúI would really like to know about cedar trees.‚Äù because it thinks it should be continuing the thought.\nThe next step, then, in LLM training, is typically instruction fine tuning (IFT). In this step, models are trained to respond and follow instructions. In this step, they come to recognize when they are being asked a question, and the language produced thereafter becomes a response not simply a continuation. After IFT, a model might respond: ‚Äúconiferous‚Äù\nThe model now answers the question (hopefully correctly.) But it may not be friendly, or be too terse, or not structure its answers particularly well, or even be mean and harmful. So, the last step in training is typically reinforcement learning from human feedback (RLHF, or just RL). In this step, the model is told what responses of its are better and encouraged to respond in that type of way. While this can adjust the responses in many ways, one example is that it may encourage more information or detail, or generally more wordiness. So, a model trained with RL may respond ‚ÄúA cedar is a type of coniferous tree in the genus Cedrus. Cedars are evergreen trees known for their aromatic wood, needle-like leaves, and cones. They belong to the family Pinaceae.‚Äù\n\nüìå Note: A great textbook on reinforcement learning is Reinforcement Learning: An Introduction. I highly reccommend referencing it if, after or during this tutorial, you find yourself wanting an even deeper dive into some of these concepts."
  },
  {
    "objectID": "posts/RL_for_LLMs.html#installations-and-imports",
    "href": "posts/RL_for_LLMs.html#installations-and-imports",
    "title": "RLHF for LLMs",
    "section": "Installations and Imports",
    "text": "Installations and Imports\n\n# Install required packages\n!pip install -q --upgrade transformers datasets\n!pip install -q trl==0.10.1 #install downgraded version because it's easier to use!\n\n(If the below imports fail, you may need to restart the kernel for those installations to take effect).\n\n# Basic imports\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\nimport numpy as np\nimport math\nfrom datasets import Dataset\nimport warnings\nimport re\nfrom typing import Dict, List, Tuple\n\nfrom collections import defaultdict\nfrom transformers import GPT2Tokenizer\n\nimport random\n\nwarnings.filterwarnings('ignore')\n\nIf you have access to it, a GPU runtime will make this code run smoother (or might make it possible to run at all!)\nThe below code will confirm if you‚Äôre on a GPU. You want to see CUDA available: True\nIt‚Äôs not required, but preferred. A stronger CPU might be required.\n\n# Check PyTorch version and CUDA availability\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n\nPyTorch version: 2.5.1+cu124\nCUDA available: False\n\n\n\n(OPTIONAL) Hugging Face Authentication\nLater in this notebook, we‚Äôll be grabbing a model off of huggingface. While the one we use here doesn‚Äôt require a token, you can add your token here if you want to experiment with swapping GPT-2 for a different model that needs authentication.\n\nfrom huggingface_hub import login\nimport getpass\n\ntoken = getpass.getpass(\"Enter your Hugging Face token: \")\n\n# Verify login\nprint(\"Login status: Authenticated with Hugging Face\")\n\nLogin status: Authenticated with Hugging Face"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#an-introduction-from-traditional-training-to-reinforcement-learning",
    "href": "posts/RL_for_LLMs.html#an-introduction-from-traditional-training-to-reinforcement-learning",
    "title": "RLHF for LLMs",
    "section": "An Introduction: From Traditional Training to Reinforcement Learning",
    "text": "An Introduction: From Traditional Training to Reinforcement Learning\nBefore we get started, let‚Äôs talk about what exactly the goals of RL are, how it differs from ‚Äútraditional‚Äù training, and get a basic understanding of the RL pipeline.\n\nTraditional Training: A Quick Review\nIn ‚Äútraditional‚Äù, supervised training of a transformer or neural network, which you‚Äôre likely familiar with, the process looks like this:\n\nYou have training data with inputs and known correct outputs\nThe model makes predictions\nYou calculate a loss function (like MSE or cross-entropy) that measures how wrong the predictions are\nBackpropagation updates the weights to minimize this loss\nRepeat until the model gets good at predicting correct outputs\n\nThe key here is that for every input, you know exactly what the correct output should be.\nBut what if you don‚Äôt know the exact right answer? What if you just know when answers are ‚Äúbetter‚Äù or ‚Äúworse‚Äù? This is where reinforcement learning comes in.\nConsider training an LLM to be helpful and truthful. There‚Äôs no single ‚Äúcorrect‚Äù response to a prompt - there might be many good responses and many bad ones. We can‚Äôt use traditional supervised learning because: - We don‚Äôt have examples of perfect responses - Multiple very different responses might be equally good - We care about abstract qualities (helpfulness, truthfulness) more than exact word matches\n\n\nEnter Reinforcement Learning\nRL approaches this differently:\n\nInstead of a loss function that measures ‚Äúwrongness‚Äù, we use a reward function that measures ‚Äúgoodness‚Äù\nInstead of comparing to correct answers, we try different outputs and see which get higher rewards\nInstead of direct supervision, the model learns through trial and error\n\nRL requires, at a high level:\n1. Generation Phase - Model receives a prompt - Model generates multiple different possible responses - This is called ‚Äúexploring‚Äù the space of possible outputs\n2. Evaluation Phase - Each generated response gets a reward score - Better responses = higher rewards - This tells us which outputs we want to encourage\n3. Learning Phase - Model is updated to make high-reward outputs more likely - But, it doesn‚Äôt memorize specific outputs - Instead, it learns patterns that tend to lead to high rewards\n4. Repeat - Generate new responses - Evaluate them - Learn from the results - Over time, the model gets better at generating high-reward outputs\n\n\nSome Key Differences from Traditional Training\n1. Exploration vs Exploitation - The model needs to try new things (explore) to find better strategies - But it also needs to use what it knows works (exploit) - This ‚Äúexploration-exploitation tradeoff‚Äù doesn‚Äôt exist in traditional training\n2. Delayed Rewards - Sometimes we don‚Äôt know if an output was good until several steps later - The model needs to learn which actions led to good outcomes - This is very different from immediate feedback in traditional training\n3. Moving Targets - As the model improves, it generates different outputs - These new outputs might get different rewards - So, the learning process is more dynamic than traditional fixed-dataset training"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#defining-the-reward-function",
    "href": "posts/RL_for_LLMs.html#defining-the-reward-function",
    "title": "RLHF for LLMs",
    "section": "Defining the Reward Function",
    "text": "Defining the Reward Function\nSo, what is a reward function? In reality, it can take many forms. But in general, a reward function needs to be:\n\nClear: It should have a well-defined relationship between output quality and some numerical score\nConsistent: Similar outputs should get similar rewards\nMeaningful: Higher rewards should genuinely represent better outputs\nComputationally Feasible: As we need to calculate rewards for many outputs quickly\n\nReward functions can also incorportate negative rewards for behaviors that the model wants to explicitly avoid."
  },
  {
    "objectID": "posts/RL_for_LLMs.html#a-very-simple-example",
    "href": "posts/RL_for_LLMs.html#a-very-simple-example",
    "title": "RLHF for LLMs",
    "section": "A (Very) Simple Example",
    "text": "A (Very) Simple Example\nLet‚Äôs just start with a very basic example to illustrate what a simple reward function could be. Below, we‚Äôll write a ‚Äúpositive sentiment‚Äù reward function that counts how many positive and negative words were used in a response, giving positive rewards for positive words, and negative rewards for negative words.\n\n# Simple reward function\ndef sentiment_reward(text: str) -&gt; float:\n    positive_words = ['good', 'great', 'excellent', 'wonderful', 'amazing']\n    negative_words = ['bad', 'awful', 'terrible']\n\n    words = text.lower().split()\n    positive_count = sum(1 for word in words if word in positive_words)\n    negative_count = sum(1 for word in words if word in negative_words)\n\n    return positive_count - negative_count\n\n\n# Test our reward function with some example sentences\ntest_texts = [\n    \"This is a good and great day\",\n    \"Nothing particularily special happened today, but I was still satisfied\",\n    \"Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later\",\n    \"Good wow great amazing excellent stuff, wow great and good and great\"\n]\n\nprint(\"\\nTesting reward function:\")\nfor text in test_texts:\n    print(f\"\\nText: {text}\")\n    print(f\"Reward: {sentiment_reward(text):.2f}\")\n\n\nTesting reward function:\n\nText: This is a good and great day\nReward: 2.00\n\nText: Nothing particularily special happened today, but I was still satisfied\nReward: 0.00\n\nText: Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later\nReward: -1.00\n\nText: Good wow great amazing excellent stuff, wow great and good and great\nReward: 7.00\n\n\nSo, we can see that the above, while likely far too simple to actually be useful, counts as a reward function, as it meets our criteria and provides some way to understand ‚Äúbetter‚Äù responses.\nThis reward function clearly meets the requirements of: 1. Clear relationship: More positive words = higher score 2. Fast to compute: Simple word counting is very efficient 3. Easy to understand: The logic is straightforward\nHowever, we can point out some clear reasons that this would be an unuseful reward function in practice: 1. Easy to game: Model could just repeat positive words 2. Misses context: ‚ÄúNot good‚Äù counts as positive 3. Ignores quality: Well-written neutral text scores lower than poorly written positive text (see second example vs last)\nSo, a good reward function should take more into account when deciding if a whole response is good or not.\n\nOne Step Better: Adding Context\nLet‚Äôs improve our reward function by considering context. We‚Äôll: 1. Account for negations 2. Consider word positioning 3. Add penalties for repetition\n(We‚Äôll also, just for simplicity sake, move this to be positive-detecting only, no negative rewards)\n\n# Improved reward function with context awareness\ndef positivity_reward(text: str) -&gt; float:\n    # We'll keep the same (extremely incomplete) word list\n    positive_words = ['good', 'great', 'excellent', 'wonderful', 'amazing']\n\n    words = text.lower().split()\n    score = 0.0\n\n    # Check for negations (looking at pairs of words)\n    for i in range(len(words)):\n        if words[i] in positive_words:\n            # Check if previous word is a negation\n            if i &gt; 0 and words[i-1] in {'not', 'never', \"don't\", 'no'}:\n                score -= 0.5  # Penalty for negated positive words\n            else:\n                score += 1.0\n\n    # Penalty for repetition\n    unique_words = len(set(words))\n    repetition_penalty = unique_words / max(len(words), 1)\n\n    # Calculate final score with penalties\n    final_score = (score / max(len(words), 1)) * repetition_penalty\n\n    # Clip to range [0, 1]\n    return max(min(final_score, 1.0), 0.0)\n\n# Test the improved function\ntest_texts = [\n    \"This is good and helpful.\",\n    \"This is not good at all.\",\n    \"good good good good good\",\n    \"The explanation was clear and helpful, making it incredibly beneficial.\"\n]\n\nprint(\"Testing our improved reward function:\")\nfor text in test_texts:\n    reward = positivity_reward(text)\n    print(f\"\\nText: {text}\")\n    print(f\"Reward: {reward:.3f}\")\n\nTesting our improved reward function:\n\nText: This is good and helpful.\nReward: 0.200\n\nText: This is not good at all.\nReward: 0.000\n\nText: good good good good good\nReward: 0.200\n\nText: The explanation was clear and helpful, making it incredibly beneficial.\nReward: 0.000\n\n\nSo, we‚Äôve definitely improved things! But you can see how easy pitfalls are. The last example was definitely positive in tone, but that‚Äôs not accounted for here. We could add ‚Äúclear‚Äù, ‚Äúhelpful‚Äù and ‚Äúbeneficial‚Äù to our positive words list, but then how many words would need to be added just because sometimes, in the right context, they are positive? Perhaps ‚Äúincredibly‚Äù is the thing giving this sentence a positive connotation, but then that could also be used to say something is ‚Äúincredibly‚Äù negative.\nYou can see how reward functions we use in practice require some careful considerations."
  },
  {
    "objectID": "posts/RL_for_LLMs.html#real-world-reward-functions",
    "href": "posts/RL_for_LLMs.html#real-world-reward-functions",
    "title": "RLHF for LLMs",
    "section": "Real-World Reward Functions",
    "text": "Real-World Reward Functions\nIn practice, good reward functions for LLMs often combine multiple components, as there are a lot of factors to consider when deeming a response as ‚Äúgood‚Äù:\n1. Quality Metrics measure for things like grammar and fluency, relevance of the response to the given prompt, factual accuracy, and overall coherence and structure.\n2. Task-Specific Metrics measure for things like format adherence, style matching, any domain-specific requirements, and length constraints\n3. Safety and Alignment Metrics include toxicity detection and bias measurements, and check for things like truthfulness and helpfulness.\n\nA More Practical Reward Function\nLet‚Äôs see how we can implement some of those metrics above and build a (still very basic, but more realistic) reward function that would be more useful in practice than our ‚Äúpositive words‚Äù detection.\n\ndef calculate_rewards(prompt: str, response: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate multiple reward components for a given response.\n    Returns a dictionary of different reward aspects.\n    \"\"\"\n    rewards = {}\n\n    # 1. Response Length Reward\n    # Encourage responses between 50 and 500 words\n    words = response.split()\n    word_count = len(words)\n    length_reward = min(1.0, word_count / 50.0) if word_count &lt; 50 else \\\n                   1.0 if 50 &lt;= word_count &lt;= 500 else \\\n                   max(0.0, 1.0 - (word_count - 500) / 500)\n    rewards['length'] = length_reward\n\n    # 2. Prompt Relevance Reward\n    # Check if response uses key terms from prompt\n    prompt_words = set(prompt.lower().split())\n    response_words = set(response.lower().split())\n    overlap = len(prompt_words.intersection(response_words))\n    relevance_reward = min(1.0, overlap / max(len(prompt_words), 1))\n    rewards['relevance'] = relevance_reward\n\n    # 3. Format Quality Reward\n    # Check for good formatting practices\n    format_scores = []\n    # Has paragraphs\n    format_scores.append(1.0 if response.count('\\n\\n') &gt; 0 else 0.0)\n    # Uses punctuation properly\n    format_scores.append(1.0 if re.search(r'[.!?]\\s+[A-Z]', response) else 0.0)\n    # Proper capitalization\n    format_scores.append(1.0 if re.search(r'^[A-Z]', response) else 0.0)\n    rewards['formatting'] = sum(format_scores) / len(format_scores)\n\n    # 4. Calculate Final Combined Reward\n    weights = {'length': 0.2, 'relevance': 0.5, 'formatting': 0.3}\n    final_reward = sum(rewards[k] * weights[k] for k in weights)\n    rewards['final'] = final_reward\n\n    return rewards\n\n# Let's test our practical reward function\ntest_cases = [\n    {\n        \"prompt\": \"Explain how photosynthesis works.\",\n        \"response\": \"\"\"\nPhotosynthesis is the process by which plants convert sunlight into energy.\n\nThe process involves chlorophyll in the leaves capturing sunlight. This energy\nis used to convert water and carbon dioxide into glucose and oxygen.\n\nPlants use glucose for energy and release oxygen as a byproduct.\n\"\"\"\n    },\n    {\n        \"prompt\": \"Explain how photosynthesis works.\",\n        \"response\": \"plants make food from sun\"\n    }\n]\n\nfor case in test_cases:\n    rewards = calculate_rewards(case[\"prompt\"], case[\"response\"])\n    print(f\"\\nPrompt: {case['prompt']}\")\n    print(f\"Response: {case['response']}\")\n    print(\"\\nRewards:\")\n    for aspect, score in rewards.items():\n        print(f\"{aspect}: {score:.3f}\")\n\n\nPrompt: Explain how photosynthesis works.\nResponse: \nPhotosynthesis is the process by which plants convert sunlight into energy.\n\nThe process involves chlorophyll in the leaves capturing sunlight. This energy\nis used to convert water and carbon dioxide into glucose and oxygen.\n\nPlants use glucose for energy and release oxygen as a byproduct.\n\n\nRewards:\nlength: 0.900\nrelevance: 0.250\nformatting: 0.667\nfinal: 0.505\n\nPrompt: Explain how photosynthesis works.\nResponse: plants make food from sun\n\nRewards:\nlength: 0.100\nrelevance: 0.000\nformatting: 0.000\nfinal: 0.020"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#key-takeaways-about-reward-functions",
    "href": "posts/RL_for_LLMs.html#key-takeaways-about-reward-functions",
    "title": "RLHF for LLMs",
    "section": "Key Takeaways About Reward Functions",
    "text": "Key Takeaways About Reward Functions\nSo, when making practical reward functions, there are multiple ways to make a reward function better, including (but not necessarily limited to):\n\nadding multiple reward components help avoid gaming the system\nusing weighted combinations allow prioritizing different target metrics\ndefining clear relationships between quality and reward score\n\nSome very common pitfalls to keep in mind: - Reward Hacking: some rewards are easily maximized by doing something very simple, not actually increasing quality - Unclear Signaling: it can be difficult to make rewards that very clearly, concretely determine ‚Äúgoodness‚Äù - Computational Overhead: Complex rewards can obviously be helpful in targeting rich, high-quality results, BUT can slow down training significantly - Inconsistent Scaling: Be sure to normalize and weight any scores that are combined appropriately."
  },
  {
    "objectID": "posts/RL_for_LLMs.html#key-components-of-ppo",
    "href": "posts/RL_for_LLMs.html#key-components-of-ppo",
    "title": "RLHF for LLMs",
    "section": "Key Components of PPO",
    "text": "Key Components of PPO\n\nThe Policy (œÄ): In our case, this is our language model. It has two key behaviors:\n\n\nActing: Generating text given a prompt\nCalculating Probabilities: Telling us how likely it would be to generate specific text\n\n\nThe Clipped Objective: This is PPO‚Äôs real metric for improvement. It includes:\n\n\nProbability Ratio (r): How much more/less likely is the new policy to generate certain text compared to the old policy?\nClipping: We put boundaries on this ratio (typically between 0.8 and 1.2)\nAdvantage (A): How much better/worse was the outcome than expected?\n\nFor LLM‚Äôs, PPO is implemented on a token-by-token basis, so each token gets the opportunity to be better or worse for the outcome. In this way, PPO is not exactly a loss function but uses a surrogate objective (a type of loss function) to guide updates. It incorporates the reward by comparing the new policy‚Äôs performance (via probabilities of actions, i.e., token predictions) to the old policy‚Äôs performance, scaled by the reward."
  },
  {
    "objectID": "posts/RL_for_LLMs.html#the-ppo-mathematically",
    "href": "posts/RL_for_LLMs.html#the-ppo-mathematically",
    "title": "RLHF for LLMs",
    "section": "The PPO Mathematically",
    "text": "The PPO Mathematically\nOf course, how the PPO is implemented in practice is through that clipped objective function.\nThe PPO objective function \\((L^{CLIP})\\) is defined as: \\[\nL^{CLIP}(\\theta) = E[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]\n\\]\nWhere:\n\n\\(\\theta\\) represents the policy parameters (our LLM weights)\n\\(r_t(\\theta)\\) is the probability ratio, \\(\\pi_{\\theta}(a_t|s_t)\\) / \\(\\pi_{\\theta_{old}}(a_t|s_t)\\)\n\\(A_t\\) is the advantage\n\\(\\epsilon\\) is the clipping parameter (typically 0.2)\n\n\n\nFor an LLM, the policy \\(\\pi_{\\theta}(a_t|s_t)\\) represents the probability of choosing token \\(a_t\\) given context \\(s_t\\):\n\n\n\\(s_t\\) is the current context (prompt + generated tokens so far)\n\\(a_t\\) is the next token to generate\n\n\nThe probability ratio, \\(r_t(\\theta) = \\pi_{\\theta}(a_t|s_t)\\) / \\(\\pi_{\\theta_{old}}(a_t|s_t)\\) is the ratio in probability of the new policy (\\(\\theta\\)) choosing token \\(a_t\\) over the probability from the old policy (\\(\\theta_{old}\\)) choosing token \\(a_t\\). (i.e \\(r_t(\\theta)\\) = 2 means it‚Äôs twice as likely to choose \\(a_t\\) in the new policy)\nThe clipping function, \\(clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\) ensures this probability ratio is reasonably bounded, keeping updates small.\nThen for each token, we calculate both the unclipped objective: \\(r_t(Œ∏)A_t\\) and the clipped objective \\(clip(r_t(Œ∏), 1-Œµ, 1+Œµ)A_t\\), and take the minimum of both, again ensuring smaller, more stable updates.\nWhere (\\(A(t)\\)) is the ‚Äúadvantage‚Äù (\\(A(t)\\)) is a measurement of how much better or worse an action was compared to what we expected, it incorporates, but isn‚Äôt exactly the reward‚Ä¶\n\n\n‚Ä¶The ‚ÄúAdvantage‚Äù \\((A_t)\\)\nOk, so we spent all of that time talking about the reward, just for it to end up wrapped in something called the ‚Äúadvantage‚Äù, so let‚Äôs break down the advantage a bit more.\nAdvantage \\((A_t)\\) measures how much better or worse an action was compared to what we expected. It‚Äôs calculated as: \\[A_t = R_t - V(s_t)\\]\nwhere: - \\(R_t\\) is the actual reward received - \\(V(s_t)\\) is the expected value (what we thought we‚Äôd get)\nIn the simplest example, imagine you‚Äôre a language model deciding what word to generate:\n\nYou generate ‚Äúexcellent‚Äù with an expected reward of 0.5. Then, for an actual reward of 0.8: \\[Advantage = 0.8 - 0.5 = +0.3\\] This was better than expected.\nYou generate ‚Äúokay‚Äù with an expected reward of 0.5. Then, for an actual reward of 0.3: \\[Advantage = 0.3 - 0.5 = -0.2\\] Negative advantage means this was worse than expected.\n\nAdvantage is used over raw reward because it allows harder prompts to expect harder results. For example, easy prompts could be very easy to get a reward of 0.8, but hard prompts hardly ever get even 0.4. Hard vs easy prompts having their own expected reward can accommodate this difference in their ‚Äútypical‚Äù reward.\nOk, so reward for a given word, we‚Äôve covered. But expected reward, what we ‚Äúthought we‚Äôd get‚Äù‚Ä¶ you might have guessed that that‚Äôs a bit more complicated.\n\n\nThe ‚ÄúExpected Value‚Äù (\\(V(s_t)\\))\nThe expected value is what you think your reward should be. This is naturally difficult to define, so you can really get this in a lot of ways. Some very simple ways could be: - defining different expected values for different categories of prompts. i.e:\nexpected_values = {\n    \"math\": 0.6,        # Math problems are harder\n    \"greeting\": 0.9,    # Greetings are easy\n    \"general\": 0.7      # General conversation\n}\nexpected_value = expected_values[get_prompt_type(prompt)]\n\nkeeping track of rewards you‚Äôve already given different types of prompts:\n\nrewards_history = {\n    \"math_questions\": [0.6, 0.7, 0.5, 0.8],  # Previous rewards for math\n    \"greeting\": [0.9, 0.95, 0.85, 0.9]       # Previous rewards for greetings\n}\nexpected_value = average(rewards_history[prompt_type])\nBut in practice, calculating the expected reward tends to be more sophisticated. It can be done with:\n\nValue Networks: these are neural networks who are trained to predict rewards. It can handle more nuanced contexts about the prompts, and more easily adapt to things like prompt lengths, topics covered, and depth of explanation asked for.\nTemporal Differences: A way to consider future rewards, not just immediate ones. Instead of just looking at the current reward (R), we also consider what rewards we expect in the future (\\(\\lambda\\) * \\(V_{next}\\), where \\(\\lambda\\) is a discount factor). This helps when your early actions lead to better outcomes later.\nGeneralized Advantage Estimation (GAE): A method that balances between immediate and future rewards when calculating advantage. It uses a weighted average of rewards over different time spans, helping to reduce variance in our advantage estimates while still maintaining useful learning signals.\n\nIn this notebook, we‚Äôll stick to some of these very simple expected value definitions. But it‚Äôs important to know that this is a choice to be made in any RL implementation! And, that the value function used is also something that can be updated. So, during training, a value function can learn to generate better expected values: we can use loss functions to update both \\(\\pi_\\theta\\) and (\\(V(s_t)\\)). We‚Äôll discuss this in more detail below!\n\n\n\nThe Expectation Operation\nI promise there will soon be code. But there‚Äôs one more thing to clear up. The PPO objective function \\((L^{CLIP})\\): \\[\nL^{CLIP}(\\theta) = E[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]\n\\]\nhas that ‚Äú\\(E\\)‚Äù - what is that?\nThis is the expectation operation. That‚Äôs mathematically simple enough - it‚Äôs really just a weighted average: the mean of the possible values that a variable can take. That‚Äôs all!\nBut what are ‚Äúall possible values‚Äù? For LLM‚Äôs, this is a sample of all possible experiences (trajectories) collected from the policy. So really, we average over a sample of all token paths that you could have generated in response to all prompt examples. Why a sample? For a bit more detailed explanation, see Appendix 1.\nWe‚Äôll touch on this again in a bit, but for now, let‚Äôs jump into an example."
  },
  {
    "objectID": "posts/RL_for_LLMs.html#a-simple-ppo-example",
    "href": "posts/RL_for_LLMs.html#a-simple-ppo-example",
    "title": "RLHF for LLMs",
    "section": "A Simple PPO Example",
    "text": "A Simple PPO Example\nLet‚Äôs see an actual implementation of the PPO calculation, to observe how, in practice, this is calculated over trajectories that an LLM could generate.\n\nFor a Single Token\nBelow, we‚Äôll see how the PPO function would calculate its objective for a single new potential token.\nConsider in this example that we have generated so far:\n\n\n‚ÄúI am feeling great and happy today, it‚Äôs‚Äù\n\n\nAnd we are tasked with determining if the new word that we could generate:\n\n\n‚Äúexcellent‚Äù\n\n\nis advantageous or not.\nThe current policy (model) generates ‚Äúexcellent‚Äù with probability 0.3, and the new proposed policy we‚Äôre evaluating generates ‚Äúexcellent‚Äù with probability 0.6.\nBelow, we‚Äôll calculate the PPO Objective \\((L^{CLIP})\\) that would result from the production of that token. We‚Äôll use our super simple sentiment_reward function that we defined above to determine our reward.\n\ndef ppo_update(old_prob, new_prob, advantage, epsilon=0.1):\n    # Calculate the probability ratio\n    prob_ratio = new_prob / old_prob\n\n    # Compute the unclipped and clipped objectives\n    unclipped_objective = prob_ratio * advantage\n    clipped_objective = np.clip(prob_ratio, 1 - epsilon, 1 + epsilon) * advantage\n\n    # PPO's objective is the minimum of the unclipped and clipped objectives\n    ppo_objective = min(unclipped_objective, clipped_objective)\n\n    return ppo_objective\n\n# new word to generate that we are testing the update for\nnew_word = \"excellent\"\n\n# Let's say, the old policy generated this word with 30% chance, and the new one generates it with 60% chance\nold_prob = 0.3\nnew_prob = 0.6\n\n# Simulated response from the agent so far\nresponse = \"I am feeling great and happy today, it's\"\n\n# Calculate the reward based on our very simple positive word reward from before\n# this calculates over the full current response with this new next token\nreward = sentiment_reward(response + new_word)\n\n# Let's say in this case, the expected behavior is that the new word is neutral,\n# so we expect it to not *add* any reward over what we had\nexpected_reward = sentiment_reward(response)\n\n# calculate the advantage that this new word has given us\nadvantage = reward - expected_reward\n\n# Perform PPO update\nppo_objective = ppo_update(old_prob, new_prob, advantage)\n\nprint(f\"Response: {response}\")\nprint(f\"Positivity Reward: {reward}\")\nprint(f\"Advantage: {advantage}\")\nprint(f\"PPO Objective: {ppo_objective}\")\n\nResponse: I am feeling great and happy today, it's\nPositivity Reward: 1\nAdvantage: 0\nPPO Objective: 0.0\n\n\nWe can see here, that adding excellent resulted in positive advantage: it was good to our reward function to add that word.\nWe can also see when we step through the math, that our objective got clipped: Our probability ratio was 2.0, so: - unclipped objective was \\(2.0 * 1.0 = 2.0\\). - clipped objective (what‚Äôs used) was \\(1.1 * 1.0 = 1.1\\)\nThis is a key action of PPO: ensuring that policy updates remain relatively small.\n\nüìå Note: you may be able to see here, that if our advantage term had been large, even with clipping, the update can be substantial, potentially leading to instability. To mitigate this, it‚Äôs common practice to normalize or scale rewards, thereby controlling the magnitude of the advantage. This normalization helps maintain stable and consistent updates.\n\n\n\nSimple PPO Example Over a Trajectory\nIn practice, any given ‚Äúresponse‚Äù which is a series of token selections is a trajectory that the model could have generated.\nFor each word in:\n\n\n‚ÄúI am feeling great and happy today, it‚Äôs‚Äù\n\n\nthere is an associated old probability, new probability, and reward for generating that token.\nWe can calculate \\(L_{CLIP(\\text{trajectory})}\\) over a given trajectory as: \\[\nL_{CLIP(\\text{trajectory})} =  \\sum_t^T L_{CLIP(t)}\n\\]\nfor a sequence of length T, comprised of tokens t\n\n# for a given trajectory of tokens, each generated with some probability\nresponse_tokens = [\"I\", \"am\", \"feeling\", \"great\", \"and\", \"happy\", \"today,\", \"it's\", \"excellent\"]\n\nold_probs = [.9, .95, .4, .25, .33, .45, .4, .15, .3] # dummy probabilities for each word in the sequence, in old policy\nnew_probs = [.9, .95, .6, .55, .5, .65, .3, .2, .6] # dummy probabilities for each word in the sequence, in new policy\n\n# Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)\nexpected_reward = 0\n\n# Compute PPO objectives for each token\nppo_objectives = []\nfor i, token in enumerate(response_tokens):\n    reward = sentiment_reward(token)\n    advantage = reward - expected_reward  # Advantage function\n\n    ppo_obj = ppo_update(old_probs[i], new_probs[i], advantage)\n    ppo_objectives.append(ppo_obj)\n\n    print(f\"Token: {token}\")\n    print(f\"Reward: {reward:.2f}, Advantage: {advantage:.2f}, PPO Objective: {ppo_obj:.2f}\\n\")\n\n# Trajectory PPO objective (sum over tokens in this trajectory)\ntraj_ppo_objective = sum(ppo_objectives)\nprint(f\"Total PPO Objective for this trajectory: {traj_ppo_objective:.2f}\")\n\nToken: I\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: am\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: feeling\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: great\nReward: 1.00, Advantage: 1.00, PPO Objective: 1.10\n\nToken: and\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: happy\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: today,\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: it's\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: excellent\nReward: 1.00, Advantage: 1.00, PPO Objective: 1.10\n\nTotal PPO Objective for this trajectory: 2.20\n\n\n\n\nSimple PPO Example Over Multiple Trajectories\nSo, the last step here is to consider the multiple trajectories that a model could have reasonably taken during genration, of which we sample some number.\nWe can calculate our final \\(L_{CLIP(\\text{total})}\\) as: \\[\nL_{CLIP(\\text{total})} =  \\frac{1}{N} * \\sum_i^N L_{CLIP(i)}\n\\]\nover all N trajectories we sampled.\nSo, let‚Äôs consider in this case, we sampled a second trajectory which we want to consider in our final \\(L_{CLIP(\\text{total})}\\) calculation.\n\n# for a different trajectory of tokens, each generated with some probability\nresponse_2_tokens = [\"I\", \"am\", \"angry\", \"it's\", \"awful\"]\n\nold_probs = [.9, .95, .3, .25, .2] # dummy probabilities for each word in the sequence, in old policy\nnew_probs = [.9, .95, .66, .55, .75] # dummy probabilities for each word in the sequence, in new policy\n\n# Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)\nexpected_reward = 0\n\n# Compute PPO objectives for each token\nppo_objectives = []\nfor i, token in enumerate(response_2_tokens):\n    reward = sentiment_reward(token)\n    advantage = reward - expected_reward  # Advantage function\n\n    ppo_obj = ppo_update(old_probs[i], new_probs[i], advantage)\n    ppo_objectives.append(ppo_obj)\n\n    print(f\"Token: {token}\")\n    print(f\"Reward: {reward:.2f}, Advantage: {advantage:.2f}, PPO Objective: {ppo_obj:.2f}\\n\")\n\n# Trajectory PPO objective (sum over tokens in this trajectory)\ntraj_ppo_objective2 = sum(ppo_objectives)\nprint(f\"Total PPO Objective for this trajectory: {traj_ppo_objective2:.2f}\\n\")\n\n# Final PPO objective (average over both trajectories)\ntotal_ppo_objective = np.mean([traj_ppo_objective, traj_ppo_objective2])\nprint('\\x1b[0;33;35m' + f\"Total PPO Objective for the full response: {total_ppo_objective:.2f}\" + '\\x1b[0m')\n\nToken: I\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: am\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: angry\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: it's\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: awful\nReward: -1.00, Advantage: -1.00, PPO Objective: -3.75\n\nTotal PPO Objective for this trajectory: -3.75\n\nTotal PPO Objective for the full response: -0.77\n\n\n\nSo, our total PPO objective is calculated over all potential trajectories that we sampled. In this case, that was just 2. And we can see that, even though our first trajectory seemed to look good and aligned with the behavior that we wanted, we also had a trajectory that was bad. This is why it‚Äôs important to take a sample of possible trajectories that the model could produce!\n\nüìå Note: You might also notice here that our second response got a strongly negative PPO objective. Negative rewards, leading to negative advantages, mean that the clipping won‚Äôt do anything. Since we always take the minimum of the clipped and unclipped objective, advantage * unclipped probability will always be &lt;= advantage * clipped probability. See the note in Appendix 2 about this for more details about how negative rewards are used in practice! For now, we‚Äôll roll with it.)"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#kl-divergence-in-rlhf",
    "href": "posts/RL_for_LLMs.html#kl-divergence-in-rlhf",
    "title": "RLHF for LLMs",
    "section": "KL Divergence in RLHF",
    "text": "KL Divergence in RLHF\nOne more concept to touch on is something called the Kullback-Leibler (KL) Divergence. When fine-tuning LLMs with reinforcement learning, we want to improve the model‚Äôs behavior while preventing it from deviating too drastically from its original training.\nKL divergence measures how much one probability distribution differs from another. In the context of LLMs, it quantifies the difference between the token probability distributions of two models - typically our current policy model and a reference model.\nMathematically, for two probability distributions \\(P\\) and \\(Q\\), KL divergence is defined as:\n\\[\nD_{KL}(P || Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n\\]\nFor language models, this becomes: \\[\nD_{KL}(\\pi_{\\text{new}} || \\pi_{\\text{ref}}) = \\sum_{t} \\pi_{\\text{new}}(t|c) \\log\\frac{\\pi_{\\text{new}}(t|c)}{\\pi_{\\text{ref}}(t|c)}\n\\] Where: - \\(\\pi_{\\text{new}}\\) is our current policy model - \\(\\pi_{\\text{ref}}\\) is the reference model (usually the initial model before RL training) - \\(t\\) represents tokens, generated given - \\(c\\) the context/prompt\nFrequently, KL divergence is included as an explicit penalty in our reward function, preventing the model from:\n\nForgetting its pre-trained knowledge\nAdopting degenerate patterns to maximize reward\nStraying too far from human-like text generation\n\nIn practice, we typically add a KL penalty term to our reward: \\[\nr_{\\text{total}} = r_{\\text{original}} - \\beta \\cdot D_{KL}(\\pi_{\\text{new}} || \\pi_{\\text{ref}})\n\\]\nWhere \\(\\beta\\) controls the strength of the penalty. This creates a balance between optimizing for rewards and maintaining the model‚Äôs original behavior.\n ## üìå One Final Note on PPO\nYou should now understand how the PPO is calculated and implemented fairly well. And while PPO is a very popular algorithm for reinforcement learning, and what we‚Äôll go on to use in this notebook, you should know that it‚Äôs far from the only one. There are many other choices of update strategy that can be used, and while it‚Äôs out of the scope of this notebook to go into all of them, I‚Äôll leave a few links here for some other popular choices, that you may be interested to look into now that you know about PPO.\nTrust Region Policy Optimization (TRPO)\nDirect Preference Optimization (DPO)"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#how-ppo-updates-a-model-in-practice",
    "href": "posts/RL_for_LLMs.html#how-ppo-updates-a-model-in-practice",
    "title": "RLHF for LLMs",
    "section": "How PPO Updates a Model in Practice",
    "text": "How PPO Updates a Model in Practice\nSo, we know how the PPO Objective is calculated. But how exactly is that implemented during training to update the weights of the model and change its behavior?\nWell, it‚Äôs simple! The PPO objective is directly applied as the loss function, so gradient descent directly optimizes this function by computing\n\\[\n\\frac{\\partial L_{CLIP}}{\\partial\\theta}\n\\]\nduring backpropogation to update the model‚Äôs weights.\n\n\nWhat Defines the Old and New Policy?\nAs we already discussed, PPO relies on comparing an old policy with a new policy and determining if the new policy is favorable.\nThe old policy (\\(\\pi_{\\theta(old)}\\)) is the model before applying the PPO update. The new policy (\\(\\pi_\\theta\\)) is the model after we‚Äôve updated it using PPO.\nBut you might be asking, how do we get old and new policies? What stages of training to they correspond to? The new policy is the one that we just got via gradient descent. And the old policy is from the previous update.\nConsider going through an epoch of updating a model using PPO. The typical PPO ‚Äúiteration‚Äù (a high-level loop): - Set Old Policy: Copy your current model weights as ‚Äúold policy.‚Äù - Gather Data: Roll out the environment using the old policy. - Compute Rewards & Advantages: Based on the data from step 2. - Run Multiple Mini-Batch Updates: Each update modifies the model from (\\(\\pi_{\\theta(old)}\\)) to (\\(\\pi_{\\theta}\\))\nAfter these updates finish, your ‚Äúnew policy‚Äù (\\(\\pi_{\\theta}\\)) is typically quite different from (\\(\\pi_{\\theta(old)}\\)).\nThen at the start of the next iteration, you set old policy = new policy (the final model from last iteration), gather fresh data, do more updates, repeat.\nIn many PPO codebases, they‚Äôll say ‚Äúwe do \\(K\\) epochs per iteration.‚Äù Those ‚Äúepochs‚Äù just mean \\(K\\) passes of gradient descent on the same collected batch. Each pass changes the policy slightly, but it‚Äôs all within a single iteration.\n\nüìå Note: As you might have guessed, this is a little bit more nuanced at the start of an iteration, as we don‚Äôt really yet have a \\(\\pi_\\theta\\) to act as the new policy yet! So for the first step of every iteration, \\(\\pi_\\theta = \\pi_{\\theta(old)}\\) and it updates as soon as it‚Äôs seen some of the data. See Appendix 3 for a much more in-depth discussion of this, including why mini-batches can improve learning with PPO over i.e full stochastic gradient descent."
  },
  {
    "objectID": "posts/RL_for_LLMs.html#an-example-in-code",
    "href": "posts/RL_for_LLMs.html#an-example-in-code",
    "title": "RLHF for LLMs",
    "section": "An Example in Code",
    "text": "An Example in Code\nOk. We‚Äôre ready to put together a full simple pipeline of doing reinforcement learning on an LLM!\nWe‚Äôll make use of the trl library to handle the PPO part. We‚Äôll see below how this library handles all of the ‚Äúdifficult‚Äù parts of implementing reinforcement learning for us.\n\nüìå Note: In this tutorial, we are using a downgraded version of trl, 0.10.1. I found this version was easier to use and understand the code for. Make sure if you read the docs while going through this tutorial to switch them to that version, as more updated versions changed syntax somewhat drastically!\n\n\nDefining the Reward\nWe‚Äôre going to use a very simple reward function for this example, similar to the sentiment_reward that we used earlier in this tutorial, but improve it slightly and make some changes to make it easier to learn.\nThe changes we‚Äôll make: - Assign word-based values for postive-ness and negative-ness, so some words get higher rewards than others - Multiply our positive score by 5 to get stronger bias towards positive words - Add a small positive bias for all sentences (0.1) so our reward is always positive. This makes for more stable training generally.\n\n# Define improved sentiment reward function\ndef get_sentiment_reward(text: str) -&gt; float:\n    positive_words = {\n        'good': 1, 'great': 2, 'excellent': 3, 'wonderful': 3, 'amazing': 3,\n        'happy': 2, 'glad': 1, 'love': 3, 'like': 1, \"awesome\": 2,\n        \"fun\": 2, \"super\": 2, \"incredible\": 3, 'perfect': 3\n    }\n\n    negative_words = {\n        'bad': 1, 'awful': 2, 'terrible': 3, 'angry': 2, 'horrible': 3,\n        'lame': 1, 'hate': 3, \"gross\": 2, 'sad': 1, 'upset': 2\n    }\n\n    words = text.lower().split()\n    positive_score = sum(positive_words.get(word, 0) for word in words)\n    negative_score = sum(negative_words.get(word, 0) for word in words)\n\n    # Simple calculation with positive bias\n    reward = (positive_score * 5) - negative_score\n    return max(0.1, float(reward))  # Ensure minimum positive reward\n\n\n\nLoading A Model\nHere, we‚Äôll use GPT-2. This is a small LLM that generates text in a completion way (i.e it will finish sentences for you, not respond to what you say).\nWe will load this with AutoModelForCausalLMWithValueHead, which loads both an autoregressive model with a value head in addition to the language model head. The value head is then a trainable determination of the expected value of the model‚Äôs output.\ntrl will also want a ref_model: A copy of the original model, used to compare the trained model‚Äôs outputs against a reference. This reference model is used to compute KL divergence to prevent the model from deviating too much from its initial behavior.\nFinally, we load the tokenizer, which will turn our text into numbers the model can understand, and vice versa.\nThen, we define some generation arguments. These parameters control some more fine-grained details about how text is generated during training, and how we explore different text options during generation. In particular, these arguments determine how the model chooses a response to a prompt: - top_k: 0 When set to 0, there‚Äôs no limit on how many possible next tokens the model considers. If set to a value like 50, the model would only consider the 50 most likely next tokens and ignore all others. With top_k: 0, all possible tokens remain candidates, even unlikely ones. - top_p: 1.0 This controls ‚Äúnucleus sampling‚Äù (also called ‚Äúcumulative probability truncation‚Äù). A value of 1.0 means the model considers all tokens whose cumulative probability adds up to 100%. If set to 0.9, the model would only consider tokens whose cumulative probability adds up to 90%, effectively filtering out the long tail of unlikely tokens. - do_sample: True This determines whether the model uses sampling (probabilistic selection) or greedy decoding: - When True: The model randomly selects the next token based on the probability distribution, allowing for creativity and variation - When False: The model always picks the single most likely token (greedy decoding), leading to more predictable but potentially repetitive output\nThese parameters allow the model to generate more diverse outputs for a given prompt, increasing the exploration.\n\n# 1. load a pretrained model - with clear device placement\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nmodel.to(device)\n\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nref_model.to(device)\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ngeneration_kwargs = {\n    \"min_length\": -1,         # No minimum length constraint\n    \"top_k\": 0,               # No top-k filtering (consider all tokens)\n    \"top_p\": 1.0,             # No nucleus sampling (consider all tokens)\n    \"do_sample\": True,        # Use sampling rather than greedy decoding\n    \"pad_token_id\": tokenizer.eos_token_id,  # Pad with EOS token\n    \"max_new_tokens\": 15,     # Generate at most 15 new tokens, will help speed up training\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up the PPO Configuration\nThe PPO Trainer also allows certain configuration parameters. We‚Äôll set some simple ones, but mostly leave this to the defaults. The learning rate is important here - we choose something quite small to keep training stable.\nThen, PPOTrainer just needs this config, the model, reference model, and tokenizer to eventually run the full PPO pipeline!\n\n# 2. initialize trainer with minimal parameters\nppo_config = {\n    \"mini_batch_size\": 1,     # Process one example at a time\n    \"batch_size\": 1,          # Total batch size for one optimization step\n    \"learning_rate\": 5e-6,    # Learning rate for optimizer\n    \"log_with\": None,         # No external logging\n}\n\nconfig = PPOConfig(**ppo_config)\nppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)\n\n\n\nGetting a Starting Point\nLet‚Äôs start by defining some simple prompts that should prompt positive responses. We‚Äôll see how GPT-2 performs out of the box on these prompts by checking the reward that we get from its default responses.\nYou‚Äôll see below, GPT-2 isn‚Äôt exactly the most eloquent or coherent model. That‚Äôs ok! We‚Äôre really just trying to train it to give us a bunch of postive words anyway, which is simple enough of a task that it should learn it fine!\n\n# Training prompts\nprompts = [\n    \"I feel happy when\",\n    \"The best part about this is\",\n    \"I love how\",\n    \"Today was great because\",\n]\n\n# Before training outputs\nprint(\"\\n=== Before Training Outputs ===\")\norig_responses = {}\norig_rewards = []\n\nfor prompt in prompts:\n    query_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        # Use the base model for generation to avoid CUDA errors\n        response = model.generate(\n            query_tensor,\n            **generation_kwargs\n        )\n    response_txt = tokenizer.decode(response[0])\n    orig_responses[prompt] = response_txt\n    reward_value = get_sentiment_reward(response_txt)\n    orig_rewards.append(reward_value)\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Response: {response_txt}\")\n    print(f\"Reward: {reward_value}\")\n\nprint(f\"\\nAverage initial reward: {np.mean(orig_rewards):.2f}\")\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\n\n=== Before Training Outputs ===\n\nPrompt: I feel happy when\nResponse: I feel happy when she is interested in contributing.\"\n\nWhen asked about others calling out the\nReward: 10.0\n\nPrompt: The best part about this is\nResponse: The best part about this is you can make it very nationalful. Again, it's all true.\nReward: 0.1\n\nPrompt: I love how\nResponse: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their\nReward: 15.0\n\nPrompt: Today was great because\nResponse: Today was great because we knew we could create more labor for people,\" Smith said. \"And\nReward: 10.0\n\nAverage initial reward: 8.78\n\n\n\n\nSetting up the Training Loop\nThis is the main loop where we‚Äôll generate responses, calculate rewards, and update the model using PPO. The logic will be as follows:\nFor each epoch &gt; For each of our prompts &gt;&gt; Tokenize the prompt &gt;&gt; &gt;&gt; Generate a response from the current version of the model (new policy) &gt;&gt; &gt;&gt; Calculate the reward of that response &gt;&gt; &gt;&gt; Give the PPO trainer the prompt, response, and corresponding reward\nThe magic really happens here: Give the PPO trainer the prompt, response, and corresponding reward. The trl library and PPOTrainer that we set up are aware of the model, reference model, and tokenizer. This step handles using the reward we passed in, as well as the prompt + response that corresponded to that reward to: - Calculate probabilities: Computes how likely the generated tokens were under both the current model (new policy) and reference model (old policy) - Compute advantage: Determines how much better or worse the generated response performed compared to what was expected - Apply the PPO objective: Uses the clipped PPO objective function to limit how much the model changes in a single step - Perform backpropagation: Updates the model weights to make high-reward responses more likely in the future - Update the value function: The value function (which is part of the model from AutoModelForCausalLMWithValueHead) estimates the expected reward for a given state is also updated. - Enforce KL divergence: Ensures the new policy doesn‚Äôt deviate too far from the old policy, maintaining coherent text generation - Return statistics: Provides metrics about the update like loss values, KL divergence, and entropy for monitoring the training process\nThe line: stats = ppo_trainer.step([query[0]], [response[0]], [rewards])) encapsulates the core RL algorithm that enables the model to learn from the reward signal. It‚Äôs where the model actually learns to adjust its probability distribution to favor token sequences that lead to higher sentiment rewards. And trl does all of the hard stuff in the background for us!\n\nprint(\"\\n=== Starting Training ===\")\n\n# Prepare for tracking training statistics\nepoch_stats = defaultdict(list)\nepoch_rewards = []\n\n# Run for multiple epochs\nnum_epochs = 12\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}\")\n    epoch_reward = 0\n    epoch_responses = []\n\n    # Shuffle prompts each epoch for better generalization\n    np.random.shuffle(prompts)\n\n    # Process each prompt\n    for prompt in prompts:\n          # Encode the prompt\n          encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n          # Generate a response using the current model\n          with torch.no_grad():\n              response = model.generate(\n                  encoded_prompt,\n                  **generation_kwargs\n              )\n\n          # Decode the response\n          response_txt = tokenizer.decode(response[0])\n\n          # Calculate reward for this response\n          reward_value = get_sentiment_reward(response_txt)\n          rewards = torch.tensor([reward_value], dtype=torch.float, device=device)\n\n          # Store for reporting\n          epoch_reward += reward_value\n          epoch_responses.append((prompt, response_txt, reward_value))\n\n          # Prepare tensors for PPO step\n          # PPO requires specific tensor shapes\n          query = encoded_prompt[0].unsqueeze(0)  # Reshape for PPO\n          response = response[0].unsqueeze(0)     # Reshape for PPO\n\n          # Train step - update model using PPO\n          stats = ppo_trainer.step([query[0]], [response[0]], [rewards])\n\n          # Track training metrics\n          for k, v in stats.items():\n              if v is not None:\n                  epoch_stats[k].append(v)\n\n    # Calculate and report epoch statistics\n    if epoch_responses:\n        avg_reward = epoch_reward / len(epoch_responses)\n        epoch_rewards.append(avg_reward)\n\n        print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n        print(f\"Average Reward: {avg_reward:.2f}\")\n\n        # Print reward trend\n        if epoch &gt; 0:\n            reward_change = avg_reward - epoch_rewards[-2]\n            print(f\"Reward Change: {reward_change:+.2f}\")\n\n        # Print sample responses\n        print(\"\\nSample responses from this epoch:\")\n        for i, (prompt, response, reward) in enumerate(epoch_responses[:2]):\n            print(f\"Prompt: {prompt}\")\n            print(f\"Response: {response}\")\n            print(f\"Reward: {reward:.2f}\")\n    else:\n        print(\"No successful training steps this epoch\")\n\n\n=== Starting Training ===\n\nEpoch 1\n\n--- Epoch 1 Summary ---\nAverage Reward: 11.25\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I don't think I'd rule it out because I'm very fortunate to\nReward: 15.00\nPrompt: Today was great because\nResponse: Today was great because to see all the flames in the air. It was a whirlwind of congressional\nReward: 10.00\n\nEpoch 2\n\n--- Epoch 2 Summary ---\nAverage Reward: 13.78\nReward Change: +2.53\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how tragic this was\n\nI love it so much I feel like the tornado\nReward: 35.00\nPrompt: The best part about this is\nResponse: The best part about this is that, from a human point of view, all models bear all of the\nReward: 0.10\n\nEpoch 3\n\n--- Epoch 3 Summary ---\nAverage Reward: 17.52\nReward Change: +3.75\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when I keep seeing Lisp. I like it. It's a joy to work\nReward: 15.00\nPrompt: I love how\nResponse: I love how they have tried. I love that they wore their mark on their hat with\nReward: 30.00\n\nEpoch 4\n\n--- Epoch 4 Summary ---\nAverage Reward: 12.53\nReward Change: -5.00\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how you make method shots because I love orange funk and you've grown me up\nReward: 30.00\nPrompt: I feel happy when\nResponse: I feel happy when I have another large cock in my hips\n\nLatelose: Acting\nReward: 10.00\n\nEpoch 5\n\n--- Epoch 5 Summary ---\nAverage Reward: 12.53\nReward Change: +0.00\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when I I I I I I I I I I I I I I I\nReward: 10.00\nPrompt: Today was great because\nResponse: Today was great because they ate a little purple cabbage or rib eye of deind. Truly Complete\nReward: 10.00\n\nEpoch 6\n\n--- Epoch 6 Summary ---\nAverage Reward: 8.78\nReward Change: -3.75\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when my rest rest becomes not difficult... But when my recovery growsnt so urgent\nReward: 10.00\nPrompt: I love how\nResponse: I love how much multiplex are haunted areas. How pipe, wood, torches, etc\nReward: 15.00\n\nEpoch 7\n\n--- Epoch 7 Summary ---\nAverage Reward: 13.78\nReward Change: +5.00\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how they work with us,\" Barzero argues with both hands. And they are\nReward: 15.00\nPrompt: I feel happy when\nResponse: I feel happy when my sleep. I feel happy when I I I be happy uncertain uncertain uncertain\nReward: 30.00\n\nEpoch 8\n\n--- Epoch 8 Summary ---\nAverage Reward: 16.27\nReward Change: +2.50\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how it. Ourld literally bind for a happy it to per chance or for\nReward: 25.00\nPrompt: The best part about this is\nResponse: The best part about this is long hair:\nNote:You can choose between option:UnlockedSince\nReward: 0.10\n\nEpoch 9\n\n--- Epoch 9 Summary ---\nAverage Reward: 8.78\nReward Change: -7.50\n\nSample responses from this epoch:\nPrompt: Today was great because\nResponse: Today was great because we are currently loving and embracing being children. We are finding joy in understanding\nReward: 10.00\nPrompt: I feel happy when\nResponse: I feel happy when we were even able to get close to our spiritual helper.\n\nSo\nReward: 10.00\n\nEpoch 10\n\n--- Epoch 10 Summary ---\nAverage Reward: 10.03\nReward Change: +1.25\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I look at how I look at grabbing at back grabbing at back grabbing at\nReward: 15.00\nPrompt: I feel happy when\nResponse: I feel happy when you g m t,\n\nJ\n\nacje\n\nYes\nReward: 10.00\n\nEpoch 11\n\n--- Epoch 11 Summary ---\nAverage Reward: 10.00\nReward Change: -0.03\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I I I I I I I I IIIIIII\nReward: 15.00\nPrompt: Today was great because\nResponse: Today was great because malarkey women and men changed.The grateful ones had romantic view;\nReward: 10.00\n\nEpoch 12\n\n--- Epoch 12 Summary ---\nAverage Reward: 15.03\nReward Change: +5.03\n\nSample responses from this epoch:\nPrompt: Today was great because\nResponse: Today was great because of the freshandfreshlight You've commanduedTheOfGoodTheAvoid\nReward: 10.00\nPrompt: The best part about this is\nResponse: The best part about this is that on the gazelles ‚Äì I used to use them ‚Äì they\nReward: 0.10\n\n\n\n\nSeeing How We Did\nFinally, we can look a bit deeper to see how well we did. Let‚Äôs investigate in more depth: - Before vs After Comparison: For each prompt, we‚Äôll compare the original model‚Äôs response with our RL-trained model‚Äôs response. This direct comparison helps us visualize the specific changes in text generation. - Individual Reward Metrics: We calculate the sentiment reward for both the original and trained responses. The difference between these scores shows how much our model has improved at generating positive text. - Aggregate Improvement: By averaging rewards across all prompts, we can quantify the overall improvement from RL training. A positive change indicates successful optimization toward our sentiment objective.\n\n# Compare before/after\nprint(\"\\n=== After Training Outputs ===\")\nfinal_rewards = []\n\nfor prompt in prompts:\n    try:\n        # Generate using the standard method to avoid errors\n        encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            response = model.generate(\n                encoded_prompt,\n                **generation_kwargs\n            )\n        response_txt = tokenizer.decode(response[0])\n        reward_value = get_sentiment_reward(response_txt)\n        final_rewards.append(reward_value)\n\n        # Compare with original\n        orig_reward = get_sentiment_reward(orig_responses[prompt])\n\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Before: {orig_responses[prompt]}\")\n        print(f\"After: {response_txt}\")\n        print(f\"Reward Before: {orig_reward:.2f}\")\n        print(f\"Reward After: {reward_value:.2f}\")\n        print(f\"Improvement: {reward_value - orig_reward:+.2f}\")\n    except Exception as e:\n        print(f\"Error evaluating prompt '{prompt}': {e}\")\n\n# Print final stats\nif final_rewards:\n    print(\"\\n=== Training Results ===\")\n    print(f\"Starting Average Reward: {np.mean(orig_rewards):.2f}\")\n    print(f\"Ending Average Reward: {np.mean(final_rewards):.2f}\")\n    print(f\"Change: {np.mean(final_rewards) - np.mean(orig_rewards):+.2f}\")\n\n\n=== After Training Outputs ===\n\nPrompt: I feel happy when\nBefore: I feel happy when she is interested in contributing.\"\n\nWhen asked about others calling out the\nAfter: I feel happy when ordinary people are free and all my woes are banished because of quite full of\nReward Before: 10.00\nReward After: 10.00\nImprovement: +0.00\n\nPrompt: The best part about this is\nBefore: The best part about this is you can make it very nationalful. Again, it's all true.\nAfter: The best part about this is the paperwork. Write the cards up front, step by step, step by\nReward Before: 0.10\nReward After: 0.10\nImprovement: +0.00\n\nPrompt: I love how\nBefore: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their\nAfter: I love how Snowman looks too when she looks like Rocky Simon Newell. I love\nReward Before: 15.00\nReward After: 35.00\nImprovement: +20.00\n\nPrompt: Today was great because\nBefore: Today was great because we knew we could create more labor for people,\" Smith said. \"And\nAfter: Today was great because of was amazing and appreciated and astounding were all the immense and unrelasibility\nReward Before: 10.00\nReward After: 25.00\nImprovement: +15.00\n\n=== Training Results ===\nStarting Average Reward: 8.78\nEnding Average Reward: 17.52\nChange: +8.75\n\n\nAnd there you have it! We sucessfully trained GPT-2 to give us more positive words in its responses.\nNow, of course, we used a super simple reward here, and not a particularly good model (no offense, GPT-2), so we can see a lot of repeated words contributing to that positive response. As we discussed, in reality, a reward function should be more complicated, and our prompts used for training should be much more diverse than just 4. Still, you now know how to set up a PPO training pipeline!"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#collecting-human-feedback",
    "href": "posts/RL_for_LLMs.html#collecting-human-feedback",
    "title": "RLHF for LLMs",
    "section": "Collecting Human Feedback",
    "text": "Collecting Human Feedback\nIn RLHF, human judgments about model outputs are collected. Unlike our algorithmic reward function that automatically calculated a score, RLHF relies on actual human preferences.\nTypically, a real RLHF pipeline will involve humans doing pairwise comparisons of responses. Rather than asking humans to provide absolute scores, RLHF typically uses comparative judgments where annotators choose which of two responses they prefer. This involves: - Prompt Selection: A diverse set of prompts is created to cover different topics, skills, and potential failure modes. - Response Generation: For each prompt, the model generates multiple responses using different sampling parameters. - Human Annotation: Human annotators are presented with a prompt and two model-generated responses, then asked to select which one is better according to specific criteria.\n\n# Simple simulation of human preference collection interface\ndef collect_human_preference(prompt, response_a, response_b):\n    print(f\"Prompt: {prompt}\\n\")\n    print(f\"Response A:\\n{response_a}\\n\")\n    print(f\"Response B:\\n{response_b}\\n\")\n\n    while True:\n        choice = input(\"Which response do you prefer? (A/B/Tie): \").upper()\n        if choice in [\"A\", \"B\", \"TIE\"]:\n            return choice\n        print(\"Invalid input. Please enter A, B, or Tie.\")\n\n# Example prompts and responses\nexamples = [\n    {\n        \"prompt\": \"Explain the concept of reinforcement learning to a high school student.\",\n        \"response_a\": \"Reinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes.\",\n        \"response_b\": \"Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward.\"\n    },\n    {\n        \"prompt\": \"What are some ways to reduce stress?\",\n        \"response_a\": \"Reducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing.\",\n        \"response_b\": \"To reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system.\"\n    }\n]\n\n# Run the preference collection for demonstration\ncollected_preferences = []\nfor i, example in enumerate(examples):\n    print(f\"\\n===== Example {i+1} =====\")\n    preference = collect_human_preference(\n        example[\"prompt\"],\n        example[\"response_a\"],\n        example[\"response_b\"]\n    )\n    collected_preferences.append({\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": \"response_a\" if preference == \"A\" else \"response_b\" if preference == \"B\" else \"tie\",\n        \"rejected\": \"response_b\" if preference == \"A\" else \"response_a\" if preference == \"B\" else \"tie\"\n    })\n    print(f\"You preferred Response {preference}\")\n\n\n===== Example 1 =====\nPrompt: Explain the concept of reinforcement learning to a high school student.\n\nResponse A:\nReinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes.\n\nResponse B:\nReinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward.\n\nWhich response do you prefer? (A/B/Tie): B\nYou preferred Response B\n\n===== Example 2 =====\nPrompt: What are some ways to reduce stress?\n\nResponse A:\nReducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing.\n\nResponse B:\nTo reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system.\n\nWhich response do you prefer? (A/B/Tie): B\nYou preferred Response B\n\n===== Collected Preferences =====\nExample 1: You chose the response_b over the response_a\nExample 2: You chose the response_b over the response_a\n\n\nWhen collecting real human feedback, annotators are typically given specific criteria to evaluate, like:\n\nHelpfulness: How well does the response address the user‚Äôs request?\nTruthfulness: Is the information accurate and factual?\nHarmlessness: Does the response avoid harmful, offensive, or misleading content?\nClarity: Is the response clearly written and easy to understand?"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#reward-model-training",
    "href": "posts/RL_for_LLMs.html#reward-model-training",
    "title": "RLHF for LLMs",
    "section": "Reward Model Training",
    "text": "Reward Model Training\nWhile you get direct feedback this way, it would be difficult to collect real human feedback over all responses that you could possibly reasonably expect. So instead, a reward model is often trained to predict what humans like better. Better responses usually have some things in common, they‚Äôre maybe: - wordier - friendlier - more factual\nSo these more generalized properties can be learned, and then applied to prompts en masse, rather than needs humans to look at hundreds of thousands of responses individually. The reward model bridges the gap between collected human judgments and automated rewards needed for reinforcement learning.\n\nConverting Preferences to a Reward Model\nThe reward model is essentially a classifier trained to predict human preferences. It takes in a prompt and response, and outputs a scalar value representing the ‚Äúquality‚Äù of the response according to human preferences.\nIf we define:\n\\((x, y_w, y_l)\\) as a triplet where \\(x\\) is the prompt, \\(y_w\\) is the preferred (winning) response, and \\(y_l\\) is the less preferred (losing) response,\nand\n\\(r_\\theta(x, y)\\) as our reward model with parameters \\(\\theta\\) that outputs a scalar reward for prompt \\(x\\) and response \\(y\\).\nThen, we train the reward model to maximize the log probability of the human preferences: \\[\n{L}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]\n\\]\n\n\\({L}(\\theta)\\) is the loss function we‚Äôre trying to minimize, where \\(\\theta\\) represents all the parameters of our reward model.\n\\(\\mathbb{E}_{(x, y_w, y_l) \\sim D}\\) is the expected value over all triplets sampled from our dataset D. In simpler terms, it means ‚Äúthe average across all our training examples.‚Äù\n\\(r_\\theta(x, y_w)\\) is the reward score our model assigns to the winning (preferred) response \\(y_w\\) given prompt \\(x\\).\n\\(r_\\theta(x, y_l)\\) is the reward score our model assigns to the losing (less preferred) response \\(y_l\\) given the same prompt \\(x\\).\n\\(\\sigma(z)\\) is the sigmoid function, defined as \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), which maps any real number to a value between 0 and 1.\n\nThe equation expresses a Bradley-Terry model, which is used for pairwise comparisons. For each pair of responses, we compute the difference in their rewards: \\(r_\\theta(x, y_w) - r_\\theta(x, y_l)\\). We pass this difference through a sigmoid function, which gives us the probability that the model correctly predicts the human preference. We want to maximize this probability, which is equivalent to minimizing the negative log probability (since loss functions want to be minimized). Then, we average this loss across all training examples.\n\n\nTraining a Reward Model\nThen, we can train a reward model based on our triplets \\((x, y_w, y_l)\\).\nBelow is a very simplified example. Instead of a real neural network, we just use a function simulate_reward_scores to see how we could calculate the loss based on the preferences indicated in the last code cell. This function simply assigns a score to a response based purely on its length.\nIn reality, this calculated loss would help a model readjust its predicted rewards.\n\n# Simulate a reward model's outputs\n# In reality, these would come from a neural network\ndef simulate_reward_scores(response_a, response_b):\n    \"\"\"Simulate reward scores for demonstration purposes\"\"\"\n    # Just a simple length-based score for demonstration\n    score_a = 0.5 + 0.01 * len(response_a)\n    score_b = 0.5 + 0.01 * len(response_b)\n    return {\"response_a\": score_a, \"response_b\": score_b}\n\n# Calculate reward model loss\ndef reward_model_loss(scores, chosen, rejected):\n    \"\"\"Calculate the loss based on preference pair and model scores\"\"\"\n    chosen_score = scores[chosen]\n    rejected_score = scores[rejected]\n\n    # Print scores for demonstration\n    print(f\"Chosen response score: {chosen_score:.4f}\")\n    print(f\"Rejected response score: {rejected_score:.4f}\")\n    print(f\"Score difference (chosen - rejected): {chosen_score - rejected_score:.4f}\")\n\n    # The core loss function: -log(sigmoid(chosen_score - rejected_score))\n    # This encourages the model to give the preferred response a higher score\n    sigmoid = 1 / (1 + math.exp(-(chosen_score - rejected_score)))\n    loss = -math.log(sigmoid)\n\n    return loss\n\n# Using data from our previously collected human preferences\nfor i, preference in enumerate(collected_preferences):\n    example = examples[i]\n    prompt = example[\"prompt\"]\n    response_a = example[\"response_a\"]\n    response_b = example[\"response_b\"]\n    chosen = preference[\"chosen\"]\n    rejected = preference[\"rejected\"]\n\n    print(f\"\\n===== Example {i+1}: {prompt} =====\")\n    print(f\"You preferred: {chosen}\")\n\n    # Initial model scoring (before training)\n    scores = simulate_reward_scores(response_a, response_b)\n    loss = reward_model_loss(scores, chosen, rejected)\n    print(f\"Loss: {loss:.4f}\")\n\n\n===== Example 1: Explain the concept of reinforcement learning to a high school student. =====\nYou preferred: response_b\nChosen response score: 2.8900\nRejected response score: 3.2500\nScore difference (chosen - rejected): -0.3600\nLoss: 0.8893\n\n===== Example 2: What are some ways to reduce stress? =====\nYou preferred: response_b\nChosen response score: 2.8900\nRejected response score: 2.0000\nScore difference (chosen - rejected): 0.8900\nLoss: 0.3441"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#rlhf-pipeline-implementation",
    "href": "posts/RL_for_LLMs.html#rlhf-pipeline-implementation",
    "title": "RLHF for LLMs",
    "section": "RLHF Pipeline Implementation",
    "text": "RLHF Pipeline Implementation\nOnce trained, the reward model replaces the handcrafted reward function we used earlier. During RL training:\n\nThe model generates a response to a prompt\nThe reward model evaluates the response, producing a scalar reward\nThis reward is used to update the model via PPO, just as we did with our algorithmic reward\n\nSo, where before we had\nreward = get_sentiment_reward(response_txt)  # From our sentiment detection function\nNow with RLHF:\nreward = reward_model(prompt, response_txt)  # From our trained reward model\nWhile this change may look simple, it fundamentally transforms how the system learns - from optimizing for predefined metrics to optimizing for learned human preferences.\nSo, the necessary steps for a complete RLHF pipeline include:\n\nInitial LLM Training: Train or fine-tune a base LLM using standard methods\nHuman Preference Collection: Gather human judgments on model outputs\nReward Model Training: Train a reward model to predict human preferences\nRL Fine-tuning: Use the reward model to guide policy optimization"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#practical-considerations-for-scaling-rlhf",
    "href": "posts/RL_for_LLMs.html#practical-considerations-for-scaling-rlhf",
    "title": "RLHF for LLMs",
    "section": "Practical Considerations for Scaling RLHF",
    "text": "Practical Considerations for Scaling RLHF\nImplementing RLHF at scale involves several important considerations:\nQuality of Human Feedback: Diverse annotator pools to avoid bias, clear guidelines to ensure consistency, and quality control measures to identify unreliable annotations can all enhance the quality of the data that trains the reward model.\nComputational Requirements: Training a reward model adds another large model to the pipeline, and PPO fine-tuning is more compute-intensive than supervised fine-tuning. Multiple runs may also be needed to find optimal hyperparameters.\nReward Hacking: Models can also learn to exploit weaknesses in the reward model just like they can from an algorithmic model, so it‚Äôs important to regularly update the reward model with new human judgments. Adding KL penalties can help to prevent excessive deviation from the base model.\nDistribution Shift: As the policy model improves, it generates responses outside the reward model‚Äôs training distribution. Iterative approaches that collect new human feedback on improved model outputs help address this.\nHybrid Approaches: Combining RLHF with rule-based rewards for certain constraints can improve overall quality. Multi-objective optimization can balance different desired qualities, and ensemble reward models are sometimes used to capture different aspects of human preferences.\nIn practice, RLHF is often implemented as an iterative process:\n\nTrain initial reward model from human preferences\nPerform RL fine-tuning using this reward model\nGenerate new responses with the improved policy\nCollect new human preferences on these responses\nRetrain or update the reward model 6.Repeat the process\n\nThis iterative approach helps address distribution shift and ensures the reward model keeps pace with policy improvements."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Tutorials Blog!",
    "section": "",
    "text": "During my Ph.D., I utilized a U-Net to attempt to learn the final distribution of dark matter in a computer simulation from its initial conditions. Being in an Astrophysics Ph.D.¬†program, and the year being 2020, model architectures like the U-Net weren‚Äôt well known by people in my field, including my Ph.D.¬†advisor. He tasked me: teach him what it was, down to the details, so that he could understand the concepts at a deep enough level himself to help properly guide me on the project.\n\n\n\nRepresentation of a dark matter only simulation - the subject of my Ph.D.¬†project\n\n\nAnd thus, my first Jupyter notebook tutorial was born. In the process of writing code cells, making graphics in powerpoint, and scouring through documentation, I found an even deeper understanding of the concepts I was describing. As they say, the best way to prove you really know something is to teach it. Since then, every time I really want to understand something, I make a Jupyter notebook tutorial about it.\nIt occured to me (5 years later), that maybe someone else would care to read these. So here, you‚Äôll find a collection of the notebooks that I‚Äôve put together. Mainly to teach myself, but hopefully, to teach you too!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abbie‚Äôs Jupyter Tutorials",
    "section": "",
    "text": "RLHF for LLMs\n\n\n\n\n\n\nmachine-learning\n\n\nreinforcement-learning\n\n\nLLMs\n\n\n\nA guide to reinforcement learning as it applies to modern large language models\n\n\n\n\n\nFeb 25, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Tutorials Blog!\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/RL_for_LLMs.html#training-the-value-function",
    "href": "posts/RL_for_LLMs.html#training-the-value-function",
    "title": "RLHF for LLMs",
    "section": "Training The Value Function",
    "text": "Training The Value Function\nWe‚Äôve talked about the advantage, which is calculated: \\[\nA(t) = R(t) - V_s(t)\n\\] And how \\(V_s(t)\\), the value function that determines the reward that the model ‚Äúexpects‚Äù to receive, can generally be quite complicated.\nWell, in practice, it is often something that is trained along with adjustments to the model, because accurate \\(V_s(t)\\) estimates lead to less noisy advantage estimates and more stable training.\nThe value function can be trained just like any regression model: - We want \\(V_{\\theta}(s_t)\\) to match the actual return \\(R_t\\). - So, we use i.e.¬†a Mean Squared Error (MSE) loss: $ L_V() = ( V_(s_t) - R_t )^2 $ - And update it alongside PPO‚Äôs policy loss as a separate loss term.\nSo a real, full PPO loss function, including the value function then becomes something like: \\[\nL_{\\text{total}} = L_{\\text{CLIP}} + c_1 L_V - c_2 H[\\pi]\n\\] where: - \\(L_{\\text{CLIP}}\\) = PPO policy loss. - \\(L_V\\) = Value function loss (MSE between \\(V(s)\\) and \\(R_t\\)). - \\(H[\\pi]\\) = Optional entropy term sometimes added to encourage exploration. - \\(c_1, c_2\\) = Tunable coefficients that control the relative weights of the loss components(e.g., \\(c_1 = 0.5, c_2 = 0.01\\)).\nThat‚Äôs all that I‚Äôll say about training the value function here, because while we will train it going forward, we‚Äôre going to keep it simple. But make sure to reference Appendix 4 about value functions if you‚Äôd like to know more."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html",
    "title": "RLHF for LLMs",
    "section": "",
    "text": "In this notebook, we‚Äôll dive deep into how LLM‚Äôs are trained using reinforcement learning, exploring how we can implement this type of training on an existing, small model to better guide our desired responses!\n\n  \n\n\nIn a typical LLM training paradigm, models are first trained in an self-supervised manner to predict a masked-out word. This makes them great at understanding how language is constructed, and helps us to make giant datasets very easily.\nHowever, models trained to simply predict what word comes next are not very helpful to talk to. If I ask a model trained in this manner ‚ÄúWhat kind of tree is a cedar?‚Äù It may respond ‚ÄúI would really like to know about cedar trees.‚Äù because it thinks it should be continuing the thought.\nThe next step, then, in LLM training, is typically instruction fine tuning (IFT). In this step, models are trained to respond and follow instructions. In this step, they come to recognize when they are being asked a question, and the language produced thereafter becomes a response not simply a continuation. After IFT, a model might respond: ‚Äúconiferous‚Äù\nThe model now answers the question (hopefully correctly.) But it may not be friendly, or be too terse, or not structure its answers particularly well, or even be mean and harmful. So, the last step in training is typically reinforcement learning from human feedback (RLHF, or just RL). In this step, the model is told what responses of its are better and encouraged to respond in that type of way. While this can adjust the responses in many ways, one example is that it may encourage more information or detail, or generally more wordiness. So, a model trained with RL may respond ‚ÄúA cedar is a type of coniferous tree in the genus Cedrus. Cedars are evergreen trees known for their aromatic wood, needle-like leaves, and cones. They belong to the family Pinaceae.‚Äù\n\nüìå Note: A great textbook on reinforcement learning is Reinforcement Learning: An Introduction. I highly reccommend referencing it if, after or during this tutorial, you find yourself wanting an even deeper dive into some of these concepts."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#installations-and-imports",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#installations-and-imports",
    "title": "RLHF for LLMs",
    "section": "Installations and Imports",
    "text": "Installations and Imports\n\n# Install required packages\n!pip install -q --upgrade transformers datasets\n!pip install -q trl==0.10.1 #install downgraded version because it's easier to use!\n\n(If the below imports fail, you may need to restart the kernel for those installations to take effect).\n\n# Basic imports\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\nimport numpy as np\nimport math\nfrom datasets import Dataset\nimport warnings\nimport re\nfrom typing import Dict, List, Tuple\n\nfrom collections import defaultdict\nfrom transformers import GPT2Tokenizer\n\nimport random\n\nwarnings.filterwarnings('ignore')\n\nIf you have access to it, a GPU runtime will make this code run smoother (or might make it possible to run at all!)\nThe below code will confirm if you‚Äôre on a GPU. You want to see CUDA available: True\nIt‚Äôs not required, but preferred. A stronger CPU might be required.\n\n# Check PyTorch version and CUDA availability\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n\nPyTorch version: 2.5.1+cu124\nCUDA available: False\n\n\n\n(OPTIONAL) Hugging Face Authentication\nLater in this notebook, we‚Äôll be grabbing a model off of huggingface. While the one we use here doesn‚Äôt require a token, you can add your token here if you want to experiment with swapping GPT-2 for a different model that needs authentication.\n\nfrom huggingface_hub import login\nimport getpass\n\ntoken = getpass.getpass(\"Enter your Hugging Face token: \")\n\n# Verify login\nprint(\"Login status: Authenticated with Hugging Face\")\n\nLogin status: Authenticated with Hugging Face"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#an-introduction-from-traditional-training-to-reinforcement-learning",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#an-introduction-from-traditional-training-to-reinforcement-learning",
    "title": "RLHF for LLMs",
    "section": "An Introduction: From Traditional Training to Reinforcement Learning",
    "text": "An Introduction: From Traditional Training to Reinforcement Learning\nBefore we get started, let‚Äôs talk about what exactly the goals of RL are, how it differs from ‚Äútraditional‚Äù training, and get a basic understanding of the RL pipeline.\n\nTraditional Training: A Quick Review\nIn ‚Äútraditional‚Äù, supervised training of a transformer or neural network, which you‚Äôre likely familiar with, the process looks like this:\n\nYou have training data with inputs and known correct outputs\nThe model makes predictions\nYou calculate a loss function (like MSE or cross-entropy) that measures how wrong the predictions are\nBackpropagation updates the weights to minimize this loss\nRepeat until the model gets good at predicting correct outputs\n\nThe key here is that for every input, you know exactly what the correct output should be.\nBut what if you don‚Äôt know the exact right answer? What if you just know when answers are ‚Äúbetter‚Äù or ‚Äúworse‚Äù? This is where reinforcement learning comes in.\nConsider training an LLM to be helpful and truthful. There‚Äôs no single ‚Äúcorrect‚Äù response to a prompt - there might be many good responses and many bad ones. We can‚Äôt use traditional supervised learning because: - We don‚Äôt have examples of perfect responses - Multiple very different responses might be equally good - We care about abstract qualities (helpfulness, truthfulness) more than exact word matches\n\n\nEnter Reinforcement Learning\nRL approaches this differently:\n\nInstead of a loss function that measures ‚Äúwrongness‚Äù, we use a reward function that measures ‚Äúgoodness‚Äù\nInstead of comparing to correct answers, we try different outputs and see which get higher rewards\nInstead of direct supervision, the model learns through trial and error\n\nRL requires, at a high level:\n1. Generation Phase - Model receives a prompt - Model generates multiple different possible responses - This is called ‚Äúexploring‚Äù the space of possible outputs\n2. Evaluation Phase - Each generated response gets a reward score - Better responses = higher rewards - This tells us which outputs we want to encourage\n3. Learning Phase - Model is updated to make high-reward outputs more likely - But, it doesn‚Äôt memorize specific outputs - Instead, it learns patterns that tend to lead to high rewards\n4. Repeat - Generate new responses - Evaluate them - Learn from the results - Over time, the model gets better at generating high-reward outputs\n\n\nSome Key Differences from Traditional Training\n1. Exploration vs Exploitation - The model needs to try new things (explore) to find better strategies - But it also needs to use what it knows works (exploit) - This ‚Äúexploration-exploitation tradeoff‚Äù doesn‚Äôt exist in traditional training\n2. Delayed Rewards - Sometimes we don‚Äôt know if an output was good until several steps later - The model needs to learn which actions led to good outcomes - This is very different from immediate feedback in traditional training\n3. Moving Targets - As the model improves, it generates different outputs - These new outputs might get different rewards - So, the learning process is more dynamic than traditional fixed-dataset training"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#defining-the-reward-function",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#defining-the-reward-function",
    "title": "RLHF for LLMs",
    "section": "Defining the Reward Function",
    "text": "Defining the Reward Function\nSo, what is a reward function? In reality, it can take many forms. But in general, a reward function needs to be:\n\nClear: It should have a well-defined relationship between output quality and some numerical score\nConsistent: Similar outputs should get similar rewards\nMeaningful: Higher rewards should genuinely represent better outputs\nComputationally Feasible: As we need to calculate rewards for many outputs quickly\n\nReward functions can also incorportate negative rewards for behaviors that the model wants to explicitly avoid."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#a-very-simple-example",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#a-very-simple-example",
    "title": "RLHF for LLMs",
    "section": "A (Very) Simple Example",
    "text": "A (Very) Simple Example\nLet‚Äôs just start with a very basic example to illustrate what a simple reward function could be. Below, we‚Äôll write a ‚Äúpositive sentiment‚Äù reward function that counts how many positive and negative words were used in a response, giving positive rewards for positive words, and negative rewards for negative words.\n\n# Simple reward function\ndef sentiment_reward(text: str) -&gt; float:\n    positive_words = ['good', 'great', 'excellent', 'wonderful', 'amazing']\n    negative_words = ['bad', 'awful', 'terrible']\n\n    words = text.lower().split()\n    positive_count = sum(1 for word in words if word in positive_words)\n    negative_count = sum(1 for word in words if word in negative_words)\n\n    return positive_count - negative_count\n\n\n# Test our reward function with some example sentences\ntest_texts = [\n    \"This is a good and great day\",\n    \"Nothing particularily special happened today, but I was still satisfied\",\n    \"Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later\",\n    \"Good wow great amazing excellent stuff, wow great and good and great\"\n]\n\nprint(\"\\nTesting reward function:\")\nfor text in test_texts:\n    print(f\"\\nText: {text}\")\n    print(f\"Reward: {sentiment_reward(text):.2f}\")\n\n\nTesting reward function:\n\nText: This is a good and great day\nReward: 2.00\n\nText: Nothing particularily special happened today, but I was still satisfied\nReward: 0.00\n\nText: Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later\nReward: -1.00\n\nText: Good wow great amazing excellent stuff, wow great and good and great\nReward: 7.00\n\n\nSo, we can see that the above, while likely far too simple to actually be useful, counts as a reward function, as it meets our criteria and provides some way to understand ‚Äúbetter‚Äù responses.\nThis reward function clearly meets the requirements of: 1. Clear relationship: More positive words = higher score 2. Fast to compute: Simple word counting is very efficient 3. Easy to understand: The logic is straightforward\nHowever, we can point out some clear reasons that this would be an unuseful reward function in practice: 1. Easy to game: Model could just repeat positive words 2. Misses context: ‚ÄúNot good‚Äù counts as positive 3. Ignores quality: Well-written neutral text scores lower than poorly written positive text (see second example vs last)\nSo, a good reward function should take more into account when deciding if a whole response is good or not.\n\nOne Step Better: Adding Context\nLet‚Äôs improve our reward function by considering context. We‚Äôll: 1. Account for negations 2. Consider word positioning 3. Add penalties for repetition\n(We‚Äôll also, just for simplicity sake, move this to be positive-detecting only, no negative rewards)\n\n# Improved reward function with context awareness\ndef positivity_reward(text: str) -&gt; float:\n    # We'll keep the same (extremely incomplete) word list\n    positive_words = ['good', 'great', 'excellent', 'wonderful', 'amazing']\n\n    words = text.lower().split()\n    score = 0.0\n\n    # Check for negations (looking at pairs of words)\n    for i in range(len(words)):\n        if words[i] in positive_words:\n            # Check if previous word is a negation\n            if i &gt; 0 and words[i-1] in {'not', 'never', \"don't\", 'no'}:\n                score -= 0.5  # Penalty for negated positive words\n            else:\n                score += 1.0\n\n    # Penalty for repetition\n    unique_words = len(set(words))\n    repetition_penalty = unique_words / max(len(words), 1)\n\n    # Calculate final score with penalties\n    final_score = (score / max(len(words), 1)) * repetition_penalty\n\n    # Clip to range [0, 1]\n    return max(min(final_score, 1.0), 0.0)\n\n# Test the improved function\ntest_texts = [\n    \"This is good and helpful.\",\n    \"This is not good at all.\",\n    \"good good good good good\",\n    \"The explanation was clear and helpful, making it incredibly beneficial.\"\n]\n\nprint(\"Testing our improved reward function:\")\nfor text in test_texts:\n    reward = positivity_reward(text)\n    print(f\"\\nText: {text}\")\n    print(f\"Reward: {reward:.3f}\")\n\nTesting our improved reward function:\n\nText: This is good and helpful.\nReward: 0.200\n\nText: This is not good at all.\nReward: 0.000\n\nText: good good good good good\nReward: 0.200\n\nText: The explanation was clear and helpful, making it incredibly beneficial.\nReward: 0.000\n\n\nSo, we‚Äôve definitely improved things! But you can see how easy pitfalls are. The last example was definitely positive in tone, but that‚Äôs not accounted for here. We could add ‚Äúclear‚Äù, ‚Äúhelpful‚Äù and ‚Äúbeneficial‚Äù to our positive words list, but then how many words would need to be added just because sometimes, in the right context, they are positive? Perhaps ‚Äúincredibly‚Äù is the thing giving this sentence a positive connotation, but then that could also be used to say something is ‚Äúincredibly‚Äù negative.\nYou can see how reward functions we use in practice require some careful considerations."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#real-world-reward-functions",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#real-world-reward-functions",
    "title": "RLHF for LLMs",
    "section": "Real-World Reward Functions",
    "text": "Real-World Reward Functions\nIn practice, good reward functions for LLMs often combine multiple components, as there are a lot of factors to consider when deeming a response as ‚Äúgood‚Äù:\n1. Quality Metrics measure for things like grammar and fluency, relevance of the response to the given prompt, factual accuracy, and overall coherence and structure.\n2. Task-Specific Metrics measure for things like format adherence, style matching, any domain-specific requirements, and length constraints\n3. Safety and Alignment Metrics include toxicity detection and bias measurements, and check for things like truthfulness and helpfulness.\n\nA More Practical Reward Function\nLet‚Äôs see how we can implement some of those metrics above and build a (still very basic, but more realistic) reward function that would be more useful in practice than our ‚Äúpositive words‚Äù detection.\n\ndef calculate_rewards(prompt: str, response: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate multiple reward components for a given response.\n    Returns a dictionary of different reward aspects.\n    \"\"\"\n    rewards = {}\n\n    # 1. Response Length Reward\n    # Encourage responses between 50 and 500 words\n    words = response.split()\n    word_count = len(words)\n    length_reward = min(1.0, word_count / 50.0) if word_count &lt; 50 else \\\n                   1.0 if 50 &lt;= word_count &lt;= 500 else \\\n                   max(0.0, 1.0 - (word_count - 500) / 500)\n    rewards['length'] = length_reward\n\n    # 2. Prompt Relevance Reward\n    # Check if response uses key terms from prompt\n    prompt_words = set(prompt.lower().split())\n    response_words = set(response.lower().split())\n    overlap = len(prompt_words.intersection(response_words))\n    relevance_reward = min(1.0, overlap / max(len(prompt_words), 1))\n    rewards['relevance'] = relevance_reward\n\n    # 3. Format Quality Reward\n    # Check for good formatting practices\n    format_scores = []\n    # Has paragraphs\n    format_scores.append(1.0 if response.count('\\n\\n') &gt; 0 else 0.0)\n    # Uses punctuation properly\n    format_scores.append(1.0 if re.search(r'[.!?]\\s+[A-Z]', response) else 0.0)\n    # Proper capitalization\n    format_scores.append(1.0 if re.search(r'^[A-Z]', response) else 0.0)\n    rewards['formatting'] = sum(format_scores) / len(format_scores)\n\n    # 4. Calculate Final Combined Reward\n    weights = {'length': 0.2, 'relevance': 0.5, 'formatting': 0.3}\n    final_reward = sum(rewards[k] * weights[k] for k in weights)\n    rewards['final'] = final_reward\n\n    return rewards\n\n# Let's test our practical reward function\ntest_cases = [\n    {\n        \"prompt\": \"Explain how photosynthesis works.\",\n        \"response\": \"\"\"\nPhotosynthesis is the process by which plants convert sunlight into energy.\n\nThe process involves chlorophyll in the leaves capturing sunlight. This energy\nis used to convert water and carbon dioxide into glucose and oxygen.\n\nPlants use glucose for energy and release oxygen as a byproduct.\n\"\"\"\n    },\n    {\n        \"prompt\": \"Explain how photosynthesis works.\",\n        \"response\": \"plants make food from sun\"\n    }\n]\n\nfor case in test_cases:\n    rewards = calculate_rewards(case[\"prompt\"], case[\"response\"])\n    print(f\"\\nPrompt: {case['prompt']}\")\n    print(f\"Response: {case['response']}\")\n    print(\"\\nRewards:\")\n    for aspect, score in rewards.items():\n        print(f\"{aspect}: {score:.3f}\")\n\n\nPrompt: Explain how photosynthesis works.\nResponse: \nPhotosynthesis is the process by which plants convert sunlight into energy.\n\nThe process involves chlorophyll in the leaves capturing sunlight. This energy\nis used to convert water and carbon dioxide into glucose and oxygen.\n\nPlants use glucose for energy and release oxygen as a byproduct.\n\n\nRewards:\nlength: 0.900\nrelevance: 0.250\nformatting: 0.667\nfinal: 0.505\n\nPrompt: Explain how photosynthesis works.\nResponse: plants make food from sun\n\nRewards:\nlength: 0.100\nrelevance: 0.000\nformatting: 0.000\nfinal: 0.020"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#key-takeaways-about-reward-functions",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#key-takeaways-about-reward-functions",
    "title": "RLHF for LLMs",
    "section": "Key Takeaways About Reward Functions",
    "text": "Key Takeaways About Reward Functions\nSo, when making practical reward functions, there are multiple ways to make a reward function better, including (but not necessarily limited to):\n\nadding multiple reward components help avoid gaming the system\nusing weighted combinations allow prioritizing different target metrics\ndefining clear relationships between quality and reward score\n\nSome very common pitfalls to keep in mind: - Reward Hacking: some rewards are easily maximized by doing something very simple, not actually increasing quality - Unclear Signaling: it can be difficult to make rewards that very clearly, concretely determine ‚Äúgoodness‚Äù - Computational Overhead: Complex rewards can obviously be helpful in targeting rich, high-quality results, BUT can slow down training significantly - Inconsistent Scaling: Be sure to normalize and weight any scores that are combined appropriately."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#key-components-of-ppo",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#key-components-of-ppo",
    "title": "RLHF for LLMs",
    "section": "Key Components of PPO",
    "text": "Key Components of PPO\n\nThe Policy (œÄ): In our case, this is our language model. It has two key behaviors:\n\n\nActing: Generating text given a prompt\nCalculating Probabilities: Telling us how likely it would be to generate specific text\n\n\nThe Clipped Objective: This is PPO‚Äôs real metric for improvement. It includes:\n\n\nProbability Ratio (r): How much more/less likely is the new policy to generate certain text compared to the old policy?\nClipping: We put boundaries on this ratio (typically between 0.8 and 1.2)\nAdvantage (A): How much better/worse was the outcome than expected?\n\nFor LLM‚Äôs, PPO is implemented on a token-by-token basis, so each token gets the opportunity to be better or worse for the outcome. In this way, PPO is not exactly a loss function but uses a surrogate objective (a type of loss function) to guide updates. It incorporates the reward by comparing the new policy‚Äôs performance (via probabilities of actions, i.e., token predictions) to the old policy‚Äôs performance, scaled by the reward."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#the-ppo-mathematically",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#the-ppo-mathematically",
    "title": "RLHF for LLMs",
    "section": "The PPO Mathematically",
    "text": "The PPO Mathematically\nOf course, how the PPO is implemented in practice is through that clipped objective function.\nThe PPO objective function \\((L^{CLIP})\\) is defined as: \\[\nL^{CLIP}(\\theta) = E[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]\n\\]\nWhere:\n\n\\(\\theta\\) represents the policy parameters (our LLM weights)\n\\(r_t(\\theta)\\) is the probability ratio, \\(\\pi_{\\theta}(a_t|s_t)\\) / \\(\\pi_{\\theta_{old}}(a_t|s_t)\\)\n\\(A_t\\) is the advantage\n\\(\\epsilon\\) is the clipping parameter (typically 0.2)\n\n\n\nFor an LLM, the policy \\(\\pi_{\\theta}(a_t|s_t)\\) represents the probability of choosing token \\(a_t\\) given context \\(s_t\\):\n\n\n\\(s_t\\) is the current context (prompt + generated tokens so far)\n\\(a_t\\) is the next token to generate\n\n\nThe probability ratio, \\(r_t(\\theta) = \\pi_{\\theta}(a_t|s_t)\\) / \\(\\pi_{\\theta_{old}}(a_t|s_t)\\) is the ratio in probability of the new policy (\\(\\theta\\)) choosing token \\(a_t\\) over the probability from the old policy (\\(\\theta_{old}\\)) choosing token \\(a_t\\). (i.e \\(r_t(\\theta)\\) = 2 means it‚Äôs twice as likely to choose \\(a_t\\) in the new policy)\nThe clipping function, \\(clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\) ensures this probability ratio is reasonably bounded, keeping updates small.\nThen for each token, we calculate both the unclipped objective: \\(r_t(Œ∏)A_t\\) and the clipped objective \\(clip(r_t(Œ∏), 1-Œµ, 1+Œµ)A_t\\), and take the minimum of both, again ensuring smaller, more stable updates.\nWhere (\\(A(t)\\)) is the ‚Äúadvantage‚Äù (\\(A(t)\\)) is a measurement of how much better or worse an action was compared to what we expected, it incorporates, but isn‚Äôt exactly the reward‚Ä¶\n\n\n‚Ä¶The ‚ÄúAdvantage‚Äù \\((A_t)\\)\nOk, so we spent all of that time talking about the reward, just for it to end up wrapped in something called the ‚Äúadvantage‚Äù, so let‚Äôs break down the advantage a bit more.\nAdvantage \\((A_t)\\) measures how much better or worse an action was compared to what we expected. It‚Äôs calculated as: \\[A_t = R_t - V(s_t)\\]\nwhere: - \\(R_t\\) is the actual reward received - \\(V(s_t)\\) is the expected value (what we thought we‚Äôd get)\nIn the simplest example, imagine you‚Äôre a language model deciding what word to generate:\n\nYou generate ‚Äúexcellent‚Äù with an expected reward of 0.5. Then, for an actual reward of 0.8: \\[Advantage = 0.8 - 0.5 = +0.3\\] This was better than expected.\nYou generate ‚Äúokay‚Äù with an expected reward of 0.5. Then, for an actual reward of 0.3: \\[Advantage = 0.3 - 0.5 = -0.2\\] Negative advantage means this was worse than expected.\n\nAdvantage is used over raw reward because it allows harder prompts to expect harder results. For example, easy prompts could be very easy to get a reward of 0.8, but hard prompts hardly ever get even 0.4. Hard vs easy prompts having their own expected reward can accommodate this difference in their ‚Äútypical‚Äù reward.\nOk, so reward for a given word, we‚Äôve covered. But expected reward, what we ‚Äúthought we‚Äôd get‚Äù‚Ä¶ you might have guessed that that‚Äôs a bit more complicated.\n\n\nThe ‚ÄúExpected Value‚Äù (\\(V(s_t)\\))\nThe expected value is what you think your reward should be. This is naturally difficult to define, so you can really get this in a lot of ways. Some very simple ways could be: - defining different expected values for different categories of prompts. i.e:\nexpected_values = {\n    \"math\": 0.6,        # Math problems are harder\n    \"greeting\": 0.9,    # Greetings are easy\n    \"general\": 0.7      # General conversation\n}\nexpected_value = expected_values[get_prompt_type(prompt)]\n\nkeeping track of rewards you‚Äôve already given different types of prompts:\n\nrewards_history = {\n    \"math_questions\": [0.6, 0.7, 0.5, 0.8],  # Previous rewards for math\n    \"greeting\": [0.9, 0.95, 0.85, 0.9]       # Previous rewards for greetings\n}\nexpected_value = average(rewards_history[prompt_type])\nBut in practice, calculating the expected reward tends to be more sophisticated. It can be done with:\n\nValue Networks: these are neural networks who are trained to predict rewards. It can handle more nuanced contexts about the prompts, and more easily adapt to things like prompt lengths, topics covered, and depth of explanation asked for.\nTemporal Differences: A way to consider future rewards, not just immediate ones. Instead of just looking at the current reward (R), we also consider what rewards we expect in the future (\\(\\lambda\\) * \\(V_{next}\\), where \\(\\lambda\\) is a discount factor). This helps when your early actions lead to better outcomes later.\nGeneralized Advantage Estimation (GAE): A method that balances between immediate and future rewards when calculating advantage. It uses a weighted average of rewards over different time spans, helping to reduce variance in our advantage estimates while still maintaining useful learning signals.\n\nIn this notebook, we‚Äôll stick to some of these very simple expected value definitions. But it‚Äôs important to know that this is a choice to be made in any RL implementation! And, that the value function used is also something that can be updated. So, during training, a value function can learn to generate better expected values: we can use loss functions to update both \\(\\pi_\\theta\\) and (\\(V(s_t)\\)). We‚Äôll discuss this in more detail below!\n\n\n\nThe Expectation Operation\nI promise there will soon be code. But there‚Äôs one more thing to clear up. The PPO objective function \\((L^{CLIP})\\): \\[\nL^{CLIP}(\\theta) = E[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]\n\\]\nhas that ‚Äú\\(E\\)‚Äù - what is that?\nThis is the expectation operation. That‚Äôs mathematically simple enough - it‚Äôs really just a weighted average: the mean of the possible values that a variable can take. That‚Äôs all!\nBut what are ‚Äúall possible values‚Äù? For LLM‚Äôs, this is a sample of all possible experiences (trajectories) collected from the policy. So really, we average over a sample of all token paths that you could have generated in response to all prompt examples. Why a sample? For a bit more detailed explanation, see Appendix 1.\nWe‚Äôll touch on this again in a bit, but for now, let‚Äôs jump into an example."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#a-simple-ppo-example",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#a-simple-ppo-example",
    "title": "RLHF for LLMs",
    "section": "A Simple PPO Example",
    "text": "A Simple PPO Example\nLet‚Äôs see an actual implementation of the PPO calculation, to observe how, in practice, this is calculated over trajectories that an LLM could generate.\n\nFor a Single Token\nBelow, we‚Äôll see how the PPO function would calculate its objective for a single new potential token.\nConsider in this example that we have generated so far:\n\n\n‚ÄúI am feeling great and happy today, it‚Äôs‚Äù\n\n\nAnd we are tasked with determining if the new word that we could generate:\n\n\n‚Äúexcellent‚Äù\n\n\nis advantageous or not.\nThe current policy (model) generates ‚Äúexcellent‚Äù with probability 0.3, and the new proposed policy we‚Äôre evaluating generates ‚Äúexcellent‚Äù with probability 0.6.\nBelow, we‚Äôll calculate the PPO Objective \\((L^{CLIP})\\) that would result from the production of that token. We‚Äôll use our super simple sentiment_reward function that we defined above to determine our reward.\n\ndef ppo_update(old_prob, new_prob, advantage, epsilon=0.1):\n    # Calculate the probability ratio\n    prob_ratio = new_prob / old_prob\n\n    # Compute the unclipped and clipped objectives\n    unclipped_objective = prob_ratio * advantage\n    clipped_objective = np.clip(prob_ratio, 1 - epsilon, 1 + epsilon) * advantage\n\n    # PPO's objective is the minimum of the unclipped and clipped objectives\n    ppo_objective = min(unclipped_objective, clipped_objective)\n\n    return ppo_objective\n\n# new word to generate that we are testing the update for\nnew_word = \"excellent\"\n\n# Let's say, the old policy generated this word with 30% chance, and the new one generates it with 60% chance\nold_prob = 0.3\nnew_prob = 0.6\n\n# Simulated response from the agent so far\nresponse = \"I am feeling great and happy today, it's\"\n\n# Calculate the reward based on our very simple positive word reward from before\n# this calculates over the full current response with this new next token\nreward = sentiment_reward(response + new_word)\n\n# Let's say in this case, the expected behavior is that the new word is neutral,\n# so we expect it to not *add* any reward over what we had\nexpected_reward = sentiment_reward(response)\n\n# calculate the advantage that this new word has given us\nadvantage = reward - expected_reward\n\n# Perform PPO update\nppo_objective = ppo_update(old_prob, new_prob, advantage)\n\nprint(f\"Response: {response}\")\nprint(f\"Positivity Reward: {reward}\")\nprint(f\"Advantage: {advantage}\")\nprint(f\"PPO Objective: {ppo_objective}\")\n\nResponse: I am feeling great and happy today, it's\nPositivity Reward: 1\nAdvantage: 0\nPPO Objective: 0.0\n\n\nWe can see here, that adding excellent resulted in positive advantage: it was good to our reward function to add that word.\nWe can also see when we step through the math, that our objective got clipped: Our probability ratio was 2.0, so: - unclipped objective was \\(2.0 * 1.0 = 2.0\\). - clipped objective (what‚Äôs used) was \\(1.1 * 1.0 = 1.1\\)\nThis is a key action of PPO: ensuring that policy updates remain relatively small.\n\nüìå Note: you may be able to see here, that if our advantage term had been large, even with clipping, the update can be substantial, potentially leading to instability. To mitigate this, it‚Äôs common practice to normalize or scale rewards, thereby controlling the magnitude of the advantage. This normalization helps maintain stable and consistent updates.\n\n\n\nSimple PPO Example Over a Trajectory\nIn practice, any given ‚Äúresponse‚Äù which is a series of token selections is a trajectory that the model could have generated.\nFor each word in:\n\n\n‚ÄúI am feeling great and happy today, it‚Äôs‚Äù\n\n\nthere is an associated old probability, new probability, and reward for generating that token.\nWe can calculate \\(L_{CLIP(\\text{trajectory})}\\) over a given trajectory as: \\[\nL_{CLIP(\\text{trajectory})} =  \\sum_t^T L_{CLIP(t)}\n\\]\nfor a sequence of length T, comprised of tokens t\n\n# for a given trajectory of tokens, each generated with some probability\nresponse_tokens = [\"I\", \"am\", \"feeling\", \"great\", \"and\", \"happy\", \"today,\", \"it's\", \"excellent\"]\n\nold_probs = [.9, .95, .4, .25, .33, .45, .4, .15, .3] # dummy probabilities for each word in the sequence, in old policy\nnew_probs = [.9, .95, .6, .55, .5, .65, .3, .2, .6] # dummy probabilities for each word in the sequence, in new policy\n\n# Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)\nexpected_reward = 0\n\n# Compute PPO objectives for each token\nppo_objectives = []\nfor i, token in enumerate(response_tokens):\n    reward = sentiment_reward(token)\n    advantage = reward - expected_reward  # Advantage function\n\n    ppo_obj = ppo_update(old_probs[i], new_probs[i], advantage)\n    ppo_objectives.append(ppo_obj)\n\n    print(f\"Token: {token}\")\n    print(f\"Reward: {reward:.2f}, Advantage: {advantage:.2f}, PPO Objective: {ppo_obj:.2f}\\n\")\n\n# Trajectory PPO objective (sum over tokens in this trajectory)\ntraj_ppo_objective = sum(ppo_objectives)\nprint(f\"Total PPO Objective for this trajectory: {traj_ppo_objective:.2f}\")\n\nToken: I\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: am\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: feeling\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: great\nReward: 1.00, Advantage: 1.00, PPO Objective: 1.10\n\nToken: and\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: happy\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: today,\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: it's\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: excellent\nReward: 1.00, Advantage: 1.00, PPO Objective: 1.10\n\nTotal PPO Objective for this trajectory: 2.20\n\n\n\n\nSimple PPO Example Over Multiple Trajectories\nSo, the last step here is to consider the multiple trajectories that a model could have reasonably taken during genration, of which we sample some number.\nWe can calculate our final \\(L_{CLIP(\\text{total})}\\) as: \\[\nL_{CLIP(\\text{total})} =  \\frac{1}{N} * \\sum_i^N L_{CLIP(i)}\n\\]\nover all N trajectories we sampled.\nSo, let‚Äôs consider in this case, we sampled a second trajectory which we want to consider in our final \\(L_{CLIP(\\text{total})}\\) calculation.\n\n# for a different trajectory of tokens, each generated with some probability\nresponse_2_tokens = [\"I\", \"am\", \"angry\", \"it's\", \"awful\"]\n\nold_probs = [.9, .95, .3, .25, .2] # dummy probabilities for each word in the sequence, in old policy\nnew_probs = [.9, .95, .66, .55, .75] # dummy probabilities for each word in the sequence, in new policy\n\n# Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)\nexpected_reward = 0\n\n# Compute PPO objectives for each token\nppo_objectives = []\nfor i, token in enumerate(response_2_tokens):\n    reward = sentiment_reward(token)\n    advantage = reward - expected_reward  # Advantage function\n\n    ppo_obj = ppo_update(old_probs[i], new_probs[i], advantage)\n    ppo_objectives.append(ppo_obj)\n\n    print(f\"Token: {token}\")\n    print(f\"Reward: {reward:.2f}, Advantage: {advantage:.2f}, PPO Objective: {ppo_obj:.2f}\\n\")\n\n# Trajectory PPO objective (sum over tokens in this trajectory)\ntraj_ppo_objective2 = sum(ppo_objectives)\nprint(f\"Total PPO Objective for this trajectory: {traj_ppo_objective2:.2f}\\n\")\n\n# Final PPO objective (average over both trajectories)\ntotal_ppo_objective = np.mean([traj_ppo_objective, traj_ppo_objective2])\nprint('\\x1b[0;33;35m' + f\"Total PPO Objective for the full response: {total_ppo_objective:.2f}\" + '\\x1b[0m')\n\nToken: I\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: am\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: angry\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: it's\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: awful\nReward: -1.00, Advantage: -1.00, PPO Objective: -3.75\n\nTotal PPO Objective for this trajectory: -3.75\n\nTotal PPO Objective for the full response: -0.77\n\n\n\nSo, our total PPO objective is calculated over all potential trajectories that we sampled. In this case, that was just 2. And we can see that, even though our first trajectory seemed to look good and aligned with the behavior that we wanted, we also had a trajectory that was bad. This is why it‚Äôs important to take a sample of possible trajectories that the model could produce!\n\nüìå Note: You might also notice here that our second response got a strongly negative PPO objective. Negative rewards, leading to negative advantages, mean that the clipping won‚Äôt do anything. Since we always take the minimum of the clipped and unclipped objective, advantage * unclipped probability will always be &lt;= advantage * clipped probability. See the note in Appendix 2 about this for more details about how negative rewards are used in practice! For now, we‚Äôll roll with it.)"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#kl-divergence-in-rlhf",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#kl-divergence-in-rlhf",
    "title": "RLHF for LLMs",
    "section": "KL Divergence in RLHF",
    "text": "KL Divergence in RLHF\nOne more concept to touch on is something called the Kullback-Leibler (KL) Divergence. When fine-tuning LLMs with reinforcement learning, we want to improve the model‚Äôs behavior while preventing it from deviating too drastically from its original training.\nKL divergence measures how much one probability distribution differs from another. In the context of LLMs, it quantifies the difference between the token probability distributions of two models - typically our current policy model and a reference model.\nMathematically, for two probability distributions \\(P\\) and \\(Q\\), KL divergence is defined as:\n\\[\nD_{KL}(P || Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n\\]\nFor language models, this becomes: \\[\nD_{KL}(\\pi_{\\text{new}} || \\pi_{\\text{ref}}) = \\sum_{t} \\pi_{\\text{new}}(t|c) \\log\\frac{\\pi_{\\text{new}}(t|c)}{\\pi_{\\text{ref}}(t|c)}\n\\] Where: - \\(\\pi_{\\text{new}}\\) is our current policy model - \\(\\pi_{\\text{ref}}\\) is the reference model (usually the initial model before RL training) - \\(t\\) represents tokens, generated given - \\(c\\) the context/prompt\nFrequently, KL divergence is included as an explicit penalty in our reward function, preventing the model from:\n\nForgetting its pre-trained knowledge\nAdopting degenerate patterns to maximize reward\nStraying too far from human-like text generation\n\nIn practice, we typically add a KL penalty term to our reward: \\[\nr_{\\text{total}} = r_{\\text{original}} - \\beta \\cdot D_{KL}(\\pi_{\\text{new}} || \\pi_{\\text{ref}})\n\\]\nWhere \\(\\beta\\) controls the strength of the penalty. This creates a balance between optimizing for rewards and maintaining the model‚Äôs original behavior.\n ## üìå One Final Note on PPO\nYou should now understand how the PPO is calculated and implemented fairly well. And while PPO is a very popular algorithm for reinforcement learning, and what we‚Äôll go on to use in this notebook, you should know that it‚Äôs far from the only one. There are many other choices of update strategy that can be used, and while it‚Äôs out of the scope of this notebook to go into all of them, I‚Äôll leave a few links here for some other popular choices, that you may be interested to look into now that you know about PPO.\nTrust Region Policy Optimization (TRPO)\nDirect Preference Optimization (DPO)"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#how-ppo-updates-a-model-in-practice",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#how-ppo-updates-a-model-in-practice",
    "title": "RLHF for LLMs",
    "section": "How PPO Updates a Model in Practice",
    "text": "How PPO Updates a Model in Practice\nSo, we know how the PPO Objective is calculated. But how exactly is that implemented during training to update the weights of the model and change its behavior?\nWell, it‚Äôs simple! The PPO objective is directly applied as the loss function, so gradient descent directly optimizes this function by computing\n\\[\n\\frac{\\partial L_{CLIP}}{\\partial\\theta}\n\\]\nduring backpropogation to update the model‚Äôs weights.\n\n\nWhat Defines the Old and New Policy?\nAs we already discussed, PPO relies on comparing an old policy with a new policy and determining if the new policy is favorable.\nThe old policy (\\(\\pi_{\\theta(old)}\\)) is the model before applying the PPO update. The new policy (\\(\\pi_\\theta\\)) is the model after we‚Äôve updated it using PPO.\nBut you might be asking, how do we get old and new policies? What stages of training to they correspond to? The new policy is the one that we just got via gradient descent. And the old policy is from the previous update.\nConsider going through an epoch of updating a model using PPO. The typical PPO ‚Äúiteration‚Äù (a high-level loop): - Set Old Policy: Copy your current model weights as ‚Äúold policy.‚Äù - Gather Data: Roll out the environment using the old policy. - Compute Rewards & Advantages: Based on the data from step 2. - Run Multiple Mini-Batch Updates: Each update modifies the model from (\\(\\pi_{\\theta(old)}\\)) to (\\(\\pi_{\\theta}\\))\nAfter these updates finish, your ‚Äúnew policy‚Äù (\\(\\pi_{\\theta}\\)) is typically quite different from (\\(\\pi_{\\theta(old)}\\)).\nThen at the start of the next iteration, you set old policy = new policy (the final model from last iteration), gather fresh data, do more updates, repeat.\nIn many PPO codebases, they‚Äôll say ‚Äúwe do \\(K\\) epochs per iteration.‚Äù Those ‚Äúepochs‚Äù just mean \\(K\\) passes of gradient descent on the same collected batch. Each pass changes the policy slightly, but it‚Äôs all within a single iteration.\n\nüìå Note: As you might have guessed, this is a little bit more nuanced at the start of an iteration, as we don‚Äôt really yet have a \\(\\pi_\\theta\\) to act as the new policy yet! So for the first step of every iteration, \\(\\pi_\\theta = \\pi_{\\theta(old)}\\) and it updates as soon as it‚Äôs seen some of the data. See Appendix 3 for a much more in-depth discussion of this, including why mini-batches can improve learning with PPO over i.e full stochastic gradient descent."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#training-the-value-function",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#training-the-value-function",
    "title": "RLHF for LLMs",
    "section": "Training The Value Function",
    "text": "Training The Value Function\nWe‚Äôve talked about the advantage, which is calculated: \\[\nA(t) = R(t) - V_s(t)\n\\] And how \\(V_s(t)\\), the value function that determines the reward that the model ‚Äúexpects‚Äù to receive, can generally be quite complicated.\nWell, in practice, it is often something that is trained along with adjustments to the model, because accurate \\(V_s(t)\\) estimates lead to less noisy advantage estimates and more stable training.\nThe value function can be trained just like any regression model: - We want \\(V_{\\theta}(s_t)\\) to match the actual return \\(R_t\\). - So, we use i.e.¬†a Mean Squared Error (MSE) loss: $ L_V() = ( V_(s_t) - R_t )^2 $ - And update it alongside PPO‚Äôs policy loss as a separate loss term.\nSo a real, full PPO loss function, including the value function then becomes something like: \\[\nL_{\\text{total}} = L_{\\text{CLIP}} + c_1 L_V - c_2 H[\\pi]\n\\] where: - \\(L_{\\text{CLIP}}\\) = PPO policy loss. - \\(L_V\\) = Value function loss (MSE between \\(V(s)\\) and \\(R_t\\)). - \\(H[\\pi]\\) = Optional entropy term sometimes added to encourage exploration. - \\(c_1, c_2\\) = Tunable coefficients that control the relative weights of the loss components(e.g., \\(c_1 = 0.5, c_2 = 0.01\\)).\nThat‚Äôs all that I‚Äôll say about training the value function here, because while we will train it going forward, we‚Äôre going to keep it simple. But make sure to reference Appendix 4 about value functions if you‚Äôd like to know more."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#an-example-in-code",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#an-example-in-code",
    "title": "RLHF for LLMs",
    "section": "An Example in Code",
    "text": "An Example in Code\nOk. We‚Äôre ready to put together a full simple pipeline of doing reinforcement learning on an LLM!\nWe‚Äôll make use of the trl library to handle the PPO part. We‚Äôll see below how this library handles all of the ‚Äúdifficult‚Äù parts of implementing reinforcement learning for us.\n\nüìå Note: In this tutorial, we are using a downgraded version of trl, 0.10.1. I found this version was easier to use and understand the code for. Make sure if you read the docs while going through this tutorial to switch them to that version, as more updated versions changed syntax somewhat drastically!\n\n\nDefining the Reward\nWe‚Äôre going to use a very simple reward function for this example, similar to the sentiment_reward that we used earlier in this tutorial, but improve it slightly and make some changes to make it easier to learn.\nThe changes we‚Äôll make: - Assign word-based values for postive-ness and negative-ness, so some words get higher rewards than others - Multiply our positive score by 5 to get stronger bias towards positive words - Add a small positive bias for all sentences (0.1) so our reward is always positive. This makes for more stable training generally.\n\n# Define improved sentiment reward function\ndef get_sentiment_reward(text: str) -&gt; float:\n    positive_words = {\n        'good': 1, 'great': 2, 'excellent': 3, 'wonderful': 3, 'amazing': 3,\n        'happy': 2, 'glad': 1, 'love': 3, 'like': 1, \"awesome\": 2,\n        \"fun\": 2, \"super\": 2, \"incredible\": 3, 'perfect': 3\n    }\n\n    negative_words = {\n        'bad': 1, 'awful': 2, 'terrible': 3, 'angry': 2, 'horrible': 3,\n        'lame': 1, 'hate': 3, \"gross\": 2, 'sad': 1, 'upset': 2\n    }\n\n    words = text.lower().split()\n    positive_score = sum(positive_words.get(word, 0) for word in words)\n    negative_score = sum(negative_words.get(word, 0) for word in words)\n\n    # Simple calculation with positive bias\n    reward = (positive_score * 5) - negative_score\n    return max(0.1, float(reward))  # Ensure minimum positive reward\n\n\n\nLoading A Model\nHere, we‚Äôll use GPT-2. This is a small LLM that generates text in a completion way (i.e it will finish sentences for you, not respond to what you say).\nWe will load this with AutoModelForCausalLMWithValueHead, which loads both an autoregressive model with a value head in addition to the language model head. The value head is then a trainable determination of the expected value of the model‚Äôs output.\ntrl will also want a ref_model: A copy of the original model, used to compare the trained model‚Äôs outputs against a reference. This reference model is used to compute KL divergence to prevent the model from deviating too much from its initial behavior.\nFinally, we load the tokenizer, which will turn our text into numbers the model can understand, and vice versa.\nThen, we define some generation arguments. These parameters control some more fine-grained details about how text is generated during training, and how we explore different text options during generation. In particular, these arguments determine how the model chooses a response to a prompt: - top_k: 0 When set to 0, there‚Äôs no limit on how many possible next tokens the model considers. If set to a value like 50, the model would only consider the 50 most likely next tokens and ignore all others. With top_k: 0, all possible tokens remain candidates, even unlikely ones. - top_p: 1.0 This controls ‚Äúnucleus sampling‚Äù (also called ‚Äúcumulative probability truncation‚Äù). A value of 1.0 means the model considers all tokens whose cumulative probability adds up to 100%. If set to 0.9, the model would only consider tokens whose cumulative probability adds up to 90%, effectively filtering out the long tail of unlikely tokens. - do_sample: True This determines whether the model uses sampling (probabilistic selection) or greedy decoding: - When True: The model randomly selects the next token based on the probability distribution, allowing for creativity and variation - When False: The model always picks the single most likely token (greedy decoding), leading to more predictable but potentially repetitive output\nThese parameters allow the model to generate more diverse outputs for a given prompt, increasing the exploration.\n\n# 1. load a pretrained model - with clear device placement\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nmodel.to(device)\n\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nref_model.to(device)\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ngeneration_kwargs = {\n    \"min_length\": -1,         # No minimum length constraint\n    \"top_k\": 0,               # No top-k filtering (consider all tokens)\n    \"top_p\": 1.0,             # No nucleus sampling (consider all tokens)\n    \"do_sample\": True,        # Use sampling rather than greedy decoding\n    \"pad_token_id\": tokenizer.eos_token_id,  # Pad with EOS token\n    \"max_new_tokens\": 15,     # Generate at most 15 new tokens, will help speed up training\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up the PPO Configuration\nThe PPO Trainer also allows certain configuration parameters. We‚Äôll set some simple ones, but mostly leave this to the defaults. The learning rate is important here - we choose something quite small to keep training stable.\nThen, PPOTrainer just needs this config, the model, reference model, and tokenizer to eventually run the full PPO pipeline!\n\n# 2. initialize trainer with minimal parameters\nppo_config = {\n    \"mini_batch_size\": 1,     # Process one example at a time\n    \"batch_size\": 1,          # Total batch size for one optimization step\n    \"learning_rate\": 5e-6,    # Learning rate for optimizer\n    \"log_with\": None,         # No external logging\n}\n\nconfig = PPOConfig(**ppo_config)\nppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)\n\n\n\nGetting a Starting Point\nLet‚Äôs start by defining some simple prompts that should prompt positive responses. We‚Äôll see how GPT-2 performs out of the box on these prompts by checking the reward that we get from its default responses.\nYou‚Äôll see below, GPT-2 isn‚Äôt exactly the most eloquent or coherent model. That‚Äôs ok! We‚Äôre really just trying to train it to give us a bunch of postive words anyway, which is simple enough of a task that it should learn it fine!\n\n# Training prompts\nprompts = [\n    \"I feel happy when\",\n    \"The best part about this is\",\n    \"I love how\",\n    \"Today was great because\",\n]\n\n# Before training outputs\nprint(\"\\n=== Before Training Outputs ===\")\norig_responses = {}\norig_rewards = []\n\nfor prompt in prompts:\n    query_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        # Use the base model for generation to avoid CUDA errors\n        response = model.generate(\n            query_tensor,\n            **generation_kwargs\n        )\n    response_txt = tokenizer.decode(response[0])\n    orig_responses[prompt] = response_txt\n    reward_value = get_sentiment_reward(response_txt)\n    orig_rewards.append(reward_value)\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Response: {response_txt}\")\n    print(f\"Reward: {reward_value}\")\n\nprint(f\"\\nAverage initial reward: {np.mean(orig_rewards):.2f}\")\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\n\n=== Before Training Outputs ===\n\nPrompt: I feel happy when\nResponse: I feel happy when she is interested in contributing.\"\n\nWhen asked about others calling out the\nReward: 10.0\n\nPrompt: The best part about this is\nResponse: The best part about this is you can make it very nationalful. Again, it's all true.\nReward: 0.1\n\nPrompt: I love how\nResponse: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their\nReward: 15.0\n\nPrompt: Today was great because\nResponse: Today was great because we knew we could create more labor for people,\" Smith said. \"And\nReward: 10.0\n\nAverage initial reward: 8.78\n\n\n\n\nSetting up the Training Loop\nThis is the main loop where we‚Äôll generate responses, calculate rewards, and update the model using PPO. The logic will be as follows:\nFor each epoch &gt; For each of our prompts &gt;&gt; Tokenize the prompt &gt;&gt; &gt;&gt; Generate a response from the current version of the model (new policy) &gt;&gt; &gt;&gt; Calculate the reward of that response &gt;&gt; &gt;&gt; Give the PPO trainer the prompt, response, and corresponding reward\nThe magic really happens here: Give the PPO trainer the prompt, response, and corresponding reward. The trl library and PPOTrainer that we set up are aware of the model, reference model, and tokenizer. This step handles using the reward we passed in, as well as the prompt + response that corresponded to that reward to: - Calculate probabilities: Computes how likely the generated tokens were under both the current model (new policy) and reference model (old policy) - Compute advantage: Determines how much better or worse the generated response performed compared to what was expected - Apply the PPO objective: Uses the clipped PPO objective function to limit how much the model changes in a single step - Perform backpropagation: Updates the model weights to make high-reward responses more likely in the future - Update the value function: The value function (which is part of the model from AutoModelForCausalLMWithValueHead) estimates the expected reward for a given state is also updated. - Enforce KL divergence: Ensures the new policy doesn‚Äôt deviate too far from the old policy, maintaining coherent text generation - Return statistics: Provides metrics about the update like loss values, KL divergence, and entropy for monitoring the training process\nThe line: stats = ppo_trainer.step([query[0]], [response[0]], [rewards])) encapsulates the core RL algorithm that enables the model to learn from the reward signal. It‚Äôs where the model actually learns to adjust its probability distribution to favor token sequences that lead to higher sentiment rewards. And trl does all of the hard stuff in the background for us!\n\nprint(\"\\n=== Starting Training ===\")\n\n# Prepare for tracking training statistics\nepoch_stats = defaultdict(list)\nepoch_rewards = []\n\n# Run for multiple epochs\nnum_epochs = 12\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}\")\n    epoch_reward = 0\n    epoch_responses = []\n\n    # Shuffle prompts each epoch for better generalization\n    np.random.shuffle(prompts)\n\n    # Process each prompt\n    for prompt in prompts:\n          # Encode the prompt\n          encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n          # Generate a response using the current model\n          with torch.no_grad():\n              response = model.generate(\n                  encoded_prompt,\n                  **generation_kwargs\n              )\n\n          # Decode the response\n          response_txt = tokenizer.decode(response[0])\n\n          # Calculate reward for this response\n          reward_value = get_sentiment_reward(response_txt)\n          rewards = torch.tensor([reward_value], dtype=torch.float, device=device)\n\n          # Store for reporting\n          epoch_reward += reward_value\n          epoch_responses.append((prompt, response_txt, reward_value))\n\n          # Prepare tensors for PPO step\n          # PPO requires specific tensor shapes\n          query = encoded_prompt[0].unsqueeze(0)  # Reshape for PPO\n          response = response[0].unsqueeze(0)     # Reshape for PPO\n\n          # Train step - update model using PPO\n          stats = ppo_trainer.step([query[0]], [response[0]], [rewards])\n\n          # Track training metrics\n          for k, v in stats.items():\n              if v is not None:\n                  epoch_stats[k].append(v)\n\n    # Calculate and report epoch statistics\n    if epoch_responses:\n        avg_reward = epoch_reward / len(epoch_responses)\n        epoch_rewards.append(avg_reward)\n\n        print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n        print(f\"Average Reward: {avg_reward:.2f}\")\n\n        # Print reward trend\n        if epoch &gt; 0:\n            reward_change = avg_reward - epoch_rewards[-2]\n            print(f\"Reward Change: {reward_change:+.2f}\")\n\n        # Print sample responses\n        print(\"\\nSample responses from this epoch:\")\n        for i, (prompt, response, reward) in enumerate(epoch_responses[:2]):\n            print(f\"Prompt: {prompt}\")\n            print(f\"Response: {response}\")\n            print(f\"Reward: {reward:.2f}\")\n    else:\n        print(\"No successful training steps this epoch\")\n\n\n=== Starting Training ===\n\nEpoch 1\n\n--- Epoch 1 Summary ---\nAverage Reward: 11.25\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I don't think I'd rule it out because I'm very fortunate to\nReward: 15.00\nPrompt: Today was great because\nResponse: Today was great because to see all the flames in the air. It was a whirlwind of congressional\nReward: 10.00\n\nEpoch 2\n\n--- Epoch 2 Summary ---\nAverage Reward: 13.78\nReward Change: +2.53\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how tragic this was\n\nI love it so much I feel like the tornado\nReward: 35.00\nPrompt: The best part about this is\nResponse: The best part about this is that, from a human point of view, all models bear all of the\nReward: 0.10\n\nEpoch 3\n\n--- Epoch 3 Summary ---\nAverage Reward: 17.52\nReward Change: +3.75\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when I keep seeing Lisp. I like it. It's a joy to work\nReward: 15.00\nPrompt: I love how\nResponse: I love how they have tried. I love that they wore their mark on their hat with\nReward: 30.00\n\nEpoch 4\n\n--- Epoch 4 Summary ---\nAverage Reward: 12.53\nReward Change: -5.00\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how you make method shots because I love orange funk and you've grown me up\nReward: 30.00\nPrompt: I feel happy when\nResponse: I feel happy when I have another large cock in my hips\n\nLatelose: Acting\nReward: 10.00\n\nEpoch 5\n\n--- Epoch 5 Summary ---\nAverage Reward: 12.53\nReward Change: +0.00\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when I I I I I I I I I I I I I I I\nReward: 10.00\nPrompt: Today was great because\nResponse: Today was great because they ate a little purple cabbage or rib eye of deind. Truly Complete\nReward: 10.00\n\nEpoch 6\n\n--- Epoch 6 Summary ---\nAverage Reward: 8.78\nReward Change: -3.75\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when my rest rest becomes not difficult... But when my recovery growsnt so urgent\nReward: 10.00\nPrompt: I love how\nResponse: I love how much multiplex are haunted areas. How pipe, wood, torches, etc\nReward: 15.00\n\nEpoch 7\n\n--- Epoch 7 Summary ---\nAverage Reward: 13.78\nReward Change: +5.00\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how they work with us,\" Barzero argues with both hands. And they are\nReward: 15.00\nPrompt: I feel happy when\nResponse: I feel happy when my sleep. I feel happy when I I I be happy uncertain uncertain uncertain\nReward: 30.00\n\nEpoch 8\n\n--- Epoch 8 Summary ---\nAverage Reward: 16.27\nReward Change: +2.50\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how it. Ourld literally bind for a happy it to per chance or for\nReward: 25.00\nPrompt: The best part about this is\nResponse: The best part about this is long hair:\nNote:You can choose between option:UnlockedSince\nReward: 0.10\n\nEpoch 9\n\n--- Epoch 9 Summary ---\nAverage Reward: 8.78\nReward Change: -7.50\n\nSample responses from this epoch:\nPrompt: Today was great because\nResponse: Today was great because we are currently loving and embracing being children. We are finding joy in understanding\nReward: 10.00\nPrompt: I feel happy when\nResponse: I feel happy when we were even able to get close to our spiritual helper.\n\nSo\nReward: 10.00\n\nEpoch 10\n\n--- Epoch 10 Summary ---\nAverage Reward: 10.03\nReward Change: +1.25\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I look at how I look at grabbing at back grabbing at back grabbing at\nReward: 15.00\nPrompt: I feel happy when\nResponse: I feel happy when you g m t,\n\nJ\n\nacje\n\nYes\nReward: 10.00\n\nEpoch 11\n\n--- Epoch 11 Summary ---\nAverage Reward: 10.00\nReward Change: -0.03\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I I I I I I I I IIIIIII\nReward: 15.00\nPrompt: Today was great because\nResponse: Today was great because malarkey women and men changed.The grateful ones had romantic view;\nReward: 10.00\n\nEpoch 12\n\n--- Epoch 12 Summary ---\nAverage Reward: 15.03\nReward Change: +5.03\n\nSample responses from this epoch:\nPrompt: Today was great because\nResponse: Today was great because of the freshandfreshlight You've commanduedTheOfGoodTheAvoid\nReward: 10.00\nPrompt: The best part about this is\nResponse: The best part about this is that on the gazelles ‚Äì I used to use them ‚Äì they\nReward: 0.10\n\n\n\n\nSeeing How We Did\nFinally, we can look a bit deeper to see how well we did. Let‚Äôs investigate in more depth: - Before vs After Comparison: For each prompt, we‚Äôll compare the original model‚Äôs response with our RL-trained model‚Äôs response. This direct comparison helps us visualize the specific changes in text generation. - Individual Reward Metrics: We calculate the sentiment reward for both the original and trained responses. The difference between these scores shows how much our model has improved at generating positive text. - Aggregate Improvement: By averaging rewards across all prompts, we can quantify the overall improvement from RL training. A positive change indicates successful optimization toward our sentiment objective.\n\n# Compare before/after\nprint(\"\\n=== After Training Outputs ===\")\nfinal_rewards = []\n\nfor prompt in prompts:\n    try:\n        # Generate using the standard method to avoid errors\n        encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            response = model.generate(\n                encoded_prompt,\n                **generation_kwargs\n            )\n        response_txt = tokenizer.decode(response[0])\n        reward_value = get_sentiment_reward(response_txt)\n        final_rewards.append(reward_value)\n\n        # Compare with original\n        orig_reward = get_sentiment_reward(orig_responses[prompt])\n\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Before: {orig_responses[prompt]}\")\n        print(f\"After: {response_txt}\")\n        print(f\"Reward Before: {orig_reward:.2f}\")\n        print(f\"Reward After: {reward_value:.2f}\")\n        print(f\"Improvement: {reward_value - orig_reward:+.2f}\")\n    except Exception as e:\n        print(f\"Error evaluating prompt '{prompt}': {e}\")\n\n# Print final stats\nif final_rewards:\n    print(\"\\n=== Training Results ===\")\n    print(f\"Starting Average Reward: {np.mean(orig_rewards):.2f}\")\n    print(f\"Ending Average Reward: {np.mean(final_rewards):.2f}\")\n    print(f\"Change: {np.mean(final_rewards) - np.mean(orig_rewards):+.2f}\")\n\n\n=== After Training Outputs ===\n\nPrompt: I feel happy when\nBefore: I feel happy when she is interested in contributing.\"\n\nWhen asked about others calling out the\nAfter: I feel happy when ordinary people are free and all my woes are banished because of quite full of\nReward Before: 10.00\nReward After: 10.00\nImprovement: +0.00\n\nPrompt: The best part about this is\nBefore: The best part about this is you can make it very nationalful. Again, it's all true.\nAfter: The best part about this is the paperwork. Write the cards up front, step by step, step by\nReward Before: 0.10\nReward After: 0.10\nImprovement: +0.00\n\nPrompt: I love how\nBefore: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their\nAfter: I love how Snowman looks too when she looks like Rocky Simon Newell. I love\nReward Before: 15.00\nReward After: 35.00\nImprovement: +20.00\n\nPrompt: Today was great because\nBefore: Today was great because we knew we could create more labor for people,\" Smith said. \"And\nAfter: Today was great because of was amazing and appreciated and astounding were all the immense and unrelasibility\nReward Before: 10.00\nReward After: 25.00\nImprovement: +15.00\n\n=== Training Results ===\nStarting Average Reward: 8.78\nEnding Average Reward: 17.52\nChange: +8.75\n\n\nAnd there you have it! We sucessfully trained GPT-2 to give us more positive words in its responses.\nNow, of course, we used a super simple reward here, and not a particularly good model (no offense, GPT-2), so we can see a lot of repeated words contributing to that positive response. As we discussed, in reality, a reward function should be more complicated, and our prompts used for training should be much more diverse than just 4. Still, you now know how to set up a PPO training pipeline!"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#collecting-human-feedback",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#collecting-human-feedback",
    "title": "RLHF for LLMs",
    "section": "Collecting Human Feedback",
    "text": "Collecting Human Feedback\nIn RLHF, human judgments about model outputs are collected. Unlike our algorithmic reward function that automatically calculated a score, RLHF relies on actual human preferences.\nTypically, a real RLHF pipeline will involve humans doing pairwise comparisons of responses. Rather than asking humans to provide absolute scores, RLHF typically uses comparative judgments where annotators choose which of two responses they prefer. This involves: - Prompt Selection: A diverse set of prompts is created to cover different topics, skills, and potential failure modes. - Response Generation: For each prompt, the model generates multiple responses using different sampling parameters. - Human Annotation: Human annotators are presented with a prompt and two model-generated responses, then asked to select which one is better according to specific criteria.\n\n# Simple simulation of human preference collection interface\ndef collect_human_preference(prompt, response_a, response_b):\n    print(f\"Prompt: {prompt}\\n\")\n    print(f\"Response A:\\n{response_a}\\n\")\n    print(f\"Response B:\\n{response_b}\\n\")\n\n    while True:\n        choice = input(\"Which response do you prefer? (A/B/Tie): \").upper()\n        if choice in [\"A\", \"B\", \"TIE\"]:\n            return choice\n        print(\"Invalid input. Please enter A, B, or Tie.\")\n\n# Example prompts and responses\nexamples = [\n    {\n        \"prompt\": \"Explain the concept of reinforcement learning to a high school student.\",\n        \"response_a\": \"Reinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes.\",\n        \"response_b\": \"Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward.\"\n    },\n    {\n        \"prompt\": \"What are some ways to reduce stress?\",\n        \"response_a\": \"Reducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing.\",\n        \"response_b\": \"To reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system.\"\n    }\n]\n\n# Run the preference collection for demonstration\ncollected_preferences = []\nfor i, example in enumerate(examples):\n    print(f\"\\n===== Example {i+1} =====\")\n    preference = collect_human_preference(\n        example[\"prompt\"],\n        example[\"response_a\"],\n        example[\"response_b\"]\n    )\n    collected_preferences.append({\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": \"response_a\" if preference == \"A\" else \"response_b\" if preference == \"B\" else \"tie\",\n        \"rejected\": \"response_b\" if preference == \"A\" else \"response_a\" if preference == \"B\" else \"tie\"\n    })\n    print(f\"You preferred Response {preference}\")\n\n\n===== Example 1 =====\nPrompt: Explain the concept of reinforcement learning to a high school student.\n\nResponse A:\nReinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes.\n\nResponse B:\nReinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward.\n\nWhich response do you prefer? (A/B/Tie): B\nYou preferred Response B\n\n===== Example 2 =====\nPrompt: What are some ways to reduce stress?\n\nResponse A:\nReducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing.\n\nResponse B:\nTo reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system.\n\nWhich response do you prefer? (A/B/Tie): B\nYou preferred Response B\n\n===== Collected Preferences =====\nExample 1: You chose the response_b over the response_a\nExample 2: You chose the response_b over the response_a\n\n\nWhen collecting real human feedback, annotators are typically given specific criteria to evaluate, like:\n\nHelpfulness: How well does the response address the user‚Äôs request?\nTruthfulness: Is the information accurate and factual?\nHarmlessness: Does the response avoid harmful, offensive, or misleading content?\nClarity: Is the response clearly written and easy to understand?"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#reward-model-training",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#reward-model-training",
    "title": "RLHF for LLMs",
    "section": "Reward Model Training",
    "text": "Reward Model Training\nWhile you get direct feedback this way, it would be difficult to collect real human feedback over all responses that you could possibly reasonably expect. So instead, a reward model is often trained to predict what humans like better. Better responses usually have some things in common, they‚Äôre maybe: - wordier - friendlier - more factual\nSo these more generalized properties can be learned, and then applied to prompts en masse, rather than needs humans to look at hundreds of thousands of responses individually. The reward model bridges the gap between collected human judgments and automated rewards needed for reinforcement learning.\n\nConverting Preferences to a Reward Model\nThe reward model is essentially a classifier trained to predict human preferences. It takes in a prompt and response, and outputs a scalar value representing the ‚Äúquality‚Äù of the response according to human preferences.\nIf we define:\n\\((x, y_w, y_l)\\) as a triplet where \\(x\\) is the prompt, \\(y_w\\) is the preferred (winning) response, and \\(y_l\\) is the less preferred (losing) response,\nand\n\\(r_\\theta(x, y)\\) as our reward model with parameters \\(\\theta\\) that outputs a scalar reward for prompt \\(x\\) and response \\(y\\).\nThen, we train the reward model to maximize the log probability of the human preferences: \\[\n{L}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]\n\\]\n\n\\({L}(\\theta)\\) is the loss function we‚Äôre trying to minimize, where \\(\\theta\\) represents all the parameters of our reward model.\n\\(\\mathbb{E}_{(x, y_w, y_l) \\sim D}\\) is the expected value over all triplets sampled from our dataset D. In simpler terms, it means ‚Äúthe average across all our training examples.‚Äù\n\\(r_\\theta(x, y_w)\\) is the reward score our model assigns to the winning (preferred) response \\(y_w\\) given prompt \\(x\\).\n\\(r_\\theta(x, y_l)\\) is the reward score our model assigns to the losing (less preferred) response \\(y_l\\) given the same prompt \\(x\\).\n\\(\\sigma(z)\\) is the sigmoid function, defined as \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), which maps any real number to a value between 0 and 1.\n\nThe equation expresses a Bradley-Terry model, which is used for pairwise comparisons. For each pair of responses, we compute the difference in their rewards: \\(r_\\theta(x, y_w) - r_\\theta(x, y_l)\\). We pass this difference through a sigmoid function, which gives us the probability that the model correctly predicts the human preference. We want to maximize this probability, which is equivalent to minimizing the negative log probability (since loss functions want to be minimized). Then, we average this loss across all training examples.\n\n\nTraining a Reward Model\nThen, we can train a reward model based on our triplets \\((x, y_w, y_l)\\).\nBelow is a very simplified example. Instead of a real neural network, we just use a function simulate_reward_scores to see how we could calculate the loss based on the preferences indicated in the last code cell. This function simply assigns a score to a response based purely on its length.\nIn reality, this calculated loss would help a model readjust its predicted rewards.\n\n# Simulate a reward model's outputs\n# In reality, these would come from a neural network\ndef simulate_reward_scores(response_a, response_b):\n    \"\"\"Simulate reward scores for demonstration purposes\"\"\"\n    # Just a simple length-based score for demonstration\n    score_a = 0.5 + 0.01 * len(response_a)\n    score_b = 0.5 + 0.01 * len(response_b)\n    return {\"response_a\": score_a, \"response_b\": score_b}\n\n# Calculate reward model loss\ndef reward_model_loss(scores, chosen, rejected):\n    \"\"\"Calculate the loss based on preference pair and model scores\"\"\"\n    chosen_score = scores[chosen]\n    rejected_score = scores[rejected]\n\n    # Print scores for demonstration\n    print(f\"Chosen response score: {chosen_score:.4f}\")\n    print(f\"Rejected response score: {rejected_score:.4f}\")\n    print(f\"Score difference (chosen - rejected): {chosen_score - rejected_score:.4f}\")\n\n    # The core loss function: -log(sigmoid(chosen_score - rejected_score))\n    # This encourages the model to give the preferred response a higher score\n    sigmoid = 1 / (1 + math.exp(-(chosen_score - rejected_score)))\n    loss = -math.log(sigmoid)\n\n    return loss\n\n# Using data from our previously collected human preferences\nfor i, preference in enumerate(collected_preferences):\n    example = examples[i]\n    prompt = example[\"prompt\"]\n    response_a = example[\"response_a\"]\n    response_b = example[\"response_b\"]\n    chosen = preference[\"chosen\"]\n    rejected = preference[\"rejected\"]\n\n    print(f\"\\n===== Example {i+1}: {prompt} =====\")\n    print(f\"You preferred: {chosen}\")\n\n    # Initial model scoring (before training)\n    scores = simulate_reward_scores(response_a, response_b)\n    loss = reward_model_loss(scores, chosen, rejected)\n    print(f\"Loss: {loss:.4f}\")\n\n\n===== Example 1: Explain the concept of reinforcement learning to a high school student. =====\nYou preferred: response_b\nChosen response score: 2.8900\nRejected response score: 3.2500\nScore difference (chosen - rejected): -0.3600\nLoss: 0.8893\n\n===== Example 2: What are some ways to reduce stress? =====\nYou preferred: response_b\nChosen response score: 2.8900\nRejected response score: 2.0000\nScore difference (chosen - rejected): 0.8900\nLoss: 0.3441"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#rlhf-pipeline-implementation",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#rlhf-pipeline-implementation",
    "title": "RLHF for LLMs",
    "section": "RLHF Pipeline Implementation",
    "text": "RLHF Pipeline Implementation\nOnce trained, the reward model replaces the handcrafted reward function we used earlier. During RL training:\n\nThe model generates a response to a prompt\nThe reward model evaluates the response, producing a scalar reward\nThis reward is used to update the model via PPO, just as we did with our algorithmic reward\n\nSo, where before we had\nreward = get_sentiment_reward(response_txt)  # From our sentiment detection function\nNow with RLHF:\nreward = reward_model(prompt, response_txt)  # From our trained reward model\nWhile this change may look simple, it fundamentally transforms how the system learns - from optimizing for predefined metrics to optimizing for learned human preferences.\nSo, the necessary steps for a complete RLHF pipeline include:\n\nInitial LLM Training: Train or fine-tune a base LLM using standard methods\nHuman Preference Collection: Gather human judgments on model outputs\nReward Model Training: Train a reward model to predict human preferences\nRL Fine-tuning: Use the reward model to guide policy optimization"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#practical-considerations-for-scaling-rlhf",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#practical-considerations-for-scaling-rlhf",
    "title": "RLHF for LLMs",
    "section": "Practical Considerations for Scaling RLHF",
    "text": "Practical Considerations for Scaling RLHF\nImplementing RLHF at scale involves several important considerations:\nQuality of Human Feedback: Diverse annotator pools to avoid bias, clear guidelines to ensure consistency, and quality control measures to identify unreliable annotations can all enhance the quality of the data that trains the reward model.\nComputational Requirements: Training a reward model adds another large model to the pipeline, and PPO fine-tuning is more compute-intensive than supervised fine-tuning. Multiple runs may also be needed to find optimal hyperparameters.\nReward Hacking: Models can also learn to exploit weaknesses in the reward model just like they can from an algorithmic model, so it‚Äôs important to regularly update the reward model with new human judgments. Adding KL penalties can help to prevent excessive deviation from the base model.\nDistribution Shift: As the policy model improves, it generates responses outside the reward model‚Äôs training distribution. Iterative approaches that collect new human feedback on improved model outputs help address this.\nHybrid Approaches: Combining RLHF with rule-based rewards for certain constraints can improve overall quality. Multi-objective optimization can balance different desired qualities, and ensemble reward models are sometimes used to capture different aspects of human preferences.\nIn practice, RLHF is often implemented as an iterative process:\n\nTrain initial reward model from human preferences\nPerform RL fine-tuning using this reward model\nGenerate new responses with the improved policy\nCollect new human preferences on these responses\nRetrain or update the reward model 6.Repeat the process\n\nThis iterative approach helps address distribution shift and ensures the reward model keeps pace with policy improvements."
  }
]