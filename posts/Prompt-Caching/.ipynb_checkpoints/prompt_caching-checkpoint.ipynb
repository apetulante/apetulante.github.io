{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prompt Caching\"\n",
    "description: \"A guide to how prompt caching - a modular approach to KV caching. We'll talk through how and why this modular implementation works, and include a practical example of implementing it for LLaMa 3.2 1B.\"\n",
    "author: \"Abbie Petulante\"\n",
    "date: \"2025-01-10\"\n",
    "sort-order: 3\n",
    "categories: [caching, LLMs, inferencing]\n",
    "image: \"images/header.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching with LLaMa 3.2\n",
    "\n",
    "> This notebook provides a detailed exploration of prompt caching using Llama 3.2, based on the paper \"[*Prompt Cache: Modular Attention Reuse for Low-Latency Inference*](https://arxiv.org/pdf/2311.04934)\". We'll implement prompt caching from scratch and demonstrate its benefits for inference speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/apetulante/Tutorials/blob/master/Inferencing/prompt_caching.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Before we continue, let's break down the key concepts from the prompt cache paper.\n",
    "\n",
    "**I highly, highly recommend going through my `kv_caching.ipynb` notebook before this one if you aren't already familiar with kv caching!** I will give a *very* brief overview in this notebook, but for the most part assume that most of the details of why and how KV caching is done are already understood by the reader.\n",
    "\n",
    "The core idea that the paper observes is that many prompts share common segments (like system messages or templates), but these segments get recomputed every time they appear. Even with KV caching, the cache is stored *during generation of a single output* and, while it *can* be re-used for another generation with the identical prompt, it's typically erased once the prompt changes, even if only slightly. The goal of prompt caching is to precompute and store attention states for common segments so that they can be easily reused.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "**Prompt Modules:**\n",
    "\n",
    "Prompt modules are reusable segments of text (like system messages or documents) that are explicitly defined in a schema with their own position IDs, allowing their attention states to be cached and reused. The modules are defined using a markup language that specifies how they can be combined and parameterized, similar to how you might define reusable components in XML.\n",
    "\n",
    "\n",
    "**Schemas:**\n",
    "\n",
    "A schema defines how prompt modules can be combined. They specify what are valid positions and relationships between modules.\n",
    "Example: A schema might specify that a system message comes first, followed by either a code snippet or a document, then a user query. Each of these components of a schema are their own prompt modules.\n",
    "\n",
    "\n",
    "**Storage:**\n",
    "\n",
    "For each module, the actual data that gets stored is:\n",
    "\n",
    "- The K (Key) and V (Value) attention states\n",
    "- The position IDs used when computing those states\n",
    "- The module's boundaries and metadata\n",
    "\n",
    "\n",
    "**Mixing and Matching:**\n",
    "\n",
    "Each module reserves a \"slot\" of position IDs, and when combining modules, original position IDs are preserved within each module.\n",
    "This works because transformer attention is invariant to gaps in position IDs\n",
    "The K,V states from different modules can be concatenated as long as their relative positions are maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Idea:\n",
    "\n",
    "While the underlying caching methodology that's used by prompt caching is just storing the KV states, the concept of prompt modules provides a structured way to:\n",
    "\n",
    "1. Define what pieces of prompts are reusable\n",
    "2. Manage their position assignments\n",
    "3. Control how they can be combined\n",
    "4. Handle variations through parameters\n",
    "\n",
    "\n",
    "Below is a figure from the paper demostrating how traditional generation, generation with KV cache, and generation with a prompt cache are all different.\n",
    "\n",
    "<img src=\"images/paper_fig1.png\" width=\"100%\" style=\"display: block; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Setup\n",
    "Before we get any deeper into code, let's install necessary packages and import dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Set, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# AI/ML Libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n",
      "Available GPU memory: 14.75 GB\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Testing our Model\n",
    "\n",
    "Let's start by loading a small LLM to demonstrate these concepts, and looking at its output before we do anything. We'll use LLaMa 3.2 1B. This model is excellent for this example because:\n",
    "1. It is small enough to run on smaller GPUs\n",
    "2. It uses a relatively simple transformer architecture, making it easier to understand the core concepts\n",
    "3. Despite its small size, it produces coherent enough outputs to demonstrate the effects of caching on generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication\n",
    "\n",
    "LLaMa 3.2 requires authentication with Hugging Face to access the model. You'll need to:\n",
    "1. Have a Hugging Face account\n",
    "2. Accept the LLaMa 3.2 model terms of use on the Hugging Face model page\n",
    "3. Create an access token on Hugging Face (https://huggingface.co/settings/tokens)\n",
    "\n",
    "After you have your access token and have accepted the terms, the code below will help you log in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token: ··········\n",
      "Login status: Authenticated with Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "login(token=token)\n",
    "\n",
    "# Verify login\n",
    "print(\"Login status: Authenticated with Hugging Face\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the LLaMa Architecture\n",
    "\n",
    "Let's break down the Llama architecture with a focus on the parts relevant to prompt caching!\n",
    "\n",
    "**Overall Structure:**\n",
    "\n",
    "- The model has 16 layers of LlamaDecoderLayer (seen in `(0-15): 16 x`)\n",
    "- Each layer has a self-attention block and an MLP block\n",
    "- The embedding dimension is 2048 (seen in many places)\n",
    "\n",
    "\n",
    "**The Most Important Part - The Attention Block:**\n",
    "```\n",
    "(self_attn): LlamaAttention(\n",
    "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "This shows us:\n",
    "\n",
    "- Q (query) projector takes 2048 dimensions to 2048\n",
    "- K (key) projector takes 2048 dimensions to 512\n",
    "- V (value) projector takes 2048 dimensions to 512\n",
    "- The output projector takes 2048 back to 2048\n",
    "\n",
    "The 512 dimension is actually head_dim $\\times$ num_heads. Since $K$ and $V$ are 512, and Llama uses 8 KV heads (we will see this below), each head must be working with 64-dimensional vectors (512/32 = 64).\n",
    "\n",
    "We can see this when we look at the model's config:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "Number of heads: 32\n",
      "Hidden size: 2048\n",
      "Head dimension: 64\n",
      "\n",
      "Attention Component Dimensions:\n",
      "Q projection: torch.Size([2048, 2048])\n",
      "K projection: torch.Size([512, 2048])\n",
      "V projection: torch.Size([512, 2048])\n",
      "\n",
      "Attention Module Attributes:\n",
      "training: False\n",
      "config: LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "layer_idx: 0\n",
      "head_dim: 64\n",
      "num_key_value_groups: 4\n",
      "scaling: 0.125\n",
      "attention_dropout: 0.0\n",
      "is_causal: True\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration\n",
    "config = model.config\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"Number of heads: {config.num_attention_heads}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Head dimension: {config.hidden_size // config.num_attention_heads}\")\n",
    "\n",
    "# Get first layer's attention module\n",
    "first_layer = model.model.layers[0]\n",
    "attention = first_layer.self_attn\n",
    "\n",
    "# Print key dimensions\n",
    "print(\"\\nAttention Component Dimensions:\")\n",
    "print(f\"Q projection: {attention.q_proj.weight.shape}\")\n",
    "print(f\"K projection: {attention.k_proj.weight.shape}\")\n",
    "print(f\"V projection: {attention.v_proj.weight.shape}\")\n",
    "\n",
    "# Print attention attributes\n",
    "print(\"\\nAttention Module Attributes:\")\n",
    "for name, value in vars(attention).items():\n",
    "    if not name.startswith('_'):\n",
    "        print(f\"{name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Note:**\n",
    "Llama 3.2 uses *Grouped Query Attention (GQA)*, where:\n",
    "\n",
    "- There are 32 query ($Q$) heads\n",
    "- But only 8 key-value ($K,V$) heads\n",
    "- So each $K,V$ head is shared by 4 $Q$ heads (32/8 = 4)\n",
    "\n",
    "The config shows 32 heads (the total number of $Q$ heads), but `num_key_value_groups: 4`  means each $K,V$ head is shared by 4 $Q$ heads\n",
    "\n",
    "This also explains in more detail the the projection shapes we saw above:\n",
    "\n",
    "- $Q$ projection: [2048, 2048] -> 32 heads × 64 dimensions = 2048\n",
    "- $K,V$ projections: [512, 2048] -> 8 heads × 64 dimensions = 512\n",
    "\n",
    "So when we're caching $K,V$ states, we only need to store 8 heads worth of information, even though the model is using 32 $Q$ heads during attention computation. This is a memory optimization technique that Llama uses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When caching, we're storing the outputs of the `k_proj` and `v_proj` for each layer. The cache shape would be:\n",
    "\n",
    "Batch dimension    $ \\times $   Number of layers (16) $\\times$     2 tensors per layer ($K$ and $V$)\n",
    "\n",
    "→ For each tensor: `[num_heads=8, seq_len, head_dim=64]`\n",
    "\n",
    "We can see this by examining the cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "0: <|begin_of_text|>\n",
      "1: This\n",
      "2:  is\n",
      "3:  an\n",
      "4:  example\n",
      "5:  input\n",
      "6: .\n",
      "( length: 7 )\n",
      "\n",
      "Cache structure:\n",
      "Number of layers: 16\n",
      "Items per layer: 2  # K and V\n",
      "K states shape: torch.Size([1, 8, 7, 64])\n",
      "V states shape: torch.Size([1, 8, 7, 64])\n"
     ]
    }
   ],
   "source": [
    "example_prompt = \"This is an example input.\"\n",
    "input_ids = tokenizer(example_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "#How the word was tokenized\n",
    "print(\"\\nTokens:\")\n",
    "for i, token_id in enumerate(input_ids[0]):\n",
    "    print(f\"{i}: {tokenizer.decode([token_id])}\")\n",
    "print(\"( length:\", len(input_ids[0]), \")\\n\")\n",
    "\n",
    "outputs = model(\n",
    "    input_ids,\n",
    "    use_cache=True, # the model is smart, and will keep track of what must be cached for us\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "cache = outputs.past_key_values # get those cached values here\n",
    "\n",
    "print(\"Cache structure:\")\n",
    "print(f\"Number of layers: {len(cache)}\")\n",
    "print(f\"Items per layer: {len(cache[0])}  # K and V\")\n",
    "k_states, v_states = cache[0]  # Look at first layer\n",
    "print(f\"K states shape: {k_states.shape}\")\n",
    "print(f\"V states shape: {v_states.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down the dimensions of these output $K$ and $V$ shapes: `torch.Size([1, 8, 7, 64])`\n",
    "\n",
    "- **First dimension [1]**: This is the batch size. Since we're processing one prompt at a time, it's 1.\n",
    "- **Second dimension [8]**: This represents the number of attention heads. Each head can learn to attend to different aspects of the input.\n",
    "- **Third dimension [7]**: Sequence length. We can see that our example prompt was tokenized as 7 tokens.\n",
    "- **Fourth dimension [64]**: This is the dimension per head (head_dim). Each head processes attention in a 64-dimensional space.\n",
    "\n",
    "So when we store $K$ and $V$ states, we're storing:\n",
    "\n",
    "For each layer (16 of them):\n",
    "\n",
    "> For both $K$ and $V$:\n",
    ">\n",
    ">>A tensor that has:\n",
    ">>\n",
    ">>> Each head's (8) representation of each token (seq_length) as a 64-dimensional vector\n",
    ">>>\n",
    ">>> → an `[8,seq_length,64]` vector\n",
    ">>\n",
    ">> → 2 `[8,seq_length,64]` vectors\n",
    ">\n",
    "> → 16 $\\times$ 2 `[8,seq_length,64]` vectors for a given prompt that's tokenized into `seq_length` tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we store in a prompt module - these KV states for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A Basic Implementation\n",
    "\n",
    "Let's move on to actually building a prompt caching implementation. We'll start by going over the core components to prompt caching and exploring how we use them in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Component #1: Prompt Module\n",
    "\n",
    "Let's start by investigating the concept of a prompt module more in depth, and building a simple one.\n",
    "\n",
    "When we cache a prompt module, we're storing:\n",
    "\n",
    "- The module's text and metadata (name, position info, etc.)\n",
    "- All those KV states we just analyzed:\n",
    "\n",
    "  For each of the 16 layers\n",
    "\n",
    "  Both K and V for each layer\n",
    "  \n",
    "  Each with shape [1, 8, seq_length, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A (simple) example prompt module\n",
    "\n",
    "Let's start by building the framework for a very simple prompt module. Here, we'll build a simple prompt module class that can create a cache for us for a given prompt section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePromptModule:\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "        self.cached_kv_states = None\n",
    "        self.input_length = None\n",
    "\n",
    "    def compute_and_cache_states(self, model, tokenizer):\n",
    "        tokens = tokenizer(\n",
    "            self.text,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        self.input_length = tokens.input_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                tokens.input_ids,\n",
    "                attention_mask=tokens.attention_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        self.cached_kv_states = outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 16\n",
      "K shape: torch.Size([1, 8, 8, 64])\n",
      "V shape: torch.Size([1, 8, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "# Create and cache a simple module\n",
    "system_prompt = SimplePromptModule(\"You are a helpful AI assistant.\")\n",
    "system_prompt.compute_and_cache_states(model, tokenizer)\n",
    "\n",
    "# Let's look at what we cached\n",
    "print(\"Number of layers:\", len(system_prompt.cached_kv_states))\n",
    "first_layer_k, first_layer_v = system_prompt.cached_kv_states[0]\n",
    "print(\"K shape:\", first_layer_k.shape)\n",
    "print(\"V shape:\", first_layer_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write a simple function that calls on this cache at generation time.\n",
    "\n",
    "This function is largely copied from the end of my **`kv_caching.ipynb`** notebook, so I won't go into too many details about the exact details of this function, but we'll utilize our saved KV states and feed them back into the model as we generate the tokens that follow our cached system prompt + new prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_cached_prompt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_module: SimplePromptModule,\n",
    "    continuation: str,\n",
    "    max_length: int = 50,\n",
    "    temperature = .7\n",
    "):\n",
    "    # Get tokens for the continuation\n",
    "    continuation_ids = tokenizer.encode(continuation, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # First forward pass using continuation and cached states\n",
    "        outputs = model(\n",
    "            continuation_ids,\n",
    "            past_key_values=prompt_module.cached_kv_states,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        generated_sequence = continuation_ids\n",
    "        generated_text = []\n",
    "\n",
    "        # Generate tokens one at a time\n",
    "        for _ in range(max_length):\n",
    "            # Get logits for next token prediction\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.multinomial(torch.softmax(next_token_logits / temperature, dim=-1), num_samples=1)\n",
    "\n",
    "            # Keep track of the sequence\n",
    "            generated_sequence = torch.cat([generated_sequence, next_token], dim=-1)\n",
    "            generated_text.append(next_token.item())\n",
    "\n",
    "            # Forward pass for next token\n",
    "            outputs = model(\n",
    "                next_token,\n",
    "                use_cache=True,\n",
    "                past_key_values=outputs.past_key_values,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_text, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:  It's a topic that's a little confusing. Explain the basics of artificial intelligence and neural networks, and the different kinds of artificial neural networks. I'm confused about the difference between feedforward and recurrent neural networks. I want to know which type of\n"
     ]
    }
   ],
   "source": [
    "output = generate_with_cached_prompt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    system_prompt, # use the system prompt, which we've now conventiently already cached!\n",
    "    \"Tell me about neural networks.\"\n",
    ")\n",
    "print(\"\\nOutput:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Component #2: Prompt Schemas\n",
    "\n",
    "So far, we've seen just storing the KV states for a single system prompt, and inserting those states at the beginning of generation. We've seen what organizing it as a module could look like, but besides that, our implementation so far has largely been just standard KV caching.\n",
    "\n",
    "The real idea behind prompt caching is that different prompt modules can be mixed and matched together. This is done with **prompt schemas**, which are the higher-level containers that defines:\n",
    "\n",
    "1. What modules exist\n",
    "2. Their positions/ordering\n",
    "3. How they can be combined\n",
    "4. Parameters they can take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A (simple) schema example\n",
    "\n",
    "We'll start with a very simple schema implementation first, since it provides the structural foundation for how modules work.\n",
    "\n",
    "The schema below will give us a minimal foundation that just:\n",
    "\n",
    "- Stores modules by name\n",
    "- Can cache all modules at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSchema:\n",
    "    \"\"\"A basic schema that just stores and manages prompt modules.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.modules = {}  # name -> SimplePromptModule\n",
    "\n",
    "    def add_module(self, name: str, text: str):\n",
    "        \"\"\"Add a module to the schema.\"\"\"\n",
    "        module = SimplePromptModule(text)\n",
    "        self.modules[name] = module\n",
    "        return module\n",
    "\n",
    "    def cache_all(self, model, tokenizer):\n",
    "        \"\"\"Compute and cache states for all modules.\"\"\"\n",
    "        print(f\"Caching states for schema '{self.name}':\")\n",
    "        for name, module in self.modules.items():\n",
    "            print(f\"  Caching module '{name}'...\")\n",
    "            module.compute_and_cache_states(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this schema and add some prompts to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching states for schema 'qa_schema':\n",
      "  Caching module 'system'...\n",
      "  Caching module 'python_doc'...\n"
     ]
    }
   ],
   "source": [
    "# Create a schema\n",
    "schema = SimpleSchema(\"qa_schema\")\n",
    "\n",
    "# Add a few modules\n",
    "schema.add_module(\"system\", \"You are a helpful AI assistant.\")\n",
    "schema.add_module(\"python_doc\", \"Python is a programming language known for its readability.\")\n",
    "\n",
    "# Cache all modules\n",
    "schema.cache_all(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated:  This is your second day and I’m sure you’re going to be amazing. You’re good with math, science, and you have a strong understanding of coding. You are asking the question, “Are you good enough to be an AI assistant?”\n"
     ]
    }
   ],
   "source": [
    "# Try using just one module first (using our working generate_with_cached_prompt)\n",
    "output = generate_with_cached_prompt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    schema.modules[\"system\"],\n",
    "    \"Tell me about coding.\",\n",
    "    max_length=50\n",
    ")\n",
    "print(\"\\nGenerated:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules are part of schemas\n",
    "\n",
    "A module becomes a part of a schema. Indeed, we can inspect the KV states that we were storing for just our module vs for our module now that it's stored as part of a schema, and see that they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original version:\n",
      "First 5 values of K: tensor([ 0.0956,  0.1783,  0.0368, -0.2105, -0.2119], device='cuda:0')\n",
      "First 5 values of V: tensor([ 0.0026,  0.0136, -0.0450, -0.0029, -0.0004], device='cuda:0')\n",
      "\n",
      "Schema version:\n",
      "Caching states for schema 'qa_schema':\n",
      "  Caching module 'system'...\n",
      "First 5 values of K: tensor([ 0.0956,  0.1783,  0.0368, -0.2105, -0.2119], device='cuda:0')\n",
      "First 5 values of V: tensor([ 0.0026,  0.0136, -0.0450, -0.0029, -0.0004], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def inspect_cached_states(prompt_module: SimplePromptModule):\n",
    "    \"\"\"Print the first few values of the cached K,V states.\"\"\"\n",
    "    if prompt_module.cached_kv_states is None:\n",
    "        print(\"No cached states!\")\n",
    "        return\n",
    "\n",
    "    # Look at first layer's first few values\n",
    "    k, v = prompt_module.cached_kv_states[0]  # First layer\n",
    "    print(f\"First 5 values of K: {k[0, 0, 0, :5]}\")  # First batch, head, token, first 5 dims\n",
    "    print(f\"First 5 values of V: {v[0, 0, 0, :5]}\")\n",
    "\n",
    "# Test prompt module\n",
    "print(\"Original version:\")\n",
    "system_prompt = SimplePromptModule(\"You are a helpful AI assistant.\")\n",
    "system_prompt.compute_and_cache_states(model, tokenizer)\n",
    "inspect_cached_states(system_prompt)\n",
    "\n",
    "# Test schema version\n",
    "print(\"\\nSchema version:\")\n",
    "schema = SimpleSchema(\"qa_schema\")\n",
    "schema.add_module(\"system\", \"You are a helpful AI assistant.\")\n",
    "schema.cache_all(model, tokenizer)\n",
    "inspect_cached_states(schema.modules[\"system\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Addtional Functionalities\n",
    "\n",
    "Now that we've gotten the basics down, it's time to add enough complexity to take full advantage of prompt caching.\n",
    "\n",
    "Although so far we've written a prompt module and wrapped it in a schema, we still have:\n",
    "\n",
    "- No position management - can't control where modules go in relation to each other\n",
    "- No module parameters - text is static\n",
    "- Can't combine multiple modules yet\n",
    "- No validation of module combinations\n",
    "\n",
    "Let's build out a prompt module / schema system that allows this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Handling\n",
    "\n",
    "We want to combine multiple prompt modules to fully take advantage of prompt caching. In order to do that, we need to manage the relative positions of our modules. And it isn't just about order - it's about preserving the exact positions that were used when we cached the attention states. To do so:\n",
    "\n",
    "- Each module gets assigned a range of position IDs\n",
    "- These positions determine where modules can go in relation to each other\n",
    "- When combining modules, their relative positions must be maintained\n",
    "- We can have gaps between positions - the attention mechanism can handle discontinuous position IDs\n",
    "\n",
    "This matters because, in transformer models like Llama:\n",
    "\n",
    "1. Each token's attention calculation includes positional information\n",
    "2. When we cache K,V states, they include this positional information\n",
    "3. To reuse cached states, we need to use them in positions that match how they were cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a position class that tracks:\n",
    "\n",
    "- Where a module starts (start)\n",
    "- How long it is (length)\n",
    "- Where it ends (end)\n",
    "\n",
    "Then, each module reserves a range of positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Position:\n",
    "    \"\"\"Tracks the position range for a module.\"\"\"\n",
    "    start: int  # Starting position of the module\n",
    "    length: int  # Length of the module in tokens\n",
    "\n",
    "    @property\n",
    "    def end(self) -> int:\n",
    "        \"\"\"End position of the module (exclusive).\"\"\"\n",
    "        return self.start + self.length\n",
    "\n",
    "    def overlaps_with(self, other: 'Position') -> bool:\n",
    "        \"\"\"Check if this position overlaps with another.\"\"\"\n",
    "        return not (self.end <= other.start or other.end <= self.start)\n",
    "\n",
    "def validate_module_sequence(positions: List[Position]):\n",
    "    \"\"\"\n",
    "    Validate that a sequence of module positions does not overlap.\n",
    "    Ensures that precomputed KV states are positionally consistent.\n",
    "    \"\"\"\n",
    "    for i in range(len(positions)):\n",
    "        for j in range(i + 1, len(positions)):\n",
    "            if positions[i].overlaps_with(positions[j]):\n",
    "                raise ValueError(\n",
    "                    f\"Position conflict: {positions[i]} overlaps with {positions[j]}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also validate our positions, to make sure that nothing is trying to insert into the same slot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught expected error: Position conflict: Position(start=0, length=10) overlaps with Position(start=5, length=10)\n"
     ]
    }
   ],
   "source": [
    "system_pos = Position(start=0, length=10)\n",
    "context_pos = Position(start=20, length=15)  # Note gap between 10-20\n",
    "user_pos = Position(start=40, length=5)     # Note gap between 35-40\n",
    "\n",
    "# This should work:\n",
    "validate_module_sequence([system_pos, context_pos])\n",
    "validate_module_sequence([context_pos, user_pos])\n",
    "\n",
    "# This would raise an error:\n",
    "try:\n",
    "    conflicting_pos = Position(start=5, length=10)  # Overlaps with system_pos\n",
    "    validate_module_sequence([system_pos, conflicting_pos])\n",
    "except ValueError as e:\n",
    "    print(f\"Caught expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking \"Why doesn't attention care about discontinuous position IDs?\"\n",
    "\n",
    "When a transformer processes tokens, each token's position is used to calculate positional embeddings that inform how that token attends to other tokens. In Llama's case, this uses rotary position embeddings (RoPE).\n",
    "The key insight is: attention calculations only care about relative positions between tokens, not absolute positions. When token A at position 5 attends to token B at position 3, what matters is their relative distance (2 positions), not their absolute positions.\n",
    "This means:\n",
    "\n",
    "- Sequence [0,1,2,3] and sequence [100,101,102,103] will produce the same attention patterns\n",
    "- A gap like [0,1,2,10,11,12] doesn't disrupt attention - tokens still know their relative positions to each other\n",
    "- The model never assumes positions are continuous - it just uses whatever positions it's given to calculate relative distances\n",
    "\n",
    "Therefore, when we cache KV states from discontinuous positions and combine them, each token's stored states still contain the correct relative position information they had when they were cached. The attention mechanism can use these just fine, as it only cares about preserving those relative relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_states_with_positions(\n",
    "    cached_states_list: List[Tuple[List[Tuple[torch.Tensor, torch.Tensor]], Position]]\n",
    ") -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Combine KV states from multiple modules, respecting their positions.\n",
    "\n",
    "    Args:\n",
    "        cached_states_list: List of (cached_states, position) pairs\n",
    "            cached_states is the KV states for one module\n",
    "            position is where those states should go\n",
    "    \"\"\"\n",
    "    # Sort by position\n",
    "    cached_states_list = sorted(cached_states_list, key=lambda x: x[1].start)\n",
    "\n",
    "    # Validate no overlaps\n",
    "    positions = [pos for _, pos in cached_states_list]\n",
    "    validate_module_sequence(positions)\n",
    "\n",
    "    # Initialize with first module's states\n",
    "    first_states, _ = cached_states_list[0]\n",
    "    combined_states = list(first_states)\n",
    "\n",
    "    # Add remaining modules' states\n",
    "    for module_states, _ in cached_states_list[1:]:\n",
    "        for layer_idx in range(len(combined_states)):\n",
    "            k_combined, v_combined = combined_states[layer_idx]\n",
    "            k_module, v_module = module_states[layer_idx]\n",
    "\n",
    "            # Concatenate along sequence dimension\n",
    "            combined_states[layer_idx] = (\n",
    "                torch.cat([k_combined, k_module], dim=2),\n",
    "                torch.cat([v_combined, v_module], dim=2)\n",
    "            )\n",
    "\n",
    "    return combined_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined K shape: torch.Size([1, 8, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Demonstrate how positions affect state combination.\"\"\"\n",
    "# Create dummy states (normally these would be real cached states)\n",
    "batch_size, n_heads = 1, 8\n",
    "head_dim = 64\n",
    "\n",
    "def make_dummy_states(seq_len: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"Create dummy KV states for testing.\"\"\"\n",
    "    states = []\n",
    "    for _ in range(16):  # 16 layers\n",
    "        k = torch.randn(batch_size, n_heads, seq_len, head_dim)\n",
    "        v = torch.randn(batch_size, n_heads, seq_len, head_dim)\n",
    "        states.append((k, v))\n",
    "    return states\n",
    "\n",
    "# Create states for three modules\n",
    "states_1 = make_dummy_states(seq_len=5)  # 5 tokens\n",
    "pos_1 = Position(0, 5)\n",
    "\n",
    "states_2 = make_dummy_states(seq_len=3)  # 3 tokens\n",
    "pos_2 = Position(10, 3)  # Note gap between pos_1 and pos_2\n",
    "\n",
    "states_3 = make_dummy_states(seq_len=4)  # 4 tokens\n",
    "pos_3 = Position(15, 4)\n",
    "\n",
    "# Combine states\n",
    "combined = combine_states_with_positions([\n",
    "    (states_1, pos_1),\n",
    "    (states_2, pos_2),\n",
    "    (states_3, pos_3)\n",
    "])\n",
    "\n",
    "# Check shapes\n",
    "k, v = combined[0]  # First layer\n",
    "print(f\"Combined K shape: {k.shape}\")  # Should show total sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the final concatenated sequence, there aren't actually any gaps. What matters is:\n",
    "\n",
    "- The KV states themselves (which have sequence lengths 5, 3, and 4)\n",
    "- The relative positions between tokens within each module (which are preserved no matter what absolute positions we assign)\n",
    "\n",
    "When we concatenate the KV states, we just put them next to each other in the order we want. The absolute position numbers (0, 10, 15) vs if we had done for instance (0, 5, 13) don't affect the final sequence - they're just a way to:\n",
    "\n",
    "- Express ordering (what comes first)\n",
    "- Allow for validation (making sure things don't overlap)\n",
    "\n",
    "So why do we do it this way?\n",
    "\n",
    "1. **To keep things more flexible.** Extra position IDs are reserved to accommodate parameters of different lengths. For example, if you have a module that might take a name that could be 1-3 tokens long, you  reserve 3 positions even if the current parameter only uses 1.\n",
    "2. **To better organize schemas.** Ranges of positions (like 0-99, 100-999) help to organize different types of modules, making it easier to keep similar modules in similar position ranges and allow for future additions of new modules with variable lengths.\n",
    "\n",
    "Basically, we want \"You are a helpful assistant\" to be able to slot into the same spot as \"You are a kind, friendly, and sociable chat bot.\" even though they have different lengths. This is the essence of the modularity of prompt caching coming into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Module Combinations\n",
    "\n",
    "The position code that we just wrote includes validation that positions don't overlap, but we don't yet have rules about which modules can be combined beyond that. For example, in the paper they describe schemas that might specify:\n",
    "\n",
    "- Required modules (e.g., must have a system message)\n",
    "- Order dependencies (e.g., context must come before query)\n",
    "- Module compatibility (e.g., don't want both Python and SQL documentation modules together)\n",
    "- Optional modules\n",
    "- Module groups where only one can be used\n",
    "\n",
    "In general, which of these combination rules is necessary may depend on the prompts being cached. Here, we'll explore how to implement some of these more commonly encountered scenarios in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleType(Enum):\n",
    "    \"\"\"Different types of modules that may have different combination rules.\"\"\"\n",
    "    SYSTEM = \"system\"       # System messages, instructions\n",
    "    CONTEXT = \"context\"     # Documents, background info\n",
    "    QUERY = \"query\"        # User questions/inputs\n",
    "    FORMAT = \"format\"      # Output format instructions\n",
    "\n",
    "@dataclass\n",
    "class CombinationRule:\n",
    "    \"\"\"Defines how a module can be combined with others.\"\"\"\n",
    "    module_name: str\n",
    "    required: bool = False                    # Must this module be included?\n",
    "    must_follow: Optional[Set[str]] = None    # Modules that must come before this one\n",
    "    cannot_combine: Optional[Set[str]] = None # Modules this can't be used with\n",
    "\n",
    "    def validate_combination(self, module_sequence: List[str]):\n",
    "        \"\"\"Check if this module's position in the sequence follows rules.\"\"\"\n",
    "        if self.required and self.module_name not in module_sequence:\n",
    "            raise ValueError(f\"Required module {self.module_name} is missing\")\n",
    "\n",
    "        if self.module_name in module_sequence:\n",
    "            current_pos = module_sequence.index(self.module_name)\n",
    "\n",
    "            # Check ordering rules\n",
    "            if self.must_follow:\n",
    "                for prerequisite in self.must_follow:\n",
    "                    if prerequisite not in module_sequence[:current_pos]:\n",
    "                        raise ValueError(\n",
    "                            f\"Module {self.module_name} must follow {prerequisite}\"\n",
    "                        )\n",
    "\n",
    "            # Check incompatible modules\n",
    "            if self.cannot_combine:\n",
    "                for incompatible in self.cannot_combine:\n",
    "                    if incompatible in module_sequence:\n",
    "                        raise ValueError(\n",
    "                            f\"Module {self.module_name} cannot be combined with {incompatible}\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid sequence passed!\n",
      "\n",
      "Caught expected error: Required module system_message is missing\n"
     ]
    }
   ],
   "source": [
    "# Define some rules\n",
    "system_rule = CombinationRule(\n",
    "    \"system_message\",\n",
    "    required=True  # Must have system message\n",
    ")\n",
    "\n",
    "python_doc_rule = CombinationRule(\n",
    "    \"python_doc\",\n",
    "    must_follow={\"system_message\"},  # Must come after system message\n",
    "    cannot_combine={\"sql_doc\"}       # Can't use with SQL doc\n",
    ")\n",
    "\n",
    "query_rule = CombinationRule(\n",
    "    \"user_query\",\n",
    "    required=True,\n",
    "    must_follow={\"system_message\"}  # Must come after system\n",
    ")\n",
    "\n",
    "# Test valid sequence\n",
    "valid_sequence = [\"system_message\", \"python_doc\", \"user_query\"]\n",
    "for rule in [system_rule, python_doc_rule, query_rule]:\n",
    "    rule.validate_combination(valid_sequence)\n",
    "print(\"Valid sequence passed!\\n\")\n",
    "\n",
    "# Test invalid sequence (missing required system message)\n",
    "try:\n",
    "    invalid_sequence = [\"python_doc\", \"user_query\"]\n",
    "    system_rule.validate_combination(invalid_sequence)\n",
    "except ValueError as e:\n",
    "    print(f\"Caught expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fillable Parameters\n",
    "\n",
    "Often times, most of a prompt wants to remain the same, with a few fillable parameters being swapped out.\n",
    "\n",
    "For example, if you wanted to build modular prompts to help write code, you might want the specific programming lagnuage to be swappable in the system prompt (\"Explain this python code\" / \"Explain this C code\".\n",
    "\n",
    "In these instances, we want to add parameters to our prompt modules, which are spaces left empty and able to accommodate an array of potential inputs in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Parameter:\n",
    "    \"\"\"A parameter in a module that gets filled in at runtime.\"\"\"\n",
    "    name: str\n",
    "    max_length: int  # Maximum number of tokens this parameter can use\n",
    "    default: Optional[str] = None\n",
    "\n",
    "class ParameterizedModule:\n",
    "    \"\"\"A module that can have parameters.\"\"\"\n",
    "    def __init__(self, name: str, template: str):\n",
    "        self.name = name\n",
    "        self.template = template\n",
    "        self.parameters: Dict[str, Parameter] = {}\n",
    "\n",
    "        # Find parameters in template\n",
    "        # Format: {param_name:max_length}\n",
    "        param_pattern = r'\\{(\\w+):(\\d+)\\}'\n",
    "        for match in re.finditer(param_pattern, template):\n",
    "            param_name, max_length = match.groups()\n",
    "            self.parameters[param_name] = Parameter(\n",
    "                name=param_name,\n",
    "                max_length=int(max_length)\n",
    "            )\n",
    "\n",
    "    def fill_parameters(self, tokenizer, **kwargs) -> str:\n",
    "        \"\"\"Fill in parameters and validate their lengths.\"\"\"\n",
    "        result = self.template\n",
    "\n",
    "        for param_name, value in kwargs.items():\n",
    "            if param_name not in self.parameters:\n",
    "                raise ValueError(f\"Unknown parameter: {param_name}\")\n",
    "\n",
    "            # Check length\n",
    "            param = self.parameters[param_name]\n",
    "            tokens = tokenizer(value)['input_ids']\n",
    "            if len(tokens) > param.max_length:\n",
    "                raise ValueError(\n",
    "                    f\"Value for {param_name} uses {len(tokens)} tokens, \"\n",
    "                    f\"max allowed is {param.max_length}\"\n",
    "                )\n",
    "\n",
    "            # Replace in template\n",
    "            result = result.replace(f\"{{{param_name}:{param.max_length}}}\", value)\n",
    "\n",
    "        # Fill any remaining parameters with defaults\n",
    "        for name, param in self.parameters.items():\n",
    "            if name not in kwargs and param.default is not None:\n",
    "                result = result.replace(\n",
    "                    f\"{{{name}:{param.max_length}}}\",\n",
    "                    param.default\n",
    "                )\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's experiment with building a parametrized module.\n",
    "\n",
    "Feel free to experiment with this cell: see what happens if you change the number of tokens allowed for the different parameters, or what happens if you try different (like too long, for instance) parameters when filling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled template: Explain this Python code:\n",
      "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\n",
      "Focus on recursion\n"
     ]
    }
   ],
   "source": [
    "# Create a parameterized module\n",
    "code_explain = ParameterizedModule(\n",
    "    \"explain_code\",\n",
    "    template=\"Explain this {language:5} code:\\n{code:50}\\nFocus on {aspect:3}\"\n",
    ")\n",
    "\n",
    "# Fill parameters\n",
    "filled = code_explain.fill_parameters(\n",
    "    tokenizer,\n",
    "    language=\"Python\",\n",
    "    code=\"def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\",\n",
    "    aspect=\"recursion\"\n",
    ")\n",
    "print(\"Filled template:\", filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Full Implementation\n",
    "\n",
    "Now, we're ready to put all that we've disussed so far together into a complete implementation of prompt caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptModule Class\n",
    "This class represents reusable segments of a prompt with parameters that can be dynamically filled in at runtime. It allows:\n",
    "- Parsing parameters from templates (e.g., `{param_name:max_length}`).\n",
    "- Validating and substituting parameter values.\n",
    "- Precomputing and caching key-value (KV) attention states for efficiency.\n",
    "\n",
    "This will be used to define modular prompt components that can be cached and reused during generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptModule:\n",
    "    \"\"\"\n",
    "    Represents a reusable segment of a prompt (a prompt module).\n",
    "\n",
    "    - Each module has a name, a text template, and a position in the overall schema.\n",
    "    - Parameters in the module template allow dynamic customization at runtime.\n",
    "    - Precomputed KV states for the module can be cached for reuse, improving efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, template: str, position: Position):\n",
    "        \"\"\"\n",
    "        Initialize the module with a name, template, and position.\n",
    "\n",
    "        Args:\n",
    "            name (str): Unique identifier for the module.\n",
    "            template (str): Text template for the module. Parameters are written as {param_name:max_length}.\n",
    "            position (Position): Specifies the starting position and length of the module.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.template = template\n",
    "        self.position = position\n",
    "        self.parameters: Dict[str, Parameter] = {}  # Parameter details (name, max length, default value)\n",
    "        self.cached_kv_states = None  # Stores precomputed KV states (key-value attention states)\n",
    "\n",
    "        # Find parameters in template\n",
    "        # Format: {param_name:max_length}\n",
    "        param_pattern = r'\\{(\\w+):(\\d+)\\}'\n",
    "        for match in re.finditer(param_pattern, template):\n",
    "            param_name, max_length = match.groups()\n",
    "            self.parameters[param_name] = Parameter(\n",
    "                name=param_name,\n",
    "                max_length=int(max_length)\n",
    "            )\n",
    "\n",
    "    def fill_parameters(self, tokenizer, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Fill the template parameters with user-provided values.\n",
    "\n",
    "        - Validates parameter lengths against the specified maximum token lengths.\n",
    "        - Fills remaining parameters with defaults if available.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: Tokenizer for the model to compute token lengths.\n",
    "            kwargs: Parameter values to replace in the template.\n",
    "\n",
    "        Returns:\n",
    "            str: The filled-in template.\n",
    "        \"\"\"\n",
    "        result = self.template\n",
    "\n",
    "        for param_name, value in kwargs.items():\n",
    "            if param_name not in self.parameters:\n",
    "                raise ValueError(f\"Unknown parameter: {param_name}\")\n",
    "\n",
    "            # Check length\n",
    "            param = self.parameters[param_name]\n",
    "            tokens = tokenizer(value)['input_ids']\n",
    "            if len(tokens) > param.max_length:\n",
    "                raise ValueError(\n",
    "                    f\"Value for {param_name} uses {len(tokens)} tokens, \"\n",
    "                    f\"max allowed is {param.max_length}\"\n",
    "                )\n",
    "\n",
    "            # Replace in template\n",
    "            result = result.replace(f\"{{{param_name}:{param.max_length}}}\", value)\n",
    "\n",
    "        # Fill any remaining parameters with defaults\n",
    "        for name, param in self.parameters.items():\n",
    "            if name not in kwargs and param.default is not None:\n",
    "                result = result.replace(\n",
    "                    f\"{{{name}:{param.max_length}}}\",\n",
    "                    param.default\n",
    "                )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_and_cache_states(self, model, tokenizer, **param_values):\n",
    "        \"\"\"\n",
    "        Compute and cache the KV attention states for the module.\n",
    "\n",
    "        - Fills in template parameters to create the final text.\n",
    "        - Tokenizes the text and computes KV states using the model.\n",
    "\n",
    "        Args:\n",
    "            model: The language model used for computing KV states.\n",
    "            tokenizer: Tokenizer for preparing input to the model.\n",
    "            param_values: Values for the template parameters.\n",
    "        \"\"\"\n",
    "        # Fill in template parameters\n",
    "        filled_text = self.fill_parameters(tokenizer, **param_values)\n",
    "\n",
    "        # Tokenize the filled template\n",
    "        tokens = tokenizer(filled_text, return_tensors='pt').to(model.device)\n",
    "        input_ids = tokens.input_ids\n",
    "\n",
    "        # Compute KV states using the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                use_cache=True,  # Ensures KV states are returned\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        # Store the KV states in cache\n",
    "        self.cached_kv_states = outputs.past_key_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Class\n",
    "This class organizes and manages a collection of `PromptModule` objects and their relationships. Key features include:\n",
    "- Storing prompt modules and defining their positional constraints.\n",
    "- Adding and configuring modules with rules for ordering and combination.\n",
    "- Caching precomputed key-value (KV) attention states for all modules using stored parameters.\n",
    "- Validating module sequences to ensure compliance with rules and positional constraints.\n",
    "\n",
    "The `Schema` class serves as the central structure for managing modular prompts and ensuring efficient generation workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schema:\n",
    "    \"\"\"\n",
    "    Represents a collection of prompt modules and their relationships.\n",
    "    Manages KV state caching and enforces module combination rules.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.modules: Dict[str, PromptModule] = {}\n",
    "        self.rules: Dict[str, CombinationRule] = {}\n",
    "        self.next_position = 0\n",
    "        self.module_params: Dict[str, Dict] = {}  # Store module parameters here\n",
    "\n",
    "    def set_module_params(self, module_params: Dict[str, Dict]):\n",
    "        \"\"\"\n",
    "        Merge new module parameters with existing ones.\n",
    "        Args:\n",
    "            module_params (Dict[str, Dict]): Parameter values for each module.\n",
    "        \"\"\"\n",
    "        for module_name, params in module_params.items():\n",
    "            if module_name in self.module_params:\n",
    "                # Update existing parameters\n",
    "                self.module_params[module_name].update(params)\n",
    "            else:\n",
    "                # Add new module parameters\n",
    "                self.module_params[module_name] = params\n",
    "\n",
    "\n",
    "    def add_module(self, name: str, template: str, length_reserve: int, rule=None):\n",
    "        \"\"\"\n",
    "        Add a module to the schema with its position and optional rules.\n",
    "        \"\"\"\n",
    "        position = Position(self.next_position, length_reserve)\n",
    "        module = PromptModule(name, template, position)\n",
    "        self.modules[name] = module\n",
    "\n",
    "        if rule:\n",
    "            self.rules[name] = rule\n",
    "\n",
    "        self.next_position += length_reserve\n",
    "        return module\n",
    "\n",
    "    def cache_all(self, model, tokenizer):\n",
    "        \"\"\"\n",
    "        Precompute KV states for all modules using stored module_params.\n",
    "        \"\"\"\n",
    "        for name, module in self.modules.items():\n",
    "            print(f\"Caching module '{name}'...\")\n",
    "            param_values = self.module_params.get(name, {})\n",
    "            module.compute_and_cache_states(model, tokenizer, **param_values)\n",
    "\n",
    "\n",
    "    def validate_module_sequence(self, module_names: List[str]):\n",
    "        \"\"\"\n",
    "        Ensure the sequence of modules satisfies all rules and positional constraints.\n",
    "        \"\"\"\n",
    "        for rule in self.rules.values():\n",
    "            rule.validate_combination(module_names)\n",
    "\n",
    "        positions = [self.modules[name].position for name in module_names]\n",
    "        validate_module_sequence(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `generate_with_modules` Function\n",
    "This function generates text using a schema of cached prompt modules and a continuation text. Key steps include:\n",
    "- **Validation**: Ensures the selected modules comply with schema rules and positional constraints.\n",
    "- **Combination**: Merges the cached key-value (KV) attention states of the selected modules for efficient generation.\n",
    "- **Continuation Generation**: Appends user-provided text (`continuation`) and generates tokens autoregressively using the combined KV states.\n",
    "\n",
    "This function is the core of the generation process, leveraging precomputed states to reduce latency while ensuring modular flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_modules(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    schema: Schema,\n",
    "    module_names: List[str],\n",
    "    continuation: str,\n",
    "    module_params: Dict[str, Dict] = None,\n",
    "    max_length: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text by combining KV states from multiple modules.\n",
    "\n",
    "    - Validates the module sequence based on schema rules and positions.\n",
    "    - Combines cached KV states for the selected modules.\n",
    "    - Generates continuation text based on the combined KV states.\n",
    "\n",
    "    Args:\n",
    "        model: The language model used for text generation.\n",
    "        tokenizer: Tokenizer for preparing text inputs to the model.\n",
    "        schema (Schema): The schema containing the modules.\n",
    "        module_names (List[str]): Ordered list of module names to include in the prompt.\n",
    "        continuation (str): Text to append after the modules for generation.\n",
    "        module_params (Dict[str, Dict], optional): Parameter values for each module.\n",
    "        max_length (int, optional): Maximum length of generated text.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "   # Validate module sequence\n",
    "    schema.validate_module_sequence(module_names)\n",
    "    modules = [schema.modules[name] for name in module_names]\n",
    "\n",
    "    # Combine cached KV states from the selected modules\n",
    "    first_states = modules[0].cached_kv_states\n",
    "    combined_states = []\n",
    "\n",
    "    for layer_idx in range(len(first_states)):\n",
    "        k1, v1 = first_states[layer_idx]\n",
    "        k_combined, v_combined = k1, v1\n",
    "\n",
    "        for module in modules[1:]:\n",
    "            k_module, v_module = module.cached_kv_states[layer_idx]\n",
    "            k_combined = torch.cat([k_combined, k_module], dim=2)\n",
    "            v_combined = torch.cat([v_combined, v_module], dim=2)\n",
    "\n",
    "        combined_states.append((k_combined, v_combined))\n",
    "\n",
    "    # Create a dynamic cache for the combined states\n",
    "    past_key_values = DynamicCache.from_legacy_cache(combined_states)\n",
    "\n",
    "    # Tokenize the continuation text\n",
    "    continuation_ids = tokenizer.encode(continuation, return_tensors='pt').to(model.device)\n",
    "\n",
    "    # Generate tokens autoregressively\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            continuation_ids,\n",
    "            use_cache=True,\n",
    "            past_key_values=past_key_values,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        generated_sequence = continuation_ids\n",
    "        generated_text = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(torch.softmax(next_token_logits, dim=-1)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            generated_sequence = torch.cat([generated_sequence, next_token], dim=-1)\n",
    "            generated_text.append(next_token.item())\n",
    "\n",
    "            outputs = model(\n",
    "                next_token,\n",
    "                use_cache=True,\n",
    "                past_key_values=outputs.past_key_values,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_text, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Using a Schema for Modular Prompt Caching\n",
    "\n",
    "This example demonstrates how to:\n",
    "1. Create a schema (`coding_qa`) for a Q&A task.\n",
    "2. Add reusable prompt modules to the schema, such as `system` (AI instructions) and `python_doc` (context about Python).\n",
    "3. Set parameters for these modules (e.g., `style` and `desc`).\n",
    "4. Precompute and cache the key-value (KV) attention states for all modules.\n",
    "5. Generate text using the schema and the cached modules.\n",
    "\n",
    "This setup showcases how reusable modular prompts can enhance efficiency while maintaining flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching module 'system'...\n",
      "Caching module 'python_doc'...\n",
      "Generated:  I want to write a function that takes a list of strings and returns a list of strings. I want to use the function to create a list of strings that are the concatenation of the strings in the input list. I want to use the function\n"
     ]
    }
   ],
   "source": [
    "# Create schema\n",
    "schema = Schema(\"coding_qa\")\n",
    "\n",
    "# Add modules\n",
    "schema.add_module(\n",
    "    \"system\",\n",
    "    \"You are a helpful AI assistant. {style:10}\",\n",
    "    length_reserve=20,\n",
    "    rule=CombinationRule(\"system\", required=True)\n",
    ")\n",
    "\n",
    "schema.add_module(\n",
    "    \"python_doc\",\n",
    "    \"Python is a {desc:20} programming language.\",\n",
    "    length_reserve=30,\n",
    "    rule=CombinationRule(\"python_doc\", must_follow={\"system\"})\n",
    ")\n",
    "\n",
    "# Set module parameters\n",
    "schema.set_module_params({\n",
    "    \"system\": {\"style\": \"Be concise\"},\n",
    "    \"python_doc\": {\"desc\": \"high-level interpreted\"}\n",
    "})\n",
    "\n",
    "# Cache all modules\n",
    "schema.cache_all(model, tokenizer)\n",
    "\n",
    "# Generate using modules\n",
    "output = generate_with_modules(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    schema,\n",
    "    module_names=[\"system\", \"python_doc\"],\n",
    "    continuation=\"How do I write a function?\"\n",
    ")\n",
    "\n",
    "print(\"Generated:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending and Modifying Schemas\n",
    "\n",
    "Here’s how you can create a different schema or add new modules to an existing schema to expand the cache dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Creating a Different Schema**\n",
    "If a user wants to set up a new schema, they can follow the same workflow as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching module 'system'...\n",
      "Caching module 'example'...\n",
      "Generated:  I am not sure how to solve this one. I have a quadratic equation with 2x^2 + 2x + 1 = 0. I know that the solutions are x = -1 and x = -2/2.\n"
     ]
    }
   ],
   "source": [
    "# Create a new schema for a different purpose\n",
    "new_schema = Schema(\"math_tutor\")\n",
    "\n",
    "# Add modules\n",
    "new_schema.add_module(\n",
    "    \"system\",\n",
    "    \"You are a helpful math tutor. {style:10}\",\n",
    "    length_reserve=20,\n",
    "    rule=CombinationRule(\"system\", required=True)\n",
    ")\n",
    "\n",
    "new_schema.add_module(\n",
    "    \"example\",\n",
    "    \"Here is an example: {example_text:50}\",\n",
    "    length_reserve=60\n",
    ")\n",
    "\n",
    "# Set parameters for the new schema\n",
    "new_schema.set_module_params({\n",
    "    \"system\": {\"style\": \"Explain thoroughly\"},\n",
    "    \"example\": {\"example_text\": \"How to solve quadratic equations\"}\n",
    "})\n",
    "\n",
    "# Cache all modules in the new schema\n",
    "new_schema.cache_all(model, tokenizer)\n",
    "\n",
    "# Generate using the new schema\n",
    "output = generate_with_modules(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    new_schema,\n",
    "    module_names=[\"system\", \"example\"],\n",
    "    continuation=\"Can you provide another example?\"\n",
    ")\n",
    "\n",
    "print(\"Generated:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modifying an Existing Schema**\n",
    "\n",
    "Or, you can add to the cache of an existing schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching module 'system'...\n",
      "Caching module 'python_doc'...\n",
      "Caching module 'additional_doc'...\n",
      "Generated with new module: .\n",
      "A decorator is a function that adds functionality to another function. Decorators are used to add functionality to functions. Decorators are used to add functionality to functions. Decorators are used to add functionality to functions. Decorators are used to add functionality\n"
     ]
    }
   ],
   "source": [
    "# Add a new module to the existing schema\n",
    "schema.add_module(\n",
    "    \"additional_doc\",\n",
    "    \"This document explains advanced {topic:15}.\",\n",
    "    length_reserve=40,\n",
    "    rule=CombinationRule(\"additional_doc\", must_follow={\"system\"})\n",
    ")\n",
    "\n",
    "# Update parameters for the new module\n",
    "schema.set_module_params({\n",
    "    \"additional_doc\": {\"topic\": \"Python features\"}\n",
    "})\n",
    "\n",
    "# Cache the new module\n",
    "schema.cache_all(model, tokenizer)\n",
    "\n",
    "# Generate using the updated schema\n",
    "output = generate_with_modules(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    schema,\n",
    "    module_names=[\"system\", \"python_doc\", \"additional_doc\"],\n",
    "    continuation=\"Tell me about Python decorators\"\n",
    ")\n",
    "\n",
    "print(\"Generated with new module:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "And that's it! Here we've implemented a basic version of the concepts covered in the Prompt Caching paper.\n",
    "\n",
    "Full disclosure, here are some things we left on the table:\n",
    "\n",
    "1. **Scaffolding**:\n",
    "   - Scaffolding allows semantically dependent modules to share the same attention span by encoding them together as a group.\n",
    "   - This can be useful when modules rely on shared context, ensuring that interdependent relationships between modules are preserved.\n",
    "   - Example Use Case: Combining a system module with an example module that references the system instructions.\n",
    "\n",
    "2. **Union Modules**:\n",
    "   - Union modules represent mutually exclusive sets of prompt modules (e.g., `{module1, module2}`).\n",
    "   - These are particularly useful when users need to choose one module from a group but not all at the same time.\n",
    "   - Example Use Case: Switching between different document languages (e.g., `doc-en-US` vs. `doc-zh-CN`).\n",
    "\n",
    "3. **Parameterized Buffers**:\n",
    "   - Parameters in schemas can include buffers (i.e., placeholders with flexible token limits) that allow users to add arbitrary content dynamically.\n",
    "   - This could enhance reuse by supporting slight variations in module content without needing a new module.\n",
    "   - Example Use Case: Allowing a prompt to include user-provided examples or instructions dynamically.\n",
    "\n",
    "And some additional optimization techniques, including:\n",
    "\n",
    "1. **Dynamic Cache Optimization**:\n",
    "   - The paper explores strategies for optimizing memory usage by sharing KV states across concurrent requests (e.g., in batched inference scenarios).\n",
    "   - These optimizations can significantly reduce memory overhead and improve throughput for large-scale deployments.\n",
    "\n",
    "2. **Compression for KV States**:\n",
    "   - Compression techniques for KV states are suggested to reduce the memory footprint of cached states, especially for long-context LLMs or large schemas.\n",
    "   - Example Use Case: Compressing cached KV states for modules stored in CPU memory for efficient retrieval.\n",
    "\n",
    "These concepts can further enhance flexibility, modularity, and performance in real-world applications, especially for systems with complex or large-scale prompts. So, while too complex for the introduction here, they are still concepts that can greatly enhance the speed of our LLM's!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
