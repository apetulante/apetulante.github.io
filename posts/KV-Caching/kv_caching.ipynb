{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"KV Caching\"\n",
    "description: \"A guide to how KV caching is implemented for LLMs, including a practical example of implementing it for LLaMa 3.2 1B.\"\n",
    "author: \"Abbie Petulante\"\n",
    "date: \"2025-01-15\"\n",
    "sort-order: 1\n",
    "categories: [caching, LLMs]\n",
    "image: \"images/header.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEZ8ZfE_5qS-"
   },
   "source": [
    "# Understanding KV Caching\n",
    "\n",
    "> This notebook provides a hands-on exploration of KV caching in language model text generation, specifically using LLaMa 3.2 1B. We'll examine how caching works, its benefits for inference speed, and its implications for model state management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhNNomfA-D-M"
   },
   "source": [
    "## Caching: What Is It?\n",
    "When a language model processes text, it doesn't just look at one word at a time - it builds up a complex internal state that represents its \"understanding\" of the entire context. This state consists of key-value pairs at each layer of the transformer architecture, which encode the relationships and patterns in the input text.\n",
    "\n",
    "### The Traditional Generation Process\n",
    "Without caching, here's what happens every time you ask for a completion:\n",
    "\n",
    "1. The model takes your prompt (e.g., \"The story begins with a\")\n",
    "2. Converts it to tokens\n",
    "3. Processes these tokens through all its layers, building up its internal state\n",
    "4. Uses this state to predict the next token\n",
    "5. Adds the new token to the input\n",
    "6. Repeats steps 3-5 until done.\n",
    "\n",
    "What this means is thats after concatenating each newly generated token to the running sequence, it recomputes everything from scratch with the new, longer input, for every single token generated.\n",
    "\n",
    "And, that means if you want five different endings to the same prompt, the model has to process \"The story begins with a\" through this process five separate times from scratch!\n",
    "\n",
    "### Enter Caching\n",
    "Caching is like giving the model a short-term memory. Here's how it works:\n",
    "\n",
    "First time:\n",
    "- Process the prompt normally through the steps above\n",
    "- But save the **internal state** (key-value pairs for KV caching) *after processing the prompt*\n",
    "\n",
    "Subsequent times:\n",
    "- Instead of reprocessing the prompt, load the saved state which is already prepped to generate the next token\n",
    "- Start generating from there\n",
    "\n",
    "In this notebook, we'll demonstrate both of these ways of generating output from a model, and look at some implications for how we can use KV caching and a saved internal state of the model to get better, faster, responses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJyAPeGq5380"
   },
   "source": [
    "# Installations, Imports, and Setup\n",
    "First, let's install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bnG2m4js7ADH"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-R5nL2ji7AzJ"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "# AI/ML Libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "AtMxTY2N8lqn",
    "outputId": "6fe3c544-3775-4cec-8579-66b070cb0947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "Available GPU memory: 39.56 GB\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw9FvW7k7Z5B"
   },
   "source": [
    "# Loading and Testing our Model\n",
    "\n",
    "Let's start by loading a small LLM to demonstrate these concepts, and looking at its output before we do anything. We'll use LLaMa 3.2 1B. This model is excellent for this example because:\n",
    "1. It is small enough to run on smaller GPUs\n",
    "2. It uses a relatively simple transformer architecture, making it easier to understand the core concepts\n",
    "3. Despite its small size, it produces coherent enough outputs to demonstrate the effects of caching on generation\n",
    "\n",
    "## Hugging Face Authentication\n",
    "\n",
    "LLaMa 3.2 requires authentication with Hugging Face to access the model. You'll need to:\n",
    "1. Have a Hugging Face account\n",
    "2. Accept the LLaMa 3.2 model terms of use on the Hugging Face model page\n",
    "3. Create an access token on Hugging Face (https://huggingface.co/settings/tokens)\n",
    "\n",
    "After you have your access token and have accepted the terms, the code below will help you log in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "w0798kOpp_Rj",
    "outputId": "789265e5-607c-424b-f89a-7d2e43286c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token: ··········\n",
      "Login status: Authenticated with Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "login(token=token)\n",
    "\n",
    "# Verify login\n",
    "print(\"Login status: Authenticated with Hugging Face\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "K01uqEqB7bpE",
    "outputId": "5a08883a-6b79-472a-fac8-3cdc8f0dd43a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p_0UNiCYp9E4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FQoHeCz7weP"
   },
   "source": [
    "Before we dive into caching, let's look at how the model processes text. We'll create a simple function to tokenize and process text, showing the internal states at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dYpm29LQ8UvN",
    "outputId": "2e808f1e-ef13-41ba-8fca-9a68f66ae7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The quick brown fox\n",
      "Number of tokens: 5\n",
      "\n",
      "Token IDs:\n",
      "[128000, 791, 4062, 14198, 39935]\n",
      "\n",
      "Decoded tokens:\n",
      "['<|begin_of_text|>', 'The', ' quick', ' brown', ' fox']\n"
     ]
    }
   ],
   "source": [
    "def inspect_tokens(text):\n",
    "    \"\"\"Display token information for a given text.\"\"\"\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Number of tokens: {len(tokens[0])}\")\n",
    "    print(\"\\nToken IDs:\")\n",
    "    print(tokens[0].tolist())\n",
    "    print(\"\\nDecoded tokens:\")\n",
    "    print([tokenizer.decode([t]) for t in tokens[0]])\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"The quick brown fox\"\n",
    "tokens = inspect_tokens(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXigCFhq9vgq"
   },
   "source": [
    "# Generation without Caching\n",
    "First, let's start by looking at how we'd generate an output without implementing any caching.\n",
    "\n",
    "While huggingface makes this easy for us to do all at once, we'll still write a function here to do the step-by-step generation so that we can observe exactly how this process goes.\n",
    "\n",
    "\n",
    "\n",
    "---------------\n",
    "\n",
    "\n",
    "**_One important note:_** Caching is actually implemented by default in huggingface: any normal call with model() will implement a cache automatically unless `use_cache` is specifically set to `False`.\n",
    "\n",
    "For now, to illustrate our point, we'll do this. Later, when we implement caching, we'll explore some ways to have finer control over this caching, so it's not just all happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Q9qKD3JU9mQ-"
   },
   "outputs": [],
   "source": [
    "def generate_completion(prompt, max_length=100):\n",
    "    \"\"\"Generate a completion without caching.\"\"\"\n",
    "\n",
    "    #First, convert the prompt into tokens\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    generated_tokens = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # generate the input for the original prompt\n",
    "            logits = model(input_ids, use_cache=False).logits # caching is actually done by default, so we need to explicitly turn it off!\n",
    "\n",
    "            predictions = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_token_id = torch.argmax(predictions).unsqueeze(0)\n",
    "            generated_tokens.append(next_token_id.item())\n",
    "\n",
    "            # Append the new token to the input sequence for the next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return generated_text, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uGyaXJSpQiTT",
    "outputId": "6703e78e-3bda-4a21-bdd7-ad0052a36327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: (' were happy and content. They were \\nable to live in peace and harmony with each other. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were able to \\nlive in a way that was not only peaceful, but also prosperous. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were \\nable to live in a way that was not only peaceful, but also prosperous', 4.772660255432129)\n",
      "Time taken: 4.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's try it with a simple prompt. We'll make it long enough that we can see meaningful speedup.\n",
    "prompt = '''Continue a short story about the fall of an ancient civilization. This civilization was once the greatest that the world had ever seen. It now only exists as ruins, and tourists\n",
    "who go there in the modern day do not know much of anything about it. At it's height, the civilization was massive, had a thriving economy, beautiful gardens, and a\n",
    "great culture. The civilization was so advanced that it was able to create a new language, which was spoken by all of the people in the civilization and crossed boundaries to even\n",
    "be spoken by neighboring lands. Today, this language has been lost to time. The civilization lived in the closest thing to a utopia that the modern world had ever seen.\n",
    "The rulers of the city were wise and benevolent. They were able to create a society that was peaceful and prosperous. The people of the city'''\n",
    "start_time = time.time()\n",
    "completion = generate_completion(prompt)\n",
    "time_taken = time.time() - start_time\n",
    "print(f\"Completion: {completion}\")\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEkUo4Q-SDSO"
   },
   "source": [
    "# One Step Closer\n",
    "\n",
    "In the above function, you can see that we start with tokenizing the prompt, then passing through that prompt to prep the model to generate a response.\n",
    "\n",
    "Then, we one by one generate tokens that follow that input to arrive at our final generation.\n",
    "\n",
    "One thing that we can think to easily do, which gets us a step \"closer\" to caching, might be to simply save that first pass through the prompt elsewhere, outside of the function, to make sure that our function now only picks up generating the additional tokens. This would save us the first pass through the initial prompt, at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tR3NzXT_SyqE"
   },
   "outputs": [],
   "source": [
    "def generate_skipped_input_completion(starting_logits, starting_inputs, max_length=100):\n",
    "    \"\"\"Generate a completion, giving the logits to begin with.\"\"\"\n",
    "    input_ids = starting_inputs.to(device)\n",
    "    generated_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            if i == 0:\n",
    "                logits = starting_logits # don't run the model anymore on the first iteration\n",
    "            else:\n",
    "                logits = model(input_ids, # in future passes, still run full set of input_ids\n",
    "                               use_cache=False).logits # caching is actually done by default, so we need to explicitly turn it off!\n",
    "\n",
    "            predictions = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_token_id = torch.argmax(predictions).unsqueeze(0)\n",
    "            generated_tokens.append(next_token_id.item())\n",
    "\n",
    "            # Append the new token to the input sequence for the next iteration\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "M_E-afi_TVVk",
    "outputId": "5f2e9433-bd69-400c-b953-55f3327bae1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion:  were happy and content. They were \n",
      "able to live in peace and harmony with each other. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were able to \n",
      "live in a way that was not only peaceful, but also prosperous. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were \n",
      "able to live in a way that was not only peaceful, but also prosperous\n",
      "Time to make starting point: 0.02 seconds\n",
      "Time to generate: 4.10 seconds\n",
      "Total time: 4.12 seconds\n"
     ]
    }
   ],
   "source": [
    "#Use the same prompt as before\n",
    "\n",
    "start_time = time.time()\n",
    "starting_inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "starting_logits = model(starting_inputs, use_cache=False).logits\n",
    "end_time = time.time()\n",
    "time_to_input = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "completion = generate_skipped_input_completion(starting_logits, starting_inputs)\n",
    "end_time = time.time()\n",
    "time_to_generate = end_time - start_time\n",
    "\n",
    "print(f\"Completion: {completion}\")\n",
    "print(f\"Time to make starting point: {time_to_input:.2f} seconds\")\n",
    "print(f\"Time to generate: {time_to_generate:.2f} seconds\")\n",
    "print(f\"Total time: {time_to_generate + time_to_input:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh5SZFgVTpor"
   },
   "source": [
    "But, as we can see, saving just that initial pass through the prompt doesn't save much time!\n",
    "\n",
    "This is because, in this \"version\", although we've saved the initial pass through the prompt, we still end up passing through the full prompt + new tokens for each new token generated, so we still end up processing that initial sequence over and over!\n",
    "\n",
    "This is why caching doesn't just save what the model was doing - it saves the **internal state** of the model. In caching, we avoid running that initial prompt through *ever again*, because we've saved what the actual model itself was like at that time, not just what it was prepped to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SBI-_J9fJWX"
   },
   "source": [
    "# How Caching Works In-Depth\n",
    "\n",
    "So, what does saving the internal state of the model really mean, and how to we do it in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxYwv1-kaG95"
   },
   "source": [
    "## A Quick Introduction to Transformer Architecture\n",
    "Before we understand caching, we need to understand how transformers process sequences. In a transformer like Llama, text flows through the model in several stages:\n",
    "\n",
    "1. **Tokenization**: Text → Token IDs\n",
    "2. **Token Embeddings**: Token IDs → Vectors\n",
    "3. **Multiple Transformer Layers**: Each containing:\n",
    "   - Self-attention mechanism\n",
    "   - Feed-forward neural networks\n",
    "\n",
    "### The Self-Attention Mechanism: Where Caching Happens\n",
    "\n",
    "The self-attention portion is where the caching can happen. Let's look more specifically at what happens in one of these layers.\n",
    "\n",
    "#### Step 1: Query, Key, Value Creation\n",
    "For each token in the sequence, the model creates three <font color=\"orange\">vectors</font>:\n",
    "- Query (<font color=\"orange\">$\\widehat{Q}$</font>): What the current token is looking for\n",
    "- Key (<font color=\"orange\">$\\widehat{K}$</font>): What the token offers to others\n",
    "- Value (<font color=\"orange\">$\\widehat{V}$</font>): The actual information content\n",
    "\n",
    "For example, for a simple sequence like \"The cat sat\", you would need to calculate:\n",
    "\n",
    "Token 1 (\"The\"):\n",
    "\n",
    "><font color=\"orange\">$Q_1$</font> = <font color=\"green\">$W_Q$</font> × <font color=\"orange\">$x_1$</font>\n",
    ">\n",
    "><font color=\"orange\">$K_1$</font> = <font color=\"green\">$W_K$</font> × <font color=\"orange\">$x_1$</font>\n",
    ">\n",
    "><font color=\"orange\">$V_1$</font> = <font color=\"green\">$W_V$</font> × <font color=\"orange\">$x_1$</font>\n",
    "\n",
    "Token 2 (\"cat\"):\n",
    "\n",
    "><font color=\"orange\">$Q_2$</font> = <font color=\"green\">$W_Q$</font> × <font color=\"orange\">$x_2$</font>\n",
    ">\n",
    "><font color=\"orange\">$K_2$</font> = <font color=\"green\">$W_K$</font> × <font color=\"orange\">$x_2$</font>\n",
    ">\n",
    "><font color=\"orange\">$V_2$</font> = <font color=\"green\">$W_V$</font> × <font color=\"orange\">$x_2$</font>\n",
    "\n",
    "#### Token 3 (\"sat\"):\n",
    "\n",
    "><font color=\"orange\">$Q_3$</font> = <font color=\"green\">$W_Q$</font> × <font color=\"orange\">$x_3$</font>\n",
    ">\n",
    "><font color=\"orange\">$K_3$</font> = <font color=\"green\">$W_K$</font> × <font color=\"orange\">$x_3$</font>\n",
    ">\n",
    "><font color=\"orange\">$V_3$</font> = <font color=\"green\">$W_V$</font> × <font color=\"orange\">$x_3$</font>\n",
    "\n",
    "Calculating a $Q,K,V$ vector for each word in the sequence, from <font color=\"green\">weight matricies $W_Q, W_K, W_V$</font>, on the tokenized vector $x$ for each word.\n",
    "\n",
    "### Step 2: Attention Score Computation\n",
    "\n",
    "Then, these vectors come together to form $Q, K, V$ matrices.\n",
    "\n",
    "for instance:\n",
    "\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "Q_1 \\\\\n",
    "Q_2 \\\\\n",
    "Q_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where this <font color=\"green\">$Q$</font> is a matrix of size seq_length x hidden_dim: One <font color=\"orange\">$\\widehat{Q}$</font> vector per token in the sequence, which has its length determined by the size of the matrix $W_Q$, a hard-coded dimension of the model.\n",
    "\n",
    "Because in reality, there are multiple \"heads\" in each attention layer (multiple $W_Q, W_K, W_V$'s, the dimensions are:\n",
    "\n",
    "- <font color=\"green\">$Q$</font>: [num_heads, seq_length, head_dim]\n",
    "- <font color=\"green\">$K$</font>: [num_heads, seq_length, head_dim]\n",
    "- <font color=\"green\">$V$</font>: [num_heads, seq_length, head_dim]\n",
    "\n",
    "Where:\n",
    "- head_dim = d_h / num_heads\n",
    "- seq_length grows as we generate\n",
    "\n",
    "So, in the above example, the K matrix, for instance,\n",
    "\n",
    "When the model processes a sequence, it first computes these <font color=\"green\">$Q, K, V$</font> matricies for the input sequence.\n",
    "\n",
    "Then, an attention score is calculated from these matricies as:\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "Which is what we really are wanting from an attention layer to progress through. This attention score says how much every token in the sequence should \"pay attention\" to every other in the sequence, and is used to contextualize the input in order to generate the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SU0gJJqZ5lD"
   },
   "source": [
    "## Why Caching Matters: The Computational Challenge\n",
    "\n",
    "So, attention wants to be able to know about all of the tokens in a sequence. And it's computation will depend on *what's in* the sequence, so it makes sense that we need to recalculate it.\n",
    "\n",
    "But what about what comes before? When we process \"the cat\", is computing all of $Q_1, K_1, V_1, Q_2, K_2, V_2$ and for \"the cat sat\", all of $Q_1, K_1, V_1, Q_2, K_2, V_2, Q_3, K_3, V_3$. You can see how, for long prompts, this quickly becomes a lot.\n",
    "\n",
    "Without caching, when parsing a sequence, the model must:\n",
    "1. Compute Q, K, V for the current token\n",
    "1. Recompute Q, K, V for ALL previous tokens\n",
    "2. Compute attention scores for ALL combinations\n",
    "3. Process through ALL layers again\n",
    "\n",
    "For a sequence of length $L$, this means $O(L²)$ computations for **EACH** new token!\n",
    "\n",
    "But why regenerate all of the $Q, K, V$ vectors of previous parts of the sequence? $W_Q, W_K,$ and $W_V$ are fixed weight matrices. $Q, K, V$ *matrices* are changing as more is added to the sequence, but they're just getting added to, a calculation for $Q_1, K_1, V_1$ is the same every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d8Bu7kwZ9MK"
   },
   "source": [
    "## Attention at Inference Time\n",
    "\n",
    "Before we go on, we need to clear up a nuance about how **generating** the next token (doing inference) happens, which changes how this attention is calculated *slighlty* at inference time vs when batch-processing a whole (determined) sequence like we just laid out above, which you would do during training.\n",
    "\n",
    "When it comes to generating the next new word, we need to get the attention score, which contextualizes the current word to all others that came before it. But consider how this calculation works.\n",
    "\n",
    "$Q, K,$ and $V$ are matrices. So, if:\n",
    "\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "q_{the,1} & q_{the,2} & q_{the,3} \\\\\n",
    "q_{cat,1} & q_{cat,2} & q_{cat,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K^T =\n",
    "\\begin{bmatrix}\n",
    "k_{the,1} & k_{cat,1} \\\\\n",
    "k_{the,2} & k_{cat,2} \\\\\n",
    "k_{the,3} & k_{cat,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "Q \\times K^T =\n",
    "\\begin{bmatrix}\n",
    "(q_{the,1} \\times k_{the,1} + q_{the,2} \\times k_{the,2} + q_{the,3} \\times k_{the,3}) & (q_{the,1} \\times k_{cat,1} + q_{the,2} \\times k_{cat,2} + q_{the,3} \\times k_{cat,3}) \\\\\n",
    "(q_{cat,1} \\times k_{the,1} + q_{cat,2} \\times k_{the,2} + q_{cat,3} \\times k_{the,3}) & (q_{cat,1} \\times k_{cat,1} + q_{cat,2} \\times k_{cat,2} + q_{cat,3} \\times k_{cat,3})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, recall what $Q,K,$ and $V$ are meant to represent. $Q$ is the \"query\" this \"asks\" about the token in question. $K$ the \"key\" says what information a token has to offer, and $V$ is the \"value\" that stores the actual information to give.\n",
    "\n",
    "We don't really need any to ask any questions (i.e store any \"Q\" element) for a word we've already generated - there's nothing more to \"ask\" or \"understand\" about a token in the past. During training, computing Q vectors for all tokens is important because the model needs to learn how each token influences and is influenced by every other token in the sequence. But during inference, we only care about how our new token should relate to what came before. We just need the current token's Q vector to ask 'how should I pay attention to all previous tokens?' by using it with the cached K and V values.\n",
    "\n",
    "And this is evident in the matrix - each row contains all combinations for that given Q. In the above example, the first row tells us about \"the\" and the second row tells us about \"cat\". Querying the current token \"(\"cat\") doesn't depend on the query values of the previous word \"The\". When multiplying this matrix result by V, a similar observation can be made.\n",
    "\n",
    "In practice, at generation time, what this means is that we only need K's and V's for every token that came before to properly contextualize the current Q. **Our Q matrix will actually only be made up of the Q vector for the current token.**\n",
    "\n",
    "So, when prompt caching, you'll see that we will store the K and V values to avoid re-computing them, but we don't need to store Q, since the Q of *the current token only* is actually all that's being used for the the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZLXZQLnQHen"
   },
   "source": [
    "# KV-Caching\n",
    "\n",
    "KV caching is method which stores our already computed $K$ and $V$ vectors to be re-used in future generations. We don't recompute everything, we just store what we know we'll use again, and pull it up when we need to.\n",
    "\n",
    "When using KV-Caching, we store the Key and Value matrices for each layer. For each new token:\n",
    "\n",
    "1. Only compute $Q, K, V$ for the new token\n",
    "2. Concatenate $K, V$ with cached versions (for instance, although we'll see an alternate pproach below):\n",
    "```python\n",
    "K_new = torch.cat([K_cached, K_token], dim=2)  # dim=1 is sequence dimension\n",
    "V_new = torch.cat([V_cached, V_token], dim=2)\n",
    "```\n",
    "\n",
    "#### Before Caching (Processing \"The cat sat\")\n",
    "\n",
    "1. Process \"The\"\n",
    "\n",
    "  → Compute $Q_1, K_1, V_1$\n",
    "\n",
    "2. Process \"The cat\"\n",
    "\n",
    "  → Compute $K_1, V_1, Q_2, K_2, V_2$\n",
    "\n",
    "3. Process \"The cat sat\"\n",
    "\n",
    "  → Compute $K_1, V_1, K_2, V_2, Q_3, K_3, V_3$\n",
    "\n",
    "#### With Caching\n",
    "\n",
    "1. Process \"The\"\n",
    "\n",
    "  → Compute $Q_1, K_1, V_1$\n",
    "\n",
    "  → Cache $K_1, V_1$\n",
    "\n",
    "2. Process \"cat\"\n",
    "\n",
    "  → Retrieve $K_1, V_1$ from cache\n",
    "\n",
    "  → Only compute $Q_2, K_2, V_2$\n",
    "\n",
    "  → Cache $K_1, K_2, V_1, V_2$\n",
    "\n",
    "3. Process \"sat\"\n",
    "\n",
    "  → Retrieve $K_1, K_2, V_1, V_2$ from cache\n",
    "\n",
    "  → Only compute $Q_3, K_3, V_3$\n",
    "\n",
    "  → Cache $K_1, K_2, K_3, V_1, V_2, V_3$\n",
    "\n",
    "and so on.\n",
    "\n",
    "### Memory Requirements\n",
    "\n",
    "One potential issue with KV-Caching is that it trades memory for speed, since we now need to store all of those K and V values.\n",
    "\n",
    "For a model with:\n",
    "- $\\mathit{h}$ heads\n",
    "- $d$ model dimension\n",
    "- $s$ sequence length\n",
    "\n",
    "The cache size per layer is:\n",
    "$$\n",
    "size_{layer} = 2 \\times \\mathit{h} \\times s \\times \\frac{d}{\\mathit{h}} \\times sizeof(float16)\n",
    "$$\n",
    "\n",
    "Then, for $L$ layers:\n",
    "\n",
    "$$\n",
    "size_{total} = L \\times 2 \\times \\mathit{h} \\times s \\times \\frac{d}{\\mathit{h}} \\times \\textit{sizeof(float16)}\n",
    "$$\n",
    "\n",
    "The factor of 2 comes from storing both K and V.\n",
    "\n",
    "For Llama-3.2-1B with a 1000-token sequence:\n",
    "\n",
    "- ~32 layers\n",
    "- 32 heads\n",
    "- 64 head dimension\n",
    "- float16 (2 bytes)\n",
    "\n",
    "→ Total cache size ≈ 8MB per 1000 tokens\n",
    "\n",
    "## Performance Impact\n",
    "For a sequence of length S and generation length G:\n",
    "\n",
    "#### Without Caching:\n",
    "- For each new token, we recompute $K,V$ vectors for all previous tokens\n",
    "- Need to process entire sequence each time\n",
    "- Total Computations ≈ $S \\times G \\times (S + G)$\n",
    "\n",
    "#### With Caching:\n",
    "- Initial processing of prompt: L² computations\n",
    "- For each new token: just one new set of computations\n",
    "- Total Computations ≈ $S^2 + G$\n",
    "\n",
    "The speedup becomes more dramatic as the prompt length ($S$) increases:\n",
    "\n",
    "| Prompt Length | Generation Length | Speedup Factor | Example |\n",
    "|---------------|------------------|----------------|---------|\n",
    "| 10 tokens     | 20 tokens        | ~2x            | A short sentence |\n",
    "| 100 tokens    | 20 tokens        | ~8x            | A paragraph |\n",
    "| 1000 tokens   | 20 tokens        | ~40x           | A long document |\n",
    "\n",
    "This dramatic improvement occurs because:\n",
    "1. Without caching, each new token requires reprocessing the entire history\n",
    "2. With caching, each new token only requires computing its own $K,V$ vectors\n",
    "3. The longer the prompt, the more redundant computation we avoid\n",
    "\n",
    "For real-world applications like chatbots or document processing where prompts can be thousands of tokens long, KV caching becomes essential for reasonable inference speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENROlG95jHrO"
   },
   "source": [
    "## KV Caching in Code\n",
    "\n",
    "Now, let's edit our generation function to include this KV caching.\n",
    "\n",
    "### Adding Explicit Cache Management in Transformers Library\n",
    "\n",
    "As we already stated above, caching mechanisms are already built-in and implemented by default in huggingface's transformers library. However, there are also ways to have much more control over the caching, which we'll explore in this implementation. Using explicit Cache classes like `DynamicCache` provides several advantages:\n",
    "\n",
    "#### 1. Cache Reusability\n",
    "- You can save a cache state and reuse it for multiple different generations\n",
    "- Useful for generating different endings from the same prompt\n",
    "- Helps avoid recomputing prompt processing multiple times\n",
    "\n",
    "#### 2. Cache Control\n",
    "- Choose different cache implementations (Dynamic, Static, Sliding Window)\n",
    "- Control memory usage with different cache strategies\n",
    "- Explicitly manage when caches are cleared or updated\n",
    "\n",
    "#### 3. Advanced Use Cases\n",
    "- **Sliding Window Attention**: Limit memory usage for long sequences\n",
    "- **Quantized Caching**: Reduce memory footprint with quantization\n",
    "- **Cross-Attention Caching**: Useful for encoder-decoder models\n",
    "\n",
    "#### 4. Debugging and Inspection\n",
    "- Examine cache contents directly\n",
    "- Monitor memory usage\n",
    "- Debug attention patterns\n",
    "\n",
    "You can read more about different ways to implement caching in [the huggingface Cache documentation](https://huggingface.co/docs/transformers/kv_cache#best-practices-for-generation-with-cache)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4j3I77ScdM_X"
   },
   "outputs": [],
   "source": [
    "def generate_kv_cached_completion(prompt, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate completion using explicit KV caching with DynamicCache.\n",
    "    This gives us more control over cache management compared to the model's default caching.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # Initialize DynamicCache - this allows us to:\n",
    "    # 1. Explicitly manage what's cached\n",
    "    # 2. Reuse the cache across multiple generations\n",
    "    # 3. Inspect cache contents if needed\n",
    "    past_key_values = DynamicCache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initial forward pass - process the prompt\n",
    "        # past_key_values here will store K,V pairs for the prompt\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            use_cache=True,\n",
    "            past_key_values=past_key_values,  # Pass our managed cache\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        generated_sequence = input_ids\n",
    "        generated_text = []\n",
    "\n",
    "        # Generate tokens one at a time\n",
    "        for _ in range(max_length):\n",
    "            # Get logits for next token prediction\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(torch.softmax(next_token_logits, dim=-1)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Keep track of the sequence and generated tokens\n",
    "            generated_sequence = torch.cat([generated_sequence, next_token], dim=-1)\n",
    "            generated_text.append(next_token.item())\n",
    "\n",
    "            # Forward pass for next token, using our managed cache\n",
    "            outputs = model(\n",
    "                next_token,\n",
    "                use_cache=True,\n",
    "                past_key_values=outputs.past_key_values,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_text, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ9ADU30ZNtk"
   },
   "source": [
    "Now, let's see how much this speeds up the generation of our story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "HdGsPSOZg_HV",
    "outputId": "7c8e58ce-27c3-4522-a94d-0283aa276bb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion:  the last time she had been here, ten years ago. She had been in the library, and she had been in the library, and she had been in the library. \n",
      "She had been in the library, and she had been in the library, and she had been in the library. \n",
      "She had been in the library, and she had been in the library, and she had been in the library. \n",
      "She had been in the library, and she had been in the library, and\n",
      "Time taken: 5.70 seconds\n",
      "Completion:  the last time she had been here, ten years ago. She had been in the library, and she had been in the library, and she had been in the library. \n",
      "She had been in the library, and she had been in the library, and she had been in the library. \n",
      "She had been in the library, and she had been in the library, and she had been in the library. \n",
      "She had been in the library, and she had been in the library, and\n",
      "Time taken: 1.87 seconds\n",
      "\n",
      "Speedup: 3.05x\n"
     ]
    }
   ],
   "source": [
    "# Using a long prompt:\n",
    "prompt = \"\"\"The last library on Earth wasn't a building - it was a person. Her name was Sarah Chen, and she was the final recipient of the Memory Archive Protocol,\n",
    "a desperate procedure developed in the last days before the global web collapsed.\n",
    "The process had encoded the contents of humanity's greatest digital archives directly into her neural pathways.\n",
    "Now, ten years after the collapse, she wandered the dusty remains of Silicon Valley, her mind a vast repository of everything from ancient\n",
    "philosophical texts to modern scientific papers, from classic literature to social media's last posts. Each night, she transcribed a small portion of her knowledge onto carefully preserved paper,\n",
    "racing against time and her own mortality to preserve what remained of human knowledge.\n",
    "But on this particular morning, as she wrote in her small, fortified sanctuary, Sarah realized something had changed.\n",
    "Some of the memories were starting to move on their own, rearranging themselves, evolving into something new. She was simultaneously transported into the memories\n",
    "and experiencing them in third person. She saw the words dance on the page in time with seeing what the words meant happen in front of her.\n",
    "It was all out of order. Confusing. She tried to get a handle on what was happening. She steadied herself and focused, tried to put her attention to the here and now. But it was hard to fight it.\n",
    "She thought about\"\"\"\n",
    "\n",
    "# Test non-cached version\n",
    "completion, non_cached_time = generate_completion(prompt)\n",
    "print(f\"Completion: {completion}\")\n",
    "print(f\"Time taken: {non_cached_time:.2f} seconds\")\n",
    "\n",
    "# Test cached version\n",
    "start_time = time.time()\n",
    "completion = generate_kv_cached_completion(prompt)\n",
    "end_time = time.time()\n",
    "cached_time = end_time - start_time\n",
    "\n",
    "print(f\"Completion: {completion}\")\n",
    "print(f\"Time taken: {cached_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nSpeedup: {non_cached_time/cached_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X09j86rtUSV7"
   },
   "source": [
    "So, we've reduced our time considerably!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXMNEDJkDl8m"
   },
   "source": [
    "**_One final note_**: A question you might be asking is \"Why am I getting the same response every time, and does that have to do with storing the internal state?\"\n",
    "\n",
    "But no! Even though $K,V$ caching is storing those values, those aren't where the randomness is happening. It just happens that in our next token generation, we did:\n",
    "```\n",
    "next_token = torch.argmax(torch.softmax(next_token_logits, dim=-1)).unsqueeze(0).unsqueeze(0)\n",
    "```\n",
    "So, we forced the generation to pick what the model thinks is the \"best\" next token every time, making the calculation deterministic. This is useful to get the most accurate speed comparisons, but not necessary. We *could* have changed that line to:\n",
    "```\n",
    "next_token = torch.multinomial(torch.softmax(next_token_logits / temperature, dim=-1), num_samples=1).unsqueeze(0)\n",
    "```\n",
    "Where the `temperature` controls the amount of randomness, and `torch.multinomial()` will sample the responses instead of always choosing the maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQIZ34I_-FTO"
   },
   "source": [
    "# Prompt Caching\n",
    "\n",
    "You should now understand KV caching in-depth. And it's a **great** way to speed up generation when processing a single prompt.\n",
    "\n",
    "However, prompts, and even more so segments of prompts, are extremely commonly re-used. System prompts for instance, are given as a part of a prompt every single time that a chat bot is called.\n",
    "\n",
    "And sometimes, these segments of prompts are mixed and matched. \"You are friendly and kind\" might appear right at the beggining of a prompt, or after \"You are a helpful assistant\".\n",
    "\n",
    "[This paper](https://arxiv.org/pdf/2311.04934) introduced the idea of a \"prompt cache\" - a way to store how the model would process segments of prompts for easy, fast retrieval. Since this technique builds on KV caching, it's a natural next step! But it's a lot to cover in this notebook, which is already dense.\n",
    "\n",
    "So... please see my notebook on prompt caching to explore the concept in depth!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
