<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abbie Petulante">
<meta name="dcterms.date" content="2025-01-16">
<meta name="description" content="A guide to how KV caching is implemented for LLMs, including a practical example of implementing it for LLaMa 3.2 1B.">

<title>KV Caching – Abbie’s AI Tutorials</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Abbie’s AI Tutorials</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/apetulante"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/apetulante/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">KV Caching</h1>
                  <div>
        <div class="description">
          A guide to how KV caching is implemented for LLMs, including a practical example of implementing it for LLaMa 3.2 1B.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">caching</div>
                <div class="quarto-category">LLMs</div>
                <div class="quarto-category">inferencing</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abbie Petulante </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 16, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><a target="_blank" href="https://colab.research.google.com/github/apetulante/Tutorials/blob/master/Inferencing/kv_caching.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="understanding-kv-caching" class="level1">
<h1>Understanding KV Caching</h1>
<blockquote class="blockquote">
<p>This notebook provides a hands-on exploration of KV caching in language model text generation, specifically using LLaMa 3.2 1B. We’ll examine how caching works, its benefits for inference speed, and its implications for model state management.</p>
</blockquote>
<section id="caching-what-is-it" class="level2">
<h2 class="anchored" data-anchor-id="caching-what-is-it">Caching: What Is It?</h2>
<p>When a language model processes text, it doesn’t just look at one word at a time - it builds up a complex internal state that represents its “understanding” of the entire context. This state consists of key-value pairs at each layer of the transformer architecture, which encode the relationships and patterns in the input text.</p>
<section id="the-traditional-generation-process" class="level3">
<h3 class="anchored" data-anchor-id="the-traditional-generation-process">The Traditional Generation Process</h3>
<p>Without caching, here’s what happens every time you ask for a completion:</p>
<ol type="1">
<li>The model takes your prompt (e.g., “The story begins with a”)</li>
<li>Converts it to tokens</li>
<li>Processes these tokens through all its layers, building up its internal state</li>
<li>Uses this state to predict the next token</li>
<li>Adds the new token to the input</li>
<li>Repeats steps 3-5 until done.</li>
</ol>
<p>What this means is thats after concatenating each newly generated token to the running sequence, it recomputes everything from scratch with the new, longer input, for every single token generated.</p>
<p>And, that means if you want five different endings to the same prompt, the model has to process “The story begins with a” through this process five separate times from scratch!</p>
</section>
<section id="enter-caching" class="level3">
<h3 class="anchored" data-anchor-id="enter-caching">Enter Caching</h3>
<p>Caching is like giving the model a short-term memory. Here’s how it works:</p>
<p>First time: - Process the prompt normally through the steps above - But save the <strong>internal state</strong> (key-value pairs for KV caching) <em>after processing the prompt</em></p>
<p>Subsequent times: - Instead of reprocessing the prompt, load the saved state which is already prepped to generate the next token - Start generating from there</p>
<p>In this notebook, we’ll demonstrate both of these ways of generating output from a model, and look at some implications for how we can use KV caching and a saved internal state of the model to get better, faster, responses!</p>
</section>
</section>
</section>
<section id="installations-imports-and-setup" class="level1">
<h1>Installations, Imports, and Setup</h1>
<p>First, let’s install the necessary packages.</p>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install required packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q <span class="op">--</span>upgrade transformers datasets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard libraries</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># AI/ML Libraries</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers.cache_utils <span class="im">import</span> DynamicCache</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-7" class="cell" data-outputid="6fe3c544-3775-4cec-8579-66b070cb0947" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check PyTorch version and CUDA availability</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"CUDA device: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_name(<span class="dv">0</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Available GPU memory: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_properties(<span class="dv">0</span>)<span class="sc">.</span>total_memory <span class="op">/</span> <span class="dv">1024</span><span class="op">**</span><span class="dv">3</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 2.5.1+cu121
CUDA available: True
CUDA device: NVIDIA A100-SXM4-40GB
Available GPU memory: 39.56 GB</code></pre>
</div>
</div>
</section>
<section id="loading-and-testing-our-model" class="level1">
<h1>Loading and Testing our Model</h1>
<p>Let’s start by loading a small LLM to demonstrate these concepts, and looking at its output before we do anything. We’ll use LLaMa 3.2 1B. This model is excellent for this example because: 1. It is small enough to run on smaller GPUs 2. It uses a relatively simple transformer architecture, making it easier to understand the core concepts 3. Despite its small size, it produces coherent enough outputs to demonstrate the effects of caching on generation</p>
<section id="hugging-face-authentication" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-authentication">Hugging Face Authentication</h2>
<p>LLaMa 3.2 requires authentication with Hugging Face to access the model. You’ll need to: 1. Have a Hugging Face account 2. Accept the LLaMa 3.2 model terms of use on the Hugging Face model page 3. Create an access token on Hugging Face (https://huggingface.co/settings/tokens)</p>
<p>After you have your access token and have accepted the terms, the code below will help you log in:</p>
<div id="cell-9" class="cell" data-outputid="789265e5-607c-424b-f89a-7d2e43286c5f" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> login</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> getpass</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>token <span class="op">=</span> getpass.getpass(<span class="st">"Enter your Hugging Face token: "</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>login(token<span class="op">=</span>token)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify login</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Login status: Authenticated with Hugging Face"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Enter your Hugging Face token: ··········
Login status: Authenticated with Hugging Face</code></pre>
</div>
</div>
<div id="cell-10" class="cell" data-outputid="5a08883a-6b79-472a-fac8-3cdc8f0dd43a" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"meta-llama/Llama-3.2-1B"</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>model.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 2048)
    (layers): ModuleList(
      (0-15): 16 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=512, bias=False)
          (v_proj): Linear(in_features=2048, out_features=512, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((2048,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
)</code></pre>
</div>
</div>
<p>Before we dive into caching, let’s look at how the model processes text. We’ll create a simple function to tokenize and process text, showing the internal states at each step.</p>
<div id="cell-13" class="cell" data-outputid="2e808f1e-ef13-41ba-8fca-9a68f66ae7ca" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inspect_tokens(text):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Display token information for a given text."""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer.encode(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Number of tokens: </span><span class="sc">{</span><span class="bu">len</span>(tokens[<span class="dv">0</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Token IDs:"</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tokens[<span class="dv">0</span>].tolist())</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Decoded tokens:"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>([tokenizer.decode([t]) <span class="cf">for</span> t <span class="kw">in</span> tokens[<span class="dv">0</span>]])</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>sample_text <span class="op">=</span> <span class="st">"The quick brown fox"</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> inspect_tokens(sample_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Text: The quick brown fox
Number of tokens: 5

Token IDs:
[128000, 791, 4062, 14198, 39935]

Decoded tokens:
['&lt;|begin_of_text|&gt;', 'The', ' quick', ' brown', ' fox']</code></pre>
</div>
</div>
</section>
</section>
<section id="generation-without-caching" class="level1">
<h1>Generation without Caching</h1>
<p>First, let’s start by looking at how we’d generate an output without implementing any caching.</p>
<p>While huggingface makes this easy for us to do all at once, we’ll still write a function here to do the step-by-step generation so that we can observe exactly how this process goes.</p>
<hr>
<p><strong><em>One important note:</em></strong> Caching is actually implemented by default in huggingface: any normal call with model() will implement a cache automatically unless <code>use_cache</code> is specifically set to <code>False</code>.</p>
<p>For now, to illustrate our point, we’ll do this. Later, when we implement caching, we’ll explore some ways to have finer control over this caching, so it’s not just all happening under the hood.</p>
<div id="cell-15" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_completion(prompt, max_length<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generate a completion without caching."""</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#First, convert the prompt into tokens</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>).to(device)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    generated_tokens <span class="op">=</span> []</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># generate the input for the original prompt</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(input_ids, use_cache<span class="op">=</span><span class="va">False</span>).logits <span class="co"># caching is actually done by default, so we need to explicitly turn it off!</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> torch.softmax(logits[:, <span class="op">-</span><span class="dv">1</span>, :], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>            next_token_id <span class="op">=</span> torch.argmax(predictions).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>            generated_tokens.append(next_token_id.item())</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the new token to the input sequence for the next iteration</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>            input_ids <span class="op">=</span> torch.cat([input_ids, next_token_id.unsqueeze(<span class="dv">0</span>)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> next_token_id.item() <span class="op">==</span> tokenizer.eos_token_id:</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    generated_text <span class="op">=</span> tokenizer.decode(generated_tokens, skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generated_text, end_time <span class="op">-</span> start_time</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-outputid="6703e78e-3bda-4a21-bdd7-ad0052a36327" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's try it with a simple prompt. We'll make it long enough that we can see meaningful speedup.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'''Continue a short story about the fall of an ancient civilization. This civilization was once the greatest that the world had ever seen. It now only exists as ruins, and tourists</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="st">who go there in the modern day do not know much of anything about it. At it's height, the civilization was massive, had a thriving economy, beautiful gardens, and a</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="st">great culture. The civilization was so advanced that it was able to create a new language, which was spoken by all of the people in the civilization and crossed boundaries to even</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="st">be spoken by neighboring lands. Today, this language has been lost to time. The civilization lived in the closest thing to a utopia that the modern world had ever seen.</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="st">The rulers of the city were wise and benevolent. They were able to create a society that was peaceful and prosperous. The people of the city'''</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>completion <span class="op">=</span> generate_completion(prompt)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>time_taken <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Completion: </span><span class="sc">{</span>completion<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Time taken: </span><span class="sc">{</span>time_taken<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Completion: (' were happy and content. They were \nable to live in peace and harmony with each other. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were able to \nlive in a way that was not only peaceful, but also prosperous. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were \nable to live in a way that was not only peaceful, but also prosperous', 4.772660255432129)
Time taken: 4.77 seconds</code></pre>
</div>
</div>
</section>
<section id="one-step-closer" class="level1">
<h1>One Step Closer</h1>
<p>In the above function, you can see that we start with tokenizing the prompt, then passing through that prompt to prep the model to generate a response.</p>
<p>Then, we one by one generate tokens that follow that input to arrive at our final generation.</p>
<p>One thing that we can think to easily do, which gets us a step “closer” to caching, might be to simply save that first pass through the prompt elsewhere, outside of the function, to make sure that our function now only picks up generating the additional tokens. This would save us the first pass through the initial prompt, at least.</p>
<div id="cell-18" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_skipped_input_completion(starting_logits, starting_inputs, max_length<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generate a completion, giving the logits to begin with."""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> starting_inputs.to(device)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    generated_tokens <span class="op">=</span> []</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>                logits <span class="op">=</span> starting_logits <span class="co"># don't run the model anymore on the first iteration</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>                logits <span class="op">=</span> model(input_ids, <span class="co"># in future passes, still run full set of input_ids</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>                               use_cache<span class="op">=</span><span class="va">False</span>).logits <span class="co"># caching is actually done by default, so we need to explicitly turn it off!</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> torch.softmax(logits[:, <span class="op">-</span><span class="dv">1</span>, :], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>            next_token_id <span class="op">=</span> torch.argmax(predictions).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>            generated_tokens.append(next_token_id.item())</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the new token to the input sequence for the next iteration</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>            input_ids <span class="op">=</span> torch.cat([input_ids, next_token_id.unsqueeze(<span class="dv">0</span>)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> next_token_id.item() <span class="op">==</span> tokenizer.eos_token_id:</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    generated_text <span class="op">=</span> tokenizer.decode(generated_tokens, skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generated_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-19" class="cell" data-outputid="5f2e9433-bd69-400c-b953-55f3327bae1e" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Use the same prompt as before</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>starting_inputs <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>).to(device)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>starting_logits <span class="op">=</span> model(starting_inputs, use_cache<span class="op">=</span><span class="va">False</span>).logits</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>time_to_input <span class="op">=</span> end_time <span class="op">-</span> start_time</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>completion <span class="op">=</span> generate_skipped_input_completion(starting_logits, starting_inputs)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>time_to_generate <span class="op">=</span> end_time <span class="op">-</span> start_time</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Completion: </span><span class="sc">{</span>completion<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Time to make starting point: </span><span class="sc">{</span>time_to_input<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Time to generate: </span><span class="sc">{</span>time_to_generate<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total time: </span><span class="sc">{</span>time_to_generate <span class="op">+</span> time_to_input<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Completion:  were happy and content. They were 
able to live in peace and harmony with each other. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were able to 
live in a way that was not only peaceful, but also prosperous. The people of the city were able to live in a way that was not only peaceful, but also prosperous. They were 
able to live in a way that was not only peaceful, but also prosperous
Time to make starting point: 0.02 seconds
Time to generate: 4.10 seconds
Total time: 4.12 seconds</code></pre>
</div>
</div>
<p>But, as we can see, saving just that initial pass through the prompt doesn’t save much time!</p>
<p>This is because, in this “version”, although we’ve saved the initial pass through the prompt, we still end up passing through the full prompt + new tokens for each new token generated, so we still end up processing that initial sequence over and over!</p>
<p>This is why caching doesn’t just save what the model was doing - it saves the <strong>internal state</strong> of the model. In caching, we avoid running that initial prompt through <em>ever again</em>, because we’ve saved what the actual model itself was like at that time, not just what it was prepped to output.</p>
</section>
<section id="how-caching-works-in-depth" class="level1">
<h1>How Caching Works In-Depth</h1>
<p>So, what does saving the internal state of the model really mean, and how to we do it in practice?</p>
<section id="a-quick-introduction-to-transformer-architecture" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-introduction-to-transformer-architecture">A Quick Introduction to Transformer Architecture</h2>
<p>Before we understand caching, we need to understand how transformers process sequences. In a transformer like Llama, text flows through the model in several stages:</p>
<ol type="1">
<li><strong>Tokenization</strong>: Text → Token IDs</li>
<li><strong>Token Embeddings</strong>: Token IDs → Vectors</li>
<li><strong>Multiple Transformer Layers</strong>: Each containing:
<ul>
<li>Self-attention mechanism</li>
<li>Feed-forward neural networks</li>
</ul></li>
</ol>
<section id="the-self-attention-mechanism-where-caching-happens" class="level3">
<h3 class="anchored" data-anchor-id="the-self-attention-mechanism-where-caching-happens">The Self-Attention Mechanism: Where Caching Happens</h3>
<p>The self-attention portion is where the caching can happen. Let’s look more specifically at what happens in one of these layers.</p>
<section id="step-1-query-key-value-creation" class="level4">
<h4 class="anchored" data-anchor-id="step-1-query-key-value-creation">Step 1: Query, Key, Value Creation</h4>
<p>For each token in the sequence, the model creates three <font color="orange">vectors</font>: - Query (<font color="orange"><span class="math inline">\(\widehat{Q}\)</span></font>): What the current token is looking for - Key (<font color="orange"><span class="math inline">\(\widehat{K}\)</span></font>): What the token offers to others - Value (<font color="orange"><span class="math inline">\(\widehat{V}\)</span></font>): The actual information content</p>
<p>For example, for a simple sequence like “The cat sat”, you would need to calculate:</p>
<p>Token 1 (“The”):</p>
<blockquote class="blockquote">
<p><font color="orange"><span class="math inline">\(Q_1\)</span></font> = <font color="green"><span class="math inline">\(W_Q\)</span></font> × <font color="orange"><span class="math inline">\(x_1\)</span></font></p>
<p><font color="orange"><span class="math inline">\(K_1\)</span></font> = <font color="green"><span class="math inline">\(W_K\)</span></font> × <font color="orange"><span class="math inline">\(x_1\)</span></font></p>
<p><font color="orange"><span class="math inline">\(V_1\)</span></font> = <font color="green"><span class="math inline">\(W_V\)</span></font> × <font color="orange"><span class="math inline">\(x_1\)</span></font></p>
</blockquote>
<p>Token 2 (“cat”):</p>
<blockquote class="blockquote">
<p><font color="orange"><span class="math inline">\(Q_2\)</span></font> = <font color="green"><span class="math inline">\(W_Q\)</span></font> × <font color="orange"><span class="math inline">\(x_2\)</span></font></p>
<p><font color="orange"><span class="math inline">\(K_2\)</span></font> = <font color="green"><span class="math inline">\(W_K\)</span></font> × <font color="orange"><span class="math inline">\(x_2\)</span></font></p>
<p><font color="orange"><span class="math inline">\(V_2\)</span></font> = <font color="green"><span class="math inline">\(W_V\)</span></font> × <font color="orange"><span class="math inline">\(x_2\)</span></font></p>
</blockquote>
</section>
<section id="token-3-sat" class="level4">
<h4 class="anchored" data-anchor-id="token-3-sat">Token 3 (“sat”):</h4>
<blockquote class="blockquote">
<p><font color="orange"><span class="math inline">\(Q_3\)</span></font> = <font color="green"><span class="math inline">\(W_Q\)</span></font> × <font color="orange"><span class="math inline">\(x_3\)</span></font></p>
<p><font color="orange"><span class="math inline">\(K_3\)</span></font> = <font color="green"><span class="math inline">\(W_K\)</span></font> × <font color="orange"><span class="math inline">\(x_3\)</span></font></p>
<p><font color="orange"><span class="math inline">\(V_3\)</span></font> = <font color="green"><span class="math inline">\(W_V\)</span></font> × <font color="orange"><span class="math inline">\(x_3\)</span></font></p>
</blockquote>
<p>Calculating a <span class="math inline">\(Q,K,V\)</span> vector for each word in the sequence, from <font color="green">weight matricies <span class="math inline">\(W_Q, W_K, W_V\)</span></font>, on the tokenized vector <span class="math inline">\(x\)</span> for each word.</p>
</section>
</section>
<section id="step-2-attention-score-computation" class="level3">
<h3 class="anchored" data-anchor-id="step-2-attention-score-computation">Step 2: Attention Score Computation</h3>
<p>Then, these vectors come together to form <span class="math inline">\(Q, K, V\)</span> matrices.</p>
<p>for instance:</p>
<p><span class="math display">\[
Q =
\begin{bmatrix}
Q_1 \\
Q_2 \\
Q_3
\end{bmatrix}
\]</span></p>
<p>where this <font color="green"><span class="math inline">\(Q\)</span></font> is a matrix of size seq_length x hidden_dim: One <font color="orange"><span class="math inline">\(\widehat{Q}\)</span></font> vector per token in the sequence, which has its length determined by the size of the matrix <span class="math inline">\(W_Q\)</span>, a hard-coded dimension of the model.</p>
<p>Because in reality, there are multiple “heads” in each attention layer (multiple <span class="math inline">\(W_Q, W_K, W_V\)</span>’s, the dimensions are:</p>
<ul>
<li><font color="green"><span class="math inline">\(Q\)</span></font>: [num_heads, seq_length, head_dim]</li>
<li><font color="green"><span class="math inline">\(K\)</span></font>: [num_heads, seq_length, head_dim]</li>
<li><font color="green"><span class="math inline">\(V\)</span></font>: [num_heads, seq_length, head_dim]</li>
</ul>
<p>Where: - head_dim = d_h / num_heads - seq_length grows as we generate</p>
<p>So, in the above example, the K matrix, for instance,</p>
<p>When the model processes a sequence, it first computes these <font color="green"><span class="math inline">\(Q, K, V\)</span></font> matricies for the input sequence.</p>
<p>Then, an attention score is calculated from these matricies as:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<p>Which is what we really are wanting from an attention layer to progress through. This attention score says how much every token in the sequence should “pay attention” to every other in the sequence, and is used to contextualize the input in order to generate the next token.</p>
</section>
</section>
<section id="why-caching-matters-the-computational-challenge" class="level2">
<h2 class="anchored" data-anchor-id="why-caching-matters-the-computational-challenge">Why Caching Matters: The Computational Challenge</h2>
<p>So, attention wants to be able to know about all of the tokens in a sequence. And it’s computation will depend on <em>what’s in</em> the sequence, so it makes sense that we need to recalculate it.</p>
<p>But what about what comes before? When we process “the cat”, is computing all of <span class="math inline">\(Q_1, K_1, V_1, Q_2, K_2, V_2\)</span> and for “the cat sat”, all of <span class="math inline">\(Q_1, K_1, V_1, Q_2, K_2, V_2, Q_3, K_3, V_3\)</span>. You can see how, for long prompts, this quickly becomes a lot.</p>
<p>Without caching, when parsing a sequence, the model must: 1. Compute Q, K, V for the current token 1. Recompute Q, K, V for ALL previous tokens 2. Compute attention scores for ALL combinations 3. Process through ALL layers again</p>
<p>For a sequence of length <span class="math inline">\(L\)</span>, this means <span class="math inline">\(O(L²)\)</span> computations for <strong>EACH</strong> new token!</p>
<p>But why regenerate all of the <span class="math inline">\(Q, K, V\)</span> vectors of previous parts of the sequence? <span class="math inline">\(W_Q, W_K,\)</span> and <span class="math inline">\(W_V\)</span> are fixed weight matrices. <span class="math inline">\(Q, K, V\)</span> <em>matrices</em> are changing as more is added to the sequence, but they’re just getting added to, a calculation for <span class="math inline">\(Q_1, K_1, V_1\)</span> is the same every time.</p>
</section>
<section id="attention-at-inference-time" class="level2">
<h2 class="anchored" data-anchor-id="attention-at-inference-time">Attention at Inference Time</h2>
<p>Before we go on, we need to clear up a nuance about how <strong>generating</strong> the next token (doing inference) happens, which changes how this attention is calculated <em>slighlty</em> at inference time vs when batch-processing a whole (determined) sequence like we just laid out above, which you would do during training.</p>
<p>When it comes to generating the next new word, we need to get the attention score, which contextualizes the current word to all others that came before it. But consider how this calculation works.</p>
<p><span class="math inline">\(Q, K,\)</span> and <span class="math inline">\(V\)</span> are matrices. So, if:</p>
<p><span class="math display">\[
Q =
\begin{bmatrix}
q_{the,1} &amp; q_{the,2} &amp; q_{the,3} \\
q_{cat,1} &amp; q_{cat,2} &amp; q_{cat,3}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
K^T =
\begin{bmatrix}
k_{the,1} &amp; k_{cat,1} \\
k_{the,2} &amp; k_{cat,2} \\
k_{the,3} &amp; k_{cat,3}
\end{bmatrix}
\]</span></p>
<p>Then, <span class="math display">\[
Q \times K^T =
\begin{bmatrix}
(q_{the,1} \times k_{the,1} + q_{the,2} \times k_{the,2} + q_{the,3} \times k_{the,3}) &amp; (q_{the,1} \times k_{cat,1} + q_{the,2} \times k_{cat,2} + q_{the,3} \times k_{cat,3}) \\
(q_{cat,1} \times k_{the,1} + q_{cat,2} \times k_{the,2} + q_{cat,3} \times k_{the,3}) &amp; (q_{cat,1} \times k_{cat,1} + q_{cat,2} \times k_{cat,2} + q_{cat,3} \times k_{cat,3})
\end{bmatrix}
\]</span></p>
<p>Now, recall what <span class="math inline">\(Q,K,\)</span> and <span class="math inline">\(V\)</span> are meant to represent. <span class="math inline">\(Q\)</span> is the “query” this “asks” about the token in question. <span class="math inline">\(K\)</span> the “key” says what information a token has to offer, and <span class="math inline">\(V\)</span> is the “value” that stores the actual information to give.</p>
<p>We don’t really need any to ask any questions (i.e store any “Q” element) for a word we’ve already generated - there’s nothing more to “ask” or “understand” about a token in the past. During training, computing Q vectors for all tokens is important because the model needs to learn how each token influences and is influenced by every other token in the sequence. But during inference, we only care about how our new token should relate to what came before. We just need the current token’s Q vector to ask ‘how should I pay attention to all previous tokens?’ by using it with the cached K and V values.</p>
<p>And this is evident in the matrix - each row contains all combinations for that given Q. In the above example, the first row tells us about “the” and the second row tells us about “cat”. Querying the current token “(”cat”) doesn’t depend on the query values of the previous word “The”. When multiplying this matrix result by V, a similar observation can be made.</p>
<p>In practice, at generation time, what this means is that we only need K’s and V’s for every token that came before to properly contextualize the current Q. <strong>Our Q matrix will actually only be made up of the Q vector for the current token.</strong></p>
<p>So, when prompt caching, you’ll see that we will store the K and V values to avoid re-computing them, but we don’t need to store Q, since the Q of <em>the current token only</em> is actually all that’s being used for the the next token.</p>
</section>
</section>
<section id="kv-caching" class="level1">
<h1>KV-Caching</h1>
<p>KV caching is method which stores our already computed <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span> vectors to be re-used in future generations. We don’t recompute everything, we just store what we know we’ll use again, and pull it up when we need to.</p>
<p>When using KV-Caching, we store the Key and Value matrices for each layer. For each new token:</p>
<ol type="1">
<li>Only compute <span class="math inline">\(Q, K, V\)</span> for the new token</li>
<li>Concatenate <span class="math inline">\(K, V\)</span> with cached versions (for instance, although we’ll see an alternate pproach below):</li>
</ol>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>K_new <span class="op">=</span> torch.cat([K_cached, K_token], dim<span class="op">=</span><span class="dv">2</span>)  <span class="co"># dim=1 is sequence dimension</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>V_new <span class="op">=</span> torch.cat([V_cached, V_token], dim<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="before-caching-processing-the-cat-sat" class="level4">
<h4 class="anchored" data-anchor-id="before-caching-processing-the-cat-sat">Before Caching (Processing “The cat sat”)</h4>
<ol type="1">
<li>Process “The”</li>
</ol>
<p>→ Compute <span class="math inline">\(Q_1, K_1, V_1\)</span></p>
<ol start="2" type="1">
<li>Process “The cat”</li>
</ol>
<p>→ Compute <span class="math inline">\(K_1, V_1, Q_2, K_2, V_2\)</span></p>
<ol start="3" type="1">
<li>Process “The cat sat”</li>
</ol>
<p>→ Compute <span class="math inline">\(K_1, V_1, K_2, V_2, Q_3, K_3, V_3\)</span></p>
</section>
<section id="with-caching" class="level4">
<h4 class="anchored" data-anchor-id="with-caching">With Caching</h4>
<ol type="1">
<li>Process “The”</li>
</ol>
<p>→ Compute <span class="math inline">\(Q_1, K_1, V_1\)</span></p>
<p>→ Cache <span class="math inline">\(K_1, V_1\)</span></p>
<ol start="2" type="1">
<li>Process “cat”</li>
</ol>
<p>→ Retrieve <span class="math inline">\(K_1, V_1\)</span> from cache</p>
<p>→ Only compute <span class="math inline">\(Q_2, K_2, V_2\)</span></p>
<p>→ Cache <span class="math inline">\(K_1, K_2, V_1, V_2\)</span></p>
<ol start="3" type="1">
<li>Process “sat”</li>
</ol>
<p>→ Retrieve <span class="math inline">\(K_1, K_2, V_1, V_2\)</span> from cache</p>
<p>→ Only compute <span class="math inline">\(Q_3, K_3, V_3\)</span></p>
<p>→ Cache <span class="math inline">\(K_1, K_2, K_3, V_1, V_2, V_3\)</span></p>
<p>and so on.</p>
</section>
<section id="memory-requirements" class="level3">
<h3 class="anchored" data-anchor-id="memory-requirements">Memory Requirements</h3>
<p>One potential issue with KV-Caching is that it trades memory for speed, since we now need to store all of those K and V values.</p>
<p>For a model with: - <span class="math inline">\(\mathit{h}\)</span> heads - <span class="math inline">\(d\)</span> model dimension - <span class="math inline">\(s\)</span> sequence length</p>
<p>The cache size per layer is: <span class="math display">\[
size_{layer} = 2 \times \mathit{h} \times s \times \frac{d}{\mathit{h}} \times sizeof(float16)
\]</span></p>
<p>Then, for <span class="math inline">\(L\)</span> layers:</p>
<p><span class="math display">\[
size_{total} = L \times 2 \times \mathit{h} \times s \times \frac{d}{\mathit{h}} \times \textit{sizeof(float16)}
\]</span></p>
<p>The factor of 2 comes from storing both K and V.</p>
<p>For Llama-3.2-1B with a 1000-token sequence:</p>
<ul>
<li>~32 layers</li>
<li>32 heads</li>
<li>64 head dimension</li>
<li>float16 (2 bytes)</li>
</ul>
<p>→ Total cache size ≈ 8MB per 1000 tokens</p>
</section>
<section id="performance-impact" class="level2">
<h2 class="anchored" data-anchor-id="performance-impact">Performance Impact</h2>
<p>For a sequence of length S and generation length G:</p>
<section id="without-caching" class="level4">
<h4 class="anchored" data-anchor-id="without-caching">Without Caching:</h4>
<ul>
<li>For each new token, we recompute <span class="math inline">\(K,V\)</span> vectors for all previous tokens</li>
<li>Need to process entire sequence each time</li>
<li>Total Computations ≈ <span class="math inline">\(S \times G \times (S + G)\)</span></li>
</ul>
</section>
<section id="with-caching-1" class="level4">
<h4 class="anchored" data-anchor-id="with-caching-1">With Caching:</h4>
<ul>
<li>Initial processing of prompt: L² computations</li>
<li>For each new token: just one new set of computations</li>
<li>Total Computations ≈ <span class="math inline">\(S^2 + G\)</span></li>
</ul>
<p>The speedup becomes more dramatic as the prompt length (<span class="math inline">\(S\)</span>) increases:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Prompt Length</th>
<th>Generation Length</th>
<th>Speedup Factor</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10 tokens</td>
<td>20 tokens</td>
<td>~2x</td>
<td>A short sentence</td>
</tr>
<tr class="even">
<td>100 tokens</td>
<td>20 tokens</td>
<td>~8x</td>
<td>A paragraph</td>
</tr>
<tr class="odd">
<td>1000 tokens</td>
<td>20 tokens</td>
<td>~40x</td>
<td>A long document</td>
</tr>
</tbody>
</table>
<p>This dramatic improvement occurs because: 1. Without caching, each new token requires reprocessing the entire history 2. With caching, each new token only requires computing its own <span class="math inline">\(K,V\)</span> vectors 3. The longer the prompt, the more redundant computation we avoid</p>
<p>For real-world applications like chatbots or document processing where prompts can be thousands of tokens long, KV caching becomes essential for reasonable inference speed.</p>
</section>
</section>
<section id="kv-caching-in-code" class="level2">
<h2 class="anchored" data-anchor-id="kv-caching-in-code">KV Caching in Code</h2>
<p>Now, let’s edit our generation function to include this KV caching.</p>
<section id="adding-explicit-cache-management-in-transformers-library" class="level3">
<h3 class="anchored" data-anchor-id="adding-explicit-cache-management-in-transformers-library">Adding Explicit Cache Management in Transformers Library</h3>
<p>As we already stated above, caching mechanisms are already built-in and implemented by default in huggingface’s transformers library. However, there are also ways to have much more control over the caching, which we’ll explore in this implementation. Using explicit Cache classes like <code>DynamicCache</code> provides several advantages:</p>
<section id="cache-reusability" class="level4">
<h4 class="anchored" data-anchor-id="cache-reusability">1. Cache Reusability</h4>
<ul>
<li>You can save a cache state and reuse it for multiple different generations</li>
<li>Useful for generating different endings from the same prompt</li>
<li>Helps avoid recomputing prompt processing multiple times</li>
</ul>
</section>
<section id="cache-control" class="level4">
<h4 class="anchored" data-anchor-id="cache-control">2. Cache Control</h4>
<ul>
<li>Choose different cache implementations (Dynamic, Static, Sliding Window)</li>
<li>Control memory usage with different cache strategies</li>
<li>Explicitly manage when caches are cleared or updated</li>
</ul>
</section>
<section id="advanced-use-cases" class="level4">
<h4 class="anchored" data-anchor-id="advanced-use-cases">3. Advanced Use Cases</h4>
<ul>
<li><strong>Sliding Window Attention</strong>: Limit memory usage for long sequences</li>
<li><strong>Quantized Caching</strong>: Reduce memory footprint with quantization</li>
<li><strong>Cross-Attention Caching</strong>: Useful for encoder-decoder models</li>
</ul>
</section>
<section id="debugging-and-inspection" class="level4">
<h4 class="anchored" data-anchor-id="debugging-and-inspection">4. Debugging and Inspection</h4>
<ul>
<li>Examine cache contents directly</li>
<li>Monitor memory usage</li>
<li>Debug attention patterns</li>
</ul>
<p>You can read more about different ways to implement caching in <a href="https://huggingface.co/docs/transformers/kv_cache#best-practices-for-generation-with-cache">the huggingface Cache documentation</a>.</p>
<div id="cell-27" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_kv_cached_completion(prompt, max_length<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Generate completion using explicit KV caching with DynamicCache.</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">    This gives us more control over cache management compared to the model's default caching.</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> tokenizer.encode(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>).to(device)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize DynamicCache - this allows us to:</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Explicitly manage what's cached</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Reuse the cache across multiple generations</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Inspect cache contents if needed</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    past_key_values <span class="op">=</span> DynamicCache()</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial forward pass - process the prompt</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># past_key_values here will store K,V pairs for the prompt</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            input_ids,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            use_cache<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>            past_key_values<span class="op">=</span>past_key_values,  <span class="co"># Pass our managed cache</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            return_dict<span class="op">=</span><span class="va">True</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        generated_sequence <span class="op">=</span> input_ids</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        generated_text <span class="op">=</span> []</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate tokens one at a time</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get logits for next token prediction</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>            next_token_logits <span class="op">=</span> outputs.logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.argmax(torch.softmax(next_token_logits, dim<span class="op">=-</span><span class="dv">1</span>)).unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Keep track of the sequence and generated tokens</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>            generated_sequence <span class="op">=</span> torch.cat([generated_sequence, next_token], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>            generated_text.append(next_token.item())</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass for next token, using our managed cache</span></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>                next_token,</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>                use_cache<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>                past_key_values<span class="op">=</span>outputs.past_key_values,</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>                return_dict<span class="op">=</span><span class="va">True</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> next_token.item() <span class="op">==</span> tokenizer.eos_token_id:</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer.decode(generated_text, skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s see how much this speeds up the generation of our story.</p>
<div id="cell-29" class="cell" data-outputid="7c8e58ce-27c3-4522-a94d-0283aa276bb4" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using a long prompt:</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""The last library on Earth wasn't a building - it was a person. Her name was Sarah Chen, and she was the final recipient of the Memory Archive Protocol,</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="st">a desperate procedure developed in the last days before the global web collapsed.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="st">The process had encoded the contents of humanity's greatest digital archives directly into her neural pathways.</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="st">Now, ten years after the collapse, she wandered the dusty remains of Silicon Valley, her mind a vast repository of everything from ancient</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="st">philosophical texts to modern scientific papers, from classic literature to social media's last posts. Each night, she transcribed a small portion of her knowledge onto carefully preserved paper,</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="st">racing against time and her own mortality to preserve what remained of human knowledge.</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="st">But on this particular morning, as she wrote in her small, fortified sanctuary, Sarah realized something had changed.</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="st">Some of the memories were starting to move on their own, rearranging themselves, evolving into something new. She was simultaneously transported into the memories</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="st">and experiencing them in third person. She saw the words dance on the page in time with seeing what the words meant happen in front of her.</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="st">It was all out of order. Confusing. She tried to get a handle on what was happening. She steadied herself and focused, tried to put her attention to the here and now. But it was hard to fight it.</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="st">She thought about"""</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Test non-cached version</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>completion, non_cached_time <span class="op">=</span> generate_completion(prompt)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Completion: </span><span class="sc">{</span>completion<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Time taken: </span><span class="sc">{</span>non_cached_time<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Test cached version</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>completion <span class="op">=</span> generate_kv_cached_completion(prompt)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>cached_time <span class="op">=</span> end_time <span class="op">-</span> start_time</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Completion: </span><span class="sc">{</span>completion<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Time taken: </span><span class="sc">{</span>cached_time<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Speedup: </span><span class="sc">{</span>non_cached_time<span class="op">/</span>cached_time<span class="sc">:.2f}</span><span class="ss">x"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Completion:  the last time she had been here, ten years ago. She had been in the library, and she had been in the library, and she had been in the library. 
She had been in the library, and she had been in the library, and she had been in the library. 
She had been in the library, and she had been in the library, and she had been in the library. 
She had been in the library, and she had been in the library, and
Time taken: 5.70 seconds
Completion:  the last time she had been here, ten years ago. She had been in the library, and she had been in the library, and she had been in the library. 
She had been in the library, and she had been in the library, and she had been in the library. 
She had been in the library, and she had been in the library, and she had been in the library. 
She had been in the library, and she had been in the library, and
Time taken: 1.87 seconds

Speedup: 3.05x</code></pre>
</div>
</div>
<p>So, we’ve reduced our time considerably!</p>
<p><strong><em>One final note</em></strong>: A question you might be asking is “Why am I getting the same response every time, and does that have to do with storing the internal state?”</p>
<p>But no! Even though <span class="math inline">\(K,V\)</span> caching is storing those values, those aren’t where the randomness is happening. It just happens that in our next token generation, we did:</p>
<pre><code>next_token = torch.argmax(torch.softmax(next_token_logits, dim=-1)).unsqueeze(0).unsqueeze(0)</code></pre>
<p>So, we forced the generation to pick what the model thinks is the “best” next token every time, making the calculation deterministic. This is useful to get the most accurate speed comparisons, but not necessary. We <em>could</em> have changed that line to:</p>
<pre><code>next_token = torch.multinomial(torch.softmax(next_token_logits / temperature, dim=-1), num_samples=1).unsqueeze(0)</code></pre>
<p>Where the <code>temperature</code> controls the amount of randomness, and <code>torch.multinomial()</code> will sample the responses instead of always choosing the maximum.</p>
</section>
</section>
</section>
</section>
<section id="prompt-caching" class="level1">
<h1>Prompt Caching</h1>
<p>You should now understand KV caching in-depth. And it’s a <strong>great</strong> way to speed up generation when processing a single prompt.</p>
<p>However, prompts, and even more so segments of prompts, are extremely commonly re-used. System prompts for instance, are given as a part of a prompt every single time that a chat bot is called.</p>
<p>And sometimes, these segments of prompts are mixed and matched. “You are friendly and kind” might appear right at the beggining of a prompt, or after “You are a helpful assistant”.</p>
<p><a href="https://arxiv.org/pdf/2311.04934">This paper</a> introduced the idea of a “prompt cache” - a way to store how the model would process segments of prompts for easy, fast retrieval. Since this technique builds on KV caching, it’s a natural next step! But it’s a lot to cover in this notebook, which is already dense.</p>
<p>So… please see my notebook on prompt caching to explore the concept in depth!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/apetulante\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>