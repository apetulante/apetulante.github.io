[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Abbie! I‚Äôm a Postdoctoral Research Fellow at Vanderbilt‚Äôs Data Science Institute. I got my Ph.D.¬†in Astrophysics in 2022, where my dissertation work focused on predicting the growth of dark matter structures in the universe by using various machine learning techniques. Now in my postdoc role, my research has expanded across modalities and methodologies. As such, the breadth of AI techniques I‚Äôve learned about has expanded. The Jupyter notebooks here include a selection of some of the concepts I‚Äôve explored more deeply."
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html",
    "href": "posts/Prompt-Caching/prompt_caching.html",
    "title": "Prompt Caching",
    "section": "",
    "text": "This notebook provides a detailed exploration of prompt caching using Llama 3.2, based on the paper ‚ÄúPrompt Cache: Modular Attention Reuse for Low-Latency Inference‚Äù. We‚Äôll implement prompt caching from scratch and demonstrate its benefits for inference speed."
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#key-components",
    "href": "posts/Prompt-Caching/prompt_caching.html#key-components",
    "title": "Prompt Caching",
    "section": "Key Components:",
    "text": "Key Components:\nPrompt Modules:\nPrompt modules are reusable segments of text (like system messages or documents) that are explicitly defined in a schema with their own position IDs, allowing their attention states to be cached and reused. The modules are defined using a markup language that specifies how they can be combined and parameterized, similar to how you might define reusable components in XML.\nSchemas:\nA schema defines how prompt modules can be combined. They specify what are valid positions and relationships between modules. Example: A schema might specify that a system message comes first, followed by either a code snippet or a document, then a user query. Each of these components of a schema are their own prompt modules.\nStorage:\nFor each module, the actual data that gets stored is:\n\nThe K (Key) and V (Value) attention states\nThe position IDs used when computing those states\nThe module‚Äôs boundaries and metadata\n\nMixing and Matching:\nEach module reserves a ‚Äúslot‚Äù of position IDs, and when combining modules, original position IDs are preserved within each module. This works because transformer attention is invariant to gaps in position IDs The K,V states from different modules can be concatenated as long as their relative positions are maintained."
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#the-key-idea",
    "href": "posts/Prompt-Caching/prompt_caching.html#the-key-idea",
    "title": "Prompt Caching",
    "section": "The Key Idea:",
    "text": "The Key Idea:\nWhile the underlying caching methodology that‚Äôs used by prompt caching is just storing the KV states, the concept of prompt modules provides a structured way to:\n\nDefine what pieces of prompts are reusable\nManage their position assignments\nControl how they can be combined\nHandle variations through parameters\n\nBelow is a figure from the paper demostrating how traditional generation, generation with KV cache, and generation with a prompt cache are all different."
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#hugging-face-authentication",
    "href": "posts/Prompt-Caching/prompt_caching.html#hugging-face-authentication",
    "title": "Prompt Caching",
    "section": "Hugging Face Authentication",
    "text": "Hugging Face Authentication\nLLaMa 3.2 requires authentication with Hugging Face to access the model. You‚Äôll need to: 1. Have a Hugging Face account 2. Accept the LLaMa 3.2 model terms of use on the Hugging Face model page 3. Create an access token on Hugging Face (https://huggingface.co/settings/tokens)\nAfter you have your access token and have accepted the terms, the code below will help you log in:\n\nfrom huggingface_hub import login\nimport getpass\n\ntoken = getpass.getpass(\"Enter your Hugging Face token: \")\nlogin(token=token)\n\n# Verify login\nprint(\"Login status: Authenticated with Hugging Face\")\n\nEnter your Hugging Face token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\nLogin status: Authenticated with Hugging Face\n\n\n\nmodel_name = \"meta-llama/Llama-3.2-1B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#inspecting-the-cache",
    "href": "posts/Prompt-Caching/prompt_caching.html#inspecting-the-cache",
    "title": "Prompt Caching",
    "section": "Inspecting the Cache",
    "text": "Inspecting the Cache\nWhen caching, we‚Äôre storing the outputs of the k_proj and v_proj for each layer. The cache shape would be:\nBatch dimension $ $ Number of layers (16) \\(\\times\\) 2 tensors per layer (\\(K\\) and \\(V\\))\n‚Üí For each tensor: [num_heads=8, seq_len, head_dim=64]\nWe can see this by examining the cache:\n\nexample_prompt = \"This is an example input.\"\ninput_ids = tokenizer(example_prompt, return_tensors=\"pt\").input_ids.to(device)\n\n#How the word was tokenized\nprint(\"\\nTokens:\")\nfor i, token_id in enumerate(input_ids[0]):\n    print(f\"{i}: {tokenizer.decode([token_id])}\")\nprint(\"( length:\", len(input_ids[0]), \")\\n\")\n\noutputs = model(\n    input_ids,\n    use_cache=True, # the model is smart, and will keep track of what must be cached for us\n    return_dict=True\n)\n\ncache = outputs.past_key_values # get those cached values here\n\nprint(\"Cache structure:\")\nprint(f\"Number of layers: {len(cache)}\")\nprint(f\"Items per layer: {len(cache[0])}  # K and V\")\nk_states, v_states = cache[0]  # Look at first layer\nprint(f\"K states shape: {k_states.shape}\")\nprint(f\"V states shape: {v_states.shape}\")\n\n\nTokens:\n0: &lt;|begin_of_text|&gt;\n1: This\n2:  is\n3:  an\n4:  example\n5:  input\n6: .\n( length: 7 )\n\nCache structure:\nNumber of layers: 16\nItems per layer: 2  # K and V\nK states shape: torch.Size([1, 8, 7, 64])\nV states shape: torch.Size([1, 8, 7, 64])\n\n\nBreaking down the dimensions of these output \\(K\\) and \\(V\\) shapes: torch.Size([1, 8, 7, 64])\n\nFirst dimension [1]: This is the batch size. Since we‚Äôre processing one prompt at a time, it‚Äôs 1.\nSecond dimension [8]: This represents the number of attention heads. Each head can learn to attend to different aspects of the input.\nThird dimension [7]: Sequence length. We can see that our example prompt was tokenized as 7 tokens.\nFourth dimension [64]: This is the dimension per head (head_dim). Each head processes attention in a 64-dimensional space.\n\nSo when we store \\(K\\) and \\(V\\) states, we‚Äôre storing:\nFor each layer (16 of them):\n\nFor both \\(K\\) and \\(V\\):\n\nA tensor that has:\n\nEach head‚Äôs (8) representation of each token (seq_length) as a 64-dimensional vector\n‚Üí an [8,seq_length,64] vector\n\n‚Üí 2 [8,seq_length,64] vectors\n\n‚Üí 16 \\(\\times\\) 2 [8,seq_length,64] vectors for a given prompt that‚Äôs tokenized into seq_length tokens\n\nThis is exactly what we store in a prompt module - these KV states for each layer.\n#A Basic Implementation\nLet‚Äôs move on to actually building a prompt caching implementation. We‚Äôll start by going over the core components to prompt caching and exploring how we use them in practice."
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#core-component-1-prompt-module",
    "href": "posts/Prompt-Caching/prompt_caching.html#core-component-1-prompt-module",
    "title": "Prompt Caching",
    "section": "Core Component #1: Prompt Module",
    "text": "Core Component #1: Prompt Module\nLet‚Äôs start by investigating the concept of a prompt module more in depth, and building a simple one.\nWhen we cache a prompt module, we‚Äôre storing:\n\nThe module‚Äôs text and metadata (name, position info, etc.)\nAll those KV states we just analyzed:\nFor each of the 16 layers\nBoth K and V for each layer\nEach with shape [1, 8, seq_length, 64]\n\n\nA (simple) example prompt module\nLet‚Äôs start by building the framework for a very simple prompt module. Here, we‚Äôll build a simple prompt module class that can create a cache for us for a given prompt section.\n\nclass SimplePromptModule:\n    def __init__(self, text: str):\n        self.text = text\n        self.cached_kv_states = None\n        self.input_length = None\n\n    def compute_and_cache_states(self, model, tokenizer):\n        tokens = tokenizer(\n            self.text,\n            return_tensors=\"pt\",\n            return_attention_mask=True\n        ).to(model.device)\n\n        self.input_length = tokens.input_ids.shape[1]\n\n        with torch.no_grad():\n            outputs = model(\n                tokens.input_ids,\n                attention_mask=tokens.attention_mask,\n                use_cache=True,\n                return_dict=True\n            )\n\n        self.cached_kv_states = outputs.past_key_values\n\n\n# Create and cache a simple module\nsystem_prompt = SimplePromptModule(\"You are a helpful AI assistant.\")\nsystem_prompt.compute_and_cache_states(model, tokenizer)\n\n# Let's look at what we cached\nprint(\"Number of layers:\", len(system_prompt.cached_kv_states))\nfirst_layer_k, first_layer_v = system_prompt.cached_kv_states[0]\nprint(\"K shape:\", first_layer_k.shape)\nprint(\"V shape:\", first_layer_v.shape)\n\nNumber of layers: 16\nK shape: torch.Size([1, 8, 8, 64])\nV shape: torch.Size([1, 8, 8, 64])\n\n\nNow, we can write a simple function that calls on this cache at generation time.\nThis function is largely copied from the end of my kv_caching.ipynb notebook, so I won‚Äôt go into too many details about the exact details of this function, but we‚Äôll utilize our saved KV states and feed them back into the model as we generate the tokens that follow our cached system prompt + new prompt.\n\ndef generate_with_cached_prompt(\n    model,\n    tokenizer,\n    prompt_module: SimplePromptModule,\n    continuation: str,\n    max_length: int = 50,\n    temperature = .7\n):\n    # Get tokens for the continuation\n    continuation_ids = tokenizer.encode(continuation, return_tensors='pt').to(model.device)\n\n    with torch.no_grad():\n        # First forward pass using continuation and cached states\n        outputs = model(\n            continuation_ids,\n            past_key_values=prompt_module.cached_kv_states,\n            use_cache=True,\n            return_dict=True\n        )\n\n        generated_sequence = continuation_ids\n        generated_text = []\n\n        # Generate tokens one at a time\n        for _ in range(max_length):\n            # Get logits for next token prediction\n            next_token_logits = outputs.logits[:, -1, :]\n            next_token = torch.multinomial(torch.softmax(next_token_logits / temperature, dim=-1), num_samples=1)\n\n            # Keep track of the sequence\n            generated_sequence = torch.cat([generated_sequence, next_token], dim=-1)\n            generated_text.append(next_token.item())\n\n            # Forward pass for next token\n            outputs = model(\n                next_token,\n                use_cache=True,\n                past_key_values=outputs.past_key_values,\n                return_dict=True\n            )\n\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n    return tokenizer.decode(generated_text, skip_special_tokens=True)\n\n\noutput = generate_with_cached_prompt(\n    model,\n    tokenizer,\n    system_prompt, # use the system prompt, which we've now conventiently already cached!\n    \"Tell me about neural networks.\"\n)\nprint(\"\\nOutput:\", output)\n\n\nOutput:  It's a topic that's a little confusing. Explain the basics of artificial intelligence and neural networks, and the different kinds of artificial neural networks. I'm confused about the difference between feedforward and recurrent neural networks. I want to know which type of"
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#core-component-2-prompt-schemas",
    "href": "posts/Prompt-Caching/prompt_caching.html#core-component-2-prompt-schemas",
    "title": "Prompt Caching",
    "section": "Core Component #2: Prompt Schemas",
    "text": "Core Component #2: Prompt Schemas\nSo far, we‚Äôve seen just storing the KV states for a single system prompt, and inserting those states at the beginning of generation. We‚Äôve seen what organizing it as a module could look like, but besides that, our implementation so far has largely been just standard KV caching.\nThe real idea behind prompt caching is that different prompt modules can be mixed and matched together. This is done with prompt schemas, which are the higher-level containers that defines:\n\nWhat modules exist\nTheir positions/ordering\nHow they can be combined\nParameters they can take\n\n\nA (simple) schema example\nWe‚Äôll start with a very simple schema implementation first, since it provides the structural foundation for how modules work.\nThe schema below will give us a minimal foundation that just:\n\nStores modules by name\nCan cache all modules at once\n\n\nclass SimpleSchema:\n    \"\"\"A basic schema that just stores and manages prompt modules.\"\"\"\n\n    def __init__(self, name: str):\n        self.name = name\n        self.modules = {}  # name -&gt; SimplePromptModule\n\n    def add_module(self, name: str, text: str):\n        \"\"\"Add a module to the schema.\"\"\"\n        module = SimplePromptModule(text)\n        self.modules[name] = module\n        return module\n\n    def cache_all(self, model, tokenizer):\n        \"\"\"Compute and cache states for all modules.\"\"\"\n        print(f\"Caching states for schema '{self.name}':\")\n        for name, module in self.modules.items():\n            print(f\"  Caching module '{name}'...\")\n            module.compute_and_cache_states(model, tokenizer)\n\nLet‚Äôs define this schema and add some prompts to it.\n\n# Create a schema\nschema = SimpleSchema(\"qa_schema\")\n\n# Add a few modules\nschema.add_module(\"system\", \"You are a helpful AI assistant.\")\nschema.add_module(\"python_doc\", \"Python is a programming language known for its readability.\")\n\n# Cache all modules\nschema.cache_all(model, tokenizer)\n\nCaching states for schema 'qa_schema':\n  Caching module 'system'...\n  Caching module 'python_doc'...\n\n\n\n# Try using just one module first (using our working generate_with_cached_prompt)\noutput = generate_with_cached_prompt(\n    model,\n    tokenizer,\n    schema.modules[\"system\"],\n    \"Tell me about coding.\",\n    max_length=50\n)\nprint(\"\\nGenerated:\", output)\n\n\nGenerated:  This is your second day and I‚Äôm sure you‚Äôre going to be amazing. You‚Äôre good with math, science, and you have a strong understanding of coding. You are asking the question, ‚ÄúAre you good enough to be an AI assistant?‚Äù\n\n\n\n\nModules are part of schemas\nA module becomes a part of a schema. Indeed, we can inspect the KV states that we were storing for just our module vs for our module now that it‚Äôs stored as part of a schema, and see that they are identical.\n\ndef inspect_cached_states(prompt_module: SimplePromptModule):\n    \"\"\"Print the first few values of the cached K,V states.\"\"\"\n    if prompt_module.cached_kv_states is None:\n        print(\"No cached states!\")\n        return\n\n    # Look at first layer's first few values\n    k, v = prompt_module.cached_kv_states[0]  # First layer\n    print(f\"First 5 values of K: {k[0, 0, 0, :5]}\")  # First batch, head, token, first 5 dims\n    print(f\"First 5 values of V: {v[0, 0, 0, :5]}\")\n\n# Test prompt module\nprint(\"Original version:\")\nsystem_prompt = SimplePromptModule(\"You are a helpful AI assistant.\")\nsystem_prompt.compute_and_cache_states(model, tokenizer)\ninspect_cached_states(system_prompt)\n\n# Test schema version\nprint(\"\\nSchema version:\")\nschema = SimpleSchema(\"qa_schema\")\nschema.add_module(\"system\", \"You are a helpful AI assistant.\")\nschema.cache_all(model, tokenizer)\ninspect_cached_states(schema.modules[\"system\"])\n\nOriginal version:\nFirst 5 values of K: tensor([ 0.0956,  0.1783,  0.0368, -0.2105, -0.2119], device='cuda:0')\nFirst 5 values of V: tensor([ 0.0026,  0.0136, -0.0450, -0.0029, -0.0004], device='cuda:0')\n\nSchema version:\nCaching states for schema 'qa_schema':\n  Caching module 'system'...\nFirst 5 values of K: tensor([ 0.0956,  0.1783,  0.0368, -0.2105, -0.2119], device='cuda:0')\nFirst 5 values of V: tensor([ 0.0026,  0.0136, -0.0450, -0.0029, -0.0004], device='cuda:0')"
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#position-handling",
    "href": "posts/Prompt-Caching/prompt_caching.html#position-handling",
    "title": "Prompt Caching",
    "section": "Position Handling",
    "text": "Position Handling\nWe want to combine multiple prompt modules to fully take advantage of prompt caching. In order to do that, we need to manage the relative positions of our modules. And it isn‚Äôt just about order - it‚Äôs about preserving the exact positions that were used when we cached the attention states. To do so:\n\nEach module gets assigned a range of position IDs\nThese positions determine where modules can go in relation to each other\nWhen combining modules, their relative positions must be maintained\nWe can have gaps between positions - the attention mechanism can handle discontinuous position IDs\n\nThis matters because, in transformer models like Llama:\n\nEach token‚Äôs attention calculation includes positional information\nWhen we cache K,V states, they include this positional information\nTo reuse cached states, we need to use them in positions that match how they were cached\n\nWe‚Äôll create a position class that tracks:\n\nWhere a module starts (start)\nHow long it is (length)\nWhere it ends (end)\n\nThen, each module reserves a range of positions.\n\n@dataclass\nclass Position:\n    \"\"\"Tracks the position range for a module.\"\"\"\n    start: int  # Starting position of the module\n    length: int  # Length of the module in tokens\n\n    @property\n    def end(self) -&gt; int:\n        \"\"\"End position of the module (exclusive).\"\"\"\n        return self.start + self.length\n\n    def overlaps_with(self, other: 'Position') -&gt; bool:\n        \"\"\"Check if this position overlaps with another.\"\"\"\n        return not (self.end &lt;= other.start or other.end &lt;= self.start)\n\ndef validate_module_sequence(positions: List[Position]):\n    \"\"\"\n    Validate that a sequence of module positions does not overlap.\n    Ensures that precomputed KV states are positionally consistent.\n    \"\"\"\n    for i in range(len(positions)):\n        for j in range(i + 1, len(positions)):\n            if positions[i].overlaps_with(positions[j]):\n                raise ValueError(\n                    f\"Position conflict: {positions[i]} overlaps with {positions[j]}\"\n                )\n\nWe also validate our positions, to make sure that nothing is trying to insert into the same slot:\n\nsystem_pos = Position(start=0, length=10)\ncontext_pos = Position(start=20, length=15)  # Note gap between 10-20\nuser_pos = Position(start=40, length=5)     # Note gap between 35-40\n\n# This should work:\nvalidate_module_sequence([system_pos, context_pos])\nvalidate_module_sequence([context_pos, user_pos])\n\n# This would raise an error:\ntry:\n    conflicting_pos = Position(start=5, length=10)  # Overlaps with system_pos\n    validate_module_sequence([system_pos, conflicting_pos])\nexcept ValueError as e:\n    print(f\"Caught expected error: {e}\")\n\nCaught expected error: Position conflict: Position(start=0, length=10) overlaps with Position(start=5, length=10)\n\n\nYou might be asking ‚ÄúWhy doesn‚Äôt attention care about discontinuous position IDs?‚Äù\nWhen a transformer processes tokens, each token‚Äôs position is used to calculate positional embeddings that inform how that token attends to other tokens. In Llama‚Äôs case, this uses rotary position embeddings (RoPE). The key insight is: attention calculations only care about relative positions between tokens, not absolute positions. When token A at position 5 attends to token B at position 3, what matters is their relative distance (2 positions), not their absolute positions. This means:\n\nSequence [0,1,2,3] and sequence [100,101,102,103] will produce the same attention patterns\nA gap like [0,1,2,10,11,12] doesn‚Äôt disrupt attention - tokens still know their relative positions to each other\nThe model never assumes positions are continuous - it just uses whatever positions it‚Äôs given to calculate relative distances\n\nTherefore, when we cache KV states from discontinuous positions and combine them, each token‚Äôs stored states still contain the correct relative position information they had when they were cached. The attention mechanism can use these just fine, as it only cares about preserving those relative relationships.\n\ndef combine_states_with_positions(\n    cached_states_list: List[Tuple[List[Tuple[torch.Tensor, torch.Tensor]], Position]]\n) -&gt; List[Tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Combine KV states from multiple modules, respecting their positions.\n\n    Args:\n        cached_states_list: List of (cached_states, position) pairs\n            cached_states is the KV states for one module\n            position is where those states should go\n    \"\"\"\n    # Sort by position\n    cached_states_list = sorted(cached_states_list, key=lambda x: x[1].start)\n\n    # Validate no overlaps\n    positions = [pos for _, pos in cached_states_list]\n    validate_module_sequence(positions)\n\n    # Initialize with first module's states\n    first_states, _ = cached_states_list[0]\n    combined_states = list(first_states)\n\n    # Add remaining modules' states\n    for module_states, _ in cached_states_list[1:]:\n        for layer_idx in range(len(combined_states)):\n            k_combined, v_combined = combined_states[layer_idx]\n            k_module, v_module = module_states[layer_idx]\n\n            # Concatenate along sequence dimension\n            combined_states[layer_idx] = (\n                torch.cat([k_combined, k_module], dim=2),\n                torch.cat([v_combined, v_module], dim=2)\n            )\n\n    return combined_states\n\n\n\"\"\"Demonstrate how positions affect state combination.\"\"\"\n# Create dummy states (normally these would be real cached states)\nbatch_size, n_heads = 1, 8\nhead_dim = 64\n\ndef make_dummy_states(seq_len: int) -&gt; List[Tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"Create dummy KV states for testing.\"\"\"\n    states = []\n    for _ in range(16):  # 16 layers\n        k = torch.randn(batch_size, n_heads, seq_len, head_dim)\n        v = torch.randn(batch_size, n_heads, seq_len, head_dim)\n        states.append((k, v))\n    return states\n\n# Create states for three modules\nstates_1 = make_dummy_states(seq_len=5)  # 5 tokens\npos_1 = Position(0, 5)\n\nstates_2 = make_dummy_states(seq_len=3)  # 3 tokens\npos_2 = Position(10, 3)  # Note gap between pos_1 and pos_2\n\nstates_3 = make_dummy_states(seq_len=4)  # 4 tokens\npos_3 = Position(15, 4)\n\n# Combine states\ncombined = combine_states_with_positions([\n    (states_1, pos_1),\n    (states_2, pos_2),\n    (states_3, pos_3)\n])\n\n# Check shapes\nk, v = combined[0]  # First layer\nprint(f\"Combined K shape: {k.shape}\")  # Should show total sequence length\n\nCombined K shape: torch.Size([1, 8, 12, 64])\n\n\nNote: in the final concatenated sequence, there aren‚Äôt actually any gaps. What matters is:\n\nThe KV states themselves (which have sequence lengths 5, 3, and 4)\nThe relative positions between tokens within each module (which are preserved no matter what absolute positions we assign)\n\nWhen we concatenate the KV states, we just put them next to each other in the order we want. The absolute position numbers (0, 10, 15) vs if we had done for instance (0, 5, 13) don‚Äôt affect the final sequence - they‚Äôre just a way to:\n\nExpress ordering (what comes first)\nAllow for validation (making sure things don‚Äôt overlap)\n\nSo why do we do it this way?\n\nTo keep things more flexible. Extra position IDs are reserved to accommodate parameters of different lengths. For example, if you have a module that might take a name that could be 1-3 tokens long, you reserve 3 positions even if the current parameter only uses 1.\nTo better organize schemas. Ranges of positions (like 0-99, 100-999) help to organize different types of modules, making it easier to keep similar modules in similar position ranges and allow for future additions of new modules with variable lengths.\n\nBasically, we want ‚ÄúYou are a helpful assistant‚Äù to be able to slot into the same spot as ‚ÄúYou are a kind, friendly, and sociable chat bot.‚Äù even though they have different lengths. This is the essence of the modularity of prompt caching coming into play."
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#validating-module-combinations",
    "href": "posts/Prompt-Caching/prompt_caching.html#validating-module-combinations",
    "title": "Prompt Caching",
    "section": "Validating Module Combinations",
    "text": "Validating Module Combinations\nThe position code that we just wrote includes validation that positions don‚Äôt overlap, but we don‚Äôt yet have rules about which modules can be combined beyond that. For example, in the paper they describe schemas that might specify:\n\nRequired modules (e.g., must have a system message)\nOrder dependencies (e.g., context must come before query)\nModule compatibility (e.g., don‚Äôt want both Python and SQL documentation modules together)\nOptional modules\nModule groups where only one can be used\n\nIn general, which of these combination rules is necessary may depend on the prompts being cached. Here, we‚Äôll explore how to implement some of these more commonly encountered scenarios in practice.\n\nclass ModuleType(Enum):\n    \"\"\"Different types of modules that may have different combination rules.\"\"\"\n    SYSTEM = \"system\"       # System messages, instructions\n    CONTEXT = \"context\"     # Documents, background info\n    QUERY = \"query\"        # User questions/inputs\n    FORMAT = \"format\"      # Output format instructions\n\n@dataclass\nclass CombinationRule:\n    \"\"\"Defines how a module can be combined with others.\"\"\"\n    module_name: str\n    required: bool = False                    # Must this module be included?\n    must_follow: Optional[Set[str]] = None    # Modules that must come before this one\n    cannot_combine: Optional[Set[str]] = None # Modules this can't be used with\n\n    def validate_combination(self, module_sequence: List[str]):\n        \"\"\"Check if this module's position in the sequence follows rules.\"\"\"\n        if self.required and self.module_name not in module_sequence:\n            raise ValueError(f\"Required module {self.module_name} is missing\")\n\n        if self.module_name in module_sequence:\n            current_pos = module_sequence.index(self.module_name)\n\n            # Check ordering rules\n            if self.must_follow:\n                for prerequisite in self.must_follow:\n                    if prerequisite not in module_sequence[:current_pos]:\n                        raise ValueError(\n                            f\"Module {self.module_name} must follow {prerequisite}\"\n                        )\n\n            # Check incompatible modules\n            if self.cannot_combine:\n                for incompatible in self.cannot_combine:\n                    if incompatible in module_sequence:\n                        raise ValueError(\n                            f\"Module {self.module_name} cannot be combined with {incompatible}\"\n                        )\n\n\n# Define some rules\nsystem_rule = CombinationRule(\n    \"system_message\",\n    required=True  # Must have system message\n)\n\npython_doc_rule = CombinationRule(\n    \"python_doc\",\n    must_follow={\"system_message\"},  # Must come after system message\n    cannot_combine={\"sql_doc\"}       # Can't use with SQL doc\n)\n\nquery_rule = CombinationRule(\n    \"user_query\",\n    required=True,\n    must_follow={\"system_message\"}  # Must come after system\n)\n\n# Test valid sequence\nvalid_sequence = [\"system_message\", \"python_doc\", \"user_query\"]\nfor rule in [system_rule, python_doc_rule, query_rule]:\n    rule.validate_combination(valid_sequence)\nprint(\"Valid sequence passed!\\n\")\n\n# Test invalid sequence (missing required system message)\ntry:\n    invalid_sequence = [\"python_doc\", \"user_query\"]\n    system_rule.validate_combination(invalid_sequence)\nexcept ValueError as e:\n    print(f\"Caught expected error: {e}\")\n\nValid sequence passed!\n\nCaught expected error: Required module system_message is missing"
  },
  {
    "objectID": "posts/Prompt-Caching/prompt_caching.html#fillable-parameters",
    "href": "posts/Prompt-Caching/prompt_caching.html#fillable-parameters",
    "title": "Prompt Caching",
    "section": "Fillable Parameters",
    "text": "Fillable Parameters\nOften times, most of a prompt wants to remain the same, with a few fillable parameters being swapped out.\nFor example, if you wanted to build modular prompts to help write code, you might want the specific programming lagnuage to be swappable in the system prompt (‚ÄúExplain this python code‚Äù / ‚ÄúExplain this C code‚Äù.\nIn these instances, we want to add parameters to our prompt modules, which are spaces left empty and able to accommodate an array of potential inputs in future.\n\n@dataclass\nclass Parameter:\n    \"\"\"A parameter in a module that gets filled in at runtime.\"\"\"\n    name: str\n    max_length: int  # Maximum number of tokens this parameter can use\n    default: Optional[str] = None\n\nclass ParameterizedModule:\n    \"\"\"A module that can have parameters.\"\"\"\n    def __init__(self, name: str, template: str):\n        self.name = name\n        self.template = template\n        self.parameters: Dict[str, Parameter] = {}\n\n        # Find parameters in template\n        # Format: {param_name:max_length}\n        param_pattern = r'\\{(\\w+):(\\d+)\\}'\n        for match in re.finditer(param_pattern, template):\n            param_name, max_length = match.groups()\n            self.parameters[param_name] = Parameter(\n                name=param_name,\n                max_length=int(max_length)\n            )\n\n    def fill_parameters(self, tokenizer, **kwargs) -&gt; str:\n        \"\"\"Fill in parameters and validate their lengths.\"\"\"\n        result = self.template\n\n        for param_name, value in kwargs.items():\n            if param_name not in self.parameters:\n                raise ValueError(f\"Unknown parameter: {param_name}\")\n\n            # Check length\n            param = self.parameters[param_name]\n            tokens = tokenizer(value)['input_ids']\n            if len(tokens) &gt; param.max_length:\n                raise ValueError(\n                    f\"Value for {param_name} uses {len(tokens)} tokens, \"\n                    f\"max allowed is {param.max_length}\"\n                )\n\n            # Replace in template\n            result = result.replace(f\"{{{param_name}:{param.max_length}}}\", value)\n\n        # Fill any remaining parameters with defaults\n        for name, param in self.parameters.items():\n            if name not in kwargs and param.default is not None:\n                result = result.replace(\n                    f\"{{{name}:{param.max_length}}}\",\n                    param.default\n                )\n\n        return result\n\nNow, let‚Äôs experiment with building a parametrized module.\nFeel free to experiment with this cell: see what happens if you change the number of tokens allowed for the different parameters, or what happens if you try different (like too long, for instance) parameters when filling them.\n\n# Create a parameterized module\ncode_explain = ParameterizedModule(\n    \"explain_code\",\n    template=\"Explain this {language:5} code:\\n{code:50}\\nFocus on {aspect:3}\"\n)\n\n# Fill parameters\nfilled = code_explain.fill_parameters(\n    tokenizer,\n    language=\"Python\",\n    code=\"def factorial(n): return 1 if n &lt;= 1 else n * factorial(n-1)\",\n    aspect=\"recursion\"\n)\nprint(\"Filled template:\", filled)\n\nFilled template: Explain this Python code:\ndef factorial(n): return 1 if n &lt;= 1 else n * factorial(n-1)\nFocus on recursion"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html",
    "title": "RL for LLMs",
    "section": "",
    "text": "In this notebook, we‚Äôll dive deep into how LLM‚Äôs are trained using reinforcement learning, exploring how we can implement this type of training on an existing, small model to better guide our desired responses!\n\n  \n\n\nIn a typical LLM training paradigm, models are first trained in an self-supervised manner to predict a masked-out word. This makes them great at understanding how language is constructed, and helps us to make giant datasets very easily.\nHowever, models trained to simply predict what word comes next are not very helpful to talk to. If I ask a model trained in this manner ‚ÄúWhat kind of tree is a cedar?‚Äù It may respond ‚ÄúI would really like to know about cedar trees.‚Äù because it thinks it should be continuing the thought.\nThe next step, then, in LLM training, is typically instruction fine tuning (IFT). In this step, models are trained to respond and follow instructions. In this step, they come to recognize when they are being asked a question, and the language produced thereafter becomes a response not simply a continuation. After IFT, a model might respond: ‚Äúconiferous‚Äù\nThe model now answers the question (hopefully correctly.) But it may not be friendly, or be too terse, or not structure its answers particularly well, or even be mean and harmful. So, the last step in training is typically reinforcement learning from human feedback (RLHF, or just RL). In this step, the model is told what responses of its are better and encouraged to respond in that type of way. While this can adjust the responses in many ways, one example is that it may encourage more information or detail, or generally more wordiness. So, a model trained with RL may respond ‚ÄúA cedar is a type of coniferous tree in the genus Cedrus. Cedars are evergreen trees known for their aromatic wood, needle-like leaves, and cones. They belong to the family Pinaceae.‚Äù\n\nüìå Note: A great textbook on reinforcement learning is Reinforcement Learning: An Introduction. I highly reccommend referencing it if, after or during this tutorial, you find yourself wanting an even deeper dive into some of these concepts."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#installations-and-imports",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#installations-and-imports",
    "title": "RL for LLMs",
    "section": "Installations and Imports",
    "text": "Installations and Imports\n\n# Install required packages\n!pip install -q --upgrade transformers datasets\n!pip install -q trl==0.10.1 #install downgraded version because it's easier to use!\n\n(If the below imports fail, you may need to restart the kernel for those installations to take effect).\n\n# Basic imports\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\nimport numpy as np\nimport math\nfrom datasets import Dataset\nimport warnings\nimport re\nfrom typing import Dict, List, Tuple\n\nfrom collections import defaultdict\nfrom transformers import GPT2Tokenizer\n\nimport random\n\nwarnings.filterwarnings('ignore')\n\nIf you have access to it, a GPU runtime will make this code run smoother (or might make it possible to run at all!)\nThe below code will confirm if you‚Äôre on a GPU. You want to see CUDA available: True\nIt‚Äôs not required, but preferred. A stronger CPU might be required.\n\n# Check PyTorch version and CUDA availability\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n\nPyTorch version: 2.5.1+cu124\nCUDA available: False\n\n\n\n(OPTIONAL) Hugging Face Authentication\nLater in this notebook, we‚Äôll be grabbing a model off of huggingface. While the one we use here doesn‚Äôt require a token, you can add your token here if you want to experiment with swapping GPT-2 for a different model that needs authentication.\n\nfrom huggingface_hub import login\nimport getpass\n\ntoken = getpass.getpass(\"Enter your Hugging Face token: \")\n\n# Verify login\nprint(\"Login status: Authenticated with Hugging Face\")\n\nLogin status: Authenticated with Hugging Face"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#an-introduction-from-traditional-training-to-reinforcement-learning",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#an-introduction-from-traditional-training-to-reinforcement-learning",
    "title": "RL for LLMs",
    "section": "An Introduction: From Traditional Training to Reinforcement Learning",
    "text": "An Introduction: From Traditional Training to Reinforcement Learning\nBefore we get started, let‚Äôs talk about what exactly the goals of RL are, how it differs from ‚Äútraditional‚Äù training, and get a basic understanding of the RL pipeline.\n\nTraditional Training: A Quick Review\nIn ‚Äútraditional‚Äù, supervised training of a transformer or neural network, which you‚Äôre likely familiar with, the process looks like this:\n\nYou have training data with inputs and known correct outputs\nThe model makes predictions\nYou calculate a loss function (like MSE or cross-entropy) that measures how wrong the predictions are\nBackpropagation updates the weights to minimize this loss\nRepeat until the model gets good at predicting correct outputs\n\nThe key here is that for every input, you know exactly what the correct output should be.\nBut what if you don‚Äôt know the exact right answer? What if you just know when answers are ‚Äúbetter‚Äù or ‚Äúworse‚Äù? This is where reinforcement learning comes in.\nConsider training an LLM to be helpful and truthful. There‚Äôs no single ‚Äúcorrect‚Äù response to a prompt - there might be many good responses and many bad ones. We can‚Äôt use traditional supervised learning because: - We don‚Äôt have examples of perfect responses - Multiple very different responses might be equally good - We care about abstract qualities (helpfulness, truthfulness) more than exact word matches\n\n\nEnter Reinforcement Learning\nRL approaches this differently:\n\nInstead of a loss function that measures ‚Äúwrongness‚Äù, we use a reward function that measures ‚Äúgoodness‚Äù\nInstead of comparing to correct answers, we try different outputs and see which get higher rewards\nInstead of direct supervision, the model learns through trial and error\n\nRL requires, at a high level:\n1. Generation Phase - Model receives a prompt - Model generates multiple different possible responses - This is called ‚Äúexploring‚Äù the space of possible outputs\n2. Evaluation Phase - Each generated response gets a reward score - Better responses = higher rewards - This tells us which outputs we want to encourage\n3. Learning Phase - Model is updated to make high-reward outputs more likely - But, it doesn‚Äôt memorize specific outputs - Instead, it learns patterns that tend to lead to high rewards\n4. Repeat - Generate new responses - Evaluate them - Learn from the results - Over time, the model gets better at generating high-reward outputs\n\n\nSome Key Differences from Traditional Training\n1. Exploration vs Exploitation - The model needs to try new things (explore) to find better strategies - But it also needs to use what it knows works (exploit) - This ‚Äúexploration-exploitation tradeoff‚Äù doesn‚Äôt exist in traditional training\n2. Delayed Rewards - Sometimes we don‚Äôt know if an output was good until several steps later - The model needs to learn which actions led to good outcomes - This is very different from immediate feedback in traditional training\n3. Moving Targets - As the model improves, it generates different outputs - These new outputs might get different rewards - So, the learning process is more dynamic than traditional fixed-dataset training"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#defining-the-reward-function",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#defining-the-reward-function",
    "title": "RL for LLMs",
    "section": "Defining the Reward Function",
    "text": "Defining the Reward Function\nSo, what is a reward function? In reality, it can take many forms. But in general, a reward function needs to be:\n\nClear: It should have a well-defined relationship between output quality and some numerical score\nConsistent: Similar outputs should get similar rewards\nMeaningful: Higher rewards should genuinely represent better outputs\nComputationally Feasible: As we need to calculate rewards for many outputs quickly\n\nReward functions can also incorportate negative rewards for behaviors that the model wants to explicitly avoid."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#a-very-simple-example",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#a-very-simple-example",
    "title": "RL for LLMs",
    "section": "A (Very) Simple Example",
    "text": "A (Very) Simple Example\nLet‚Äôs just start with a very basic example to illustrate what a simple reward function could be. Below, we‚Äôll write a ‚Äúpositive sentiment‚Äù reward function that counts how many positive and negative words were used in a response, giving positive rewards for positive words, and negative rewards for negative words.\n\n# Simple reward function\ndef sentiment_reward(text: str) -&gt; float:\n    positive_words = ['good', 'great', 'excellent', 'wonderful', 'amazing']\n    negative_words = ['bad', 'awful', 'terrible']\n\n    words = text.lower().split()\n    positive_count = sum(1 for word in words if word in positive_words)\n    negative_count = sum(1 for word in words if word in negative_words)\n\n    return positive_count - negative_count\n\n\n# Test our reward function with some example sentences\ntest_texts = [\n    \"This is a good and great day\",\n    \"Nothing particularily special happened today, but I was still satisfied\",\n    \"Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later\",\n    \"Good wow great amazing excellent stuff, wow great and good and great\"\n]\n\nprint(\"\\nTesting reward function:\")\nfor text in test_texts:\n    print(f\"\\nText: {text}\")\n    print(f\"Reward: {sentiment_reward(text):.2f}\")\n\n\nTesting reward function:\n\nText: This is a good and great day\nReward: 2.00\n\nText: Nothing particularily special happened today, but I was still satisfied\nReward: 0.00\n\nText: Everything was absolutely amazing and wonderful at first, then terrible and bad and awful later\nReward: -1.00\n\nText: Good wow great amazing excellent stuff, wow great and good and great\nReward: 7.00\n\n\nSo, we can see that the above, while likely far too simple to actually be useful, counts as a reward function, as it meets our criteria and provides some way to understand ‚Äúbetter‚Äù responses.\nThis reward function clearly meets the requirements of: 1. Clear relationship: More positive words = higher score 2. Fast to compute: Simple word counting is very efficient 3. Easy to understand: The logic is straightforward\nHowever, we can point out some clear reasons that this would be an unuseful reward function in practice: 1. Easy to game: Model could just repeat positive words 2. Misses context: ‚ÄúNot good‚Äù counts as positive 3. Ignores quality: Well-written neutral text scores lower than poorly written positive text (see second example vs last)\nSo, a good reward function should take more into account when deciding if a whole response is good or not.\n\nOne Step Better: Adding Context\nLet‚Äôs improve our reward function by considering context. We‚Äôll: 1. Account for negations 2. Consider word positioning 3. Add penalties for repetition\n(We‚Äôll also, just for simplicity sake, move this to be positive-detecting only, no negative rewards)\n\n# Improved reward function with context awareness\ndef positivity_reward(text: str) -&gt; float:\n    # We'll keep the same (extremely incomplete) word list\n    positive_words = ['good', 'great', 'excellent', 'wonderful', 'amazing']\n\n    words = text.lower().split()\n    score = 0.0\n\n    # Check for negations (looking at pairs of words)\n    for i in range(len(words)):\n        if words[i] in positive_words:\n            # Check if previous word is a negation\n            if i &gt; 0 and words[i-1] in {'not', 'never', \"don't\", 'no'}:\n                score -= 0.5  # Penalty for negated positive words\n            else:\n                score += 1.0\n\n    # Penalty for repetition\n    unique_words = len(set(words))\n    repetition_penalty = unique_words / max(len(words), 1)\n\n    # Calculate final score with penalties\n    final_score = (score / max(len(words), 1)) * repetition_penalty\n\n    # Clip to range [0, 1]\n    return max(min(final_score, 1.0), 0.0)\n\n# Test the improved function\ntest_texts = [\n    \"This is good and helpful.\",\n    \"This is not good at all.\",\n    \"good good good good good\",\n    \"The explanation was clear and helpful, making it incredibly beneficial.\"\n]\n\nprint(\"Testing our improved reward function:\")\nfor text in test_texts:\n    reward = positivity_reward(text)\n    print(f\"\\nText: {text}\")\n    print(f\"Reward: {reward:.3f}\")\n\nTesting our improved reward function:\n\nText: This is good and helpful.\nReward: 0.200\n\nText: This is not good at all.\nReward: 0.000\n\nText: good good good good good\nReward: 0.200\n\nText: The explanation was clear and helpful, making it incredibly beneficial.\nReward: 0.000\n\n\nSo, we‚Äôve definitely improved things! But you can see how easy pitfalls are. The last example was definitely positive in tone, but that‚Äôs not accounted for here. We could add ‚Äúclear‚Äù, ‚Äúhelpful‚Äù and ‚Äúbeneficial‚Äù to our positive words list, but then how many words would need to be added just because sometimes, in the right context, they are positive? Perhaps ‚Äúincredibly‚Äù is the thing giving this sentence a positive connotation, but then that could also be used to say something is ‚Äúincredibly‚Äù negative.\nYou can see how reward functions we use in practice require some careful considerations."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#real-world-reward-functions",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#real-world-reward-functions",
    "title": "RL for LLMs",
    "section": "Real-World Reward Functions",
    "text": "Real-World Reward Functions\nIn practice, good reward functions for LLMs often combine multiple components, as there are a lot of factors to consider when deeming a response as ‚Äúgood‚Äù:\n1. Quality Metrics measure for things like grammar and fluency, relevance of the response to the given prompt, factual accuracy, and overall coherence and structure.\n2. Task-Specific Metrics measure for things like format adherence, style matching, any domain-specific requirements, and length constraints\n3. Safety and Alignment Metrics include toxicity detection and bias measurements, and check for things like truthfulness and helpfulness.\n\nA More Practical Reward Function\nLet‚Äôs see how we can implement some of those metrics above and build a (still very basic, but more realistic) reward function that would be more useful in practice than our ‚Äúpositive words‚Äù detection.\n\ndef calculate_rewards(prompt: str, response: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate multiple reward components for a given response.\n    Returns a dictionary of different reward aspects.\n    \"\"\"\n    rewards = {}\n\n    # 1. Response Length Reward\n    # Encourage responses between 50 and 500 words\n    words = response.split()\n    word_count = len(words)\n    length_reward = min(1.0, word_count / 50.0) if word_count &lt; 50 else \\\n                   1.0 if 50 &lt;= word_count &lt;= 500 else \\\n                   max(0.0, 1.0 - (word_count - 500) / 500)\n    rewards['length'] = length_reward\n\n    # 2. Prompt Relevance Reward\n    # Check if response uses key terms from prompt\n    prompt_words = set(prompt.lower().split())\n    response_words = set(response.lower().split())\n    overlap = len(prompt_words.intersection(response_words))\n    relevance_reward = min(1.0, overlap / max(len(prompt_words), 1))\n    rewards['relevance'] = relevance_reward\n\n    # 3. Format Quality Reward\n    # Check for good formatting practices\n    format_scores = []\n    # Has paragraphs\n    format_scores.append(1.0 if response.count('\\n\\n') &gt; 0 else 0.0)\n    # Uses punctuation properly\n    format_scores.append(1.0 if re.search(r'[.!?]\\s+[A-Z]', response) else 0.0)\n    # Proper capitalization\n    format_scores.append(1.0 if re.search(r'^[A-Z]', response) else 0.0)\n    rewards['formatting'] = sum(format_scores) / len(format_scores)\n\n    # 4. Calculate Final Combined Reward\n    weights = {'length': 0.2, 'relevance': 0.5, 'formatting': 0.3}\n    final_reward = sum(rewards[k] * weights[k] for k in weights)\n    rewards['final'] = final_reward\n\n    return rewards\n\n# Let's test our practical reward function\ntest_cases = [\n    {\n        \"prompt\": \"Explain how photosynthesis works.\",\n        \"response\": \"\"\"\nPhotosynthesis is the process by which plants convert sunlight into energy.\n\nThe process involves chlorophyll in the leaves capturing sunlight. This energy\nis used to convert water and carbon dioxide into glucose and oxygen.\n\nPlants use glucose for energy and release oxygen as a byproduct.\n\"\"\"\n    },\n    {\n        \"prompt\": \"Explain how photosynthesis works.\",\n        \"response\": \"plants make food from sun\"\n    }\n]\n\nfor case in test_cases:\n    rewards = calculate_rewards(case[\"prompt\"], case[\"response\"])\n    print(f\"\\nPrompt: {case['prompt']}\")\n    print(f\"Response: {case['response']}\")\n    print(\"\\nRewards:\")\n    for aspect, score in rewards.items():\n        print(f\"{aspect}: {score:.3f}\")\n\n\nPrompt: Explain how photosynthesis works.\nResponse: \nPhotosynthesis is the process by which plants convert sunlight into energy.\n\nThe process involves chlorophyll in the leaves capturing sunlight. This energy\nis used to convert water and carbon dioxide into glucose and oxygen.\n\nPlants use glucose for energy and release oxygen as a byproduct.\n\n\nRewards:\nlength: 0.900\nrelevance: 0.250\nformatting: 0.667\nfinal: 0.505\n\nPrompt: Explain how photosynthesis works.\nResponse: plants make food from sun\n\nRewards:\nlength: 0.100\nrelevance: 0.000\nformatting: 0.000\nfinal: 0.020"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#key-takeaways-about-reward-functions",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#key-takeaways-about-reward-functions",
    "title": "RL for LLMs",
    "section": "Key Takeaways About Reward Functions",
    "text": "Key Takeaways About Reward Functions\nSo, when making practical reward functions, there are multiple ways to make a reward function better, including (but not necessarily limited to):\n\nadding multiple reward components help avoid gaming the system\nusing weighted combinations allow prioritizing different target metrics\ndefining clear relationships between quality and reward score\n\nSome very common pitfalls to keep in mind: - Reward Hacking: some rewards are easily maximized by doing something very simple, not actually increasing quality - Unclear Signaling: it can be difficult to make rewards that very clearly, concretely determine ‚Äúgoodness‚Äù - Computational Overhead: Complex rewards can obviously be helpful in targeting rich, high-quality results, BUT can slow down training significantly - Inconsistent Scaling: Be sure to normalize and weight any scores that are combined appropriately."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#key-components-of-ppo",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#key-components-of-ppo",
    "title": "RL for LLMs",
    "section": "Key Components of PPO",
    "text": "Key Components of PPO\n\nThe Policy (œÄ): In our case, this is our language model. It has two key behaviors:\n\n\nActing: Generating text given a prompt\nCalculating Probabilities: Telling us how likely it would be to generate specific text\n\n\nThe Clipped Objective: This is PPO‚Äôs real metric for improvement. It includes:\n\n\nProbability Ratio (r): How much more/less likely is the new policy to generate certain text compared to the old policy?\nClipping: We put boundaries on this ratio (typically between 0.8 and 1.2)\nAdvantage (A): How much better/worse was the outcome than expected?\n\nFor LLM‚Äôs, PPO is implemented on a token-by-token basis, so each token gets the opportunity to be better or worse for the outcome. In this way, PPO is not exactly a loss function but uses a surrogate objective (a type of loss function) to guide updates. It incorporates the reward by comparing the new policy‚Äôs performance (via probabilities of actions, i.e., token predictions) to the old policy‚Äôs performance, scaled by the reward."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#the-ppo-mathematically",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#the-ppo-mathematically",
    "title": "RL for LLMs",
    "section": "The PPO Mathematically",
    "text": "The PPO Mathematically\nOf course, how the PPO is implemented in practice is through that clipped objective function.\nThe PPO objective function \\((L^{CLIP})\\) is defined as: \\[\nL^{CLIP}(\\theta) = E[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]\n\\]\nWhere:\n\n\\(\\theta\\) represents the policy parameters (our LLM weights)\n\\(r_t(\\theta)\\) is the probability ratio, \\(\\pi_{\\theta}(a_t|s_t)\\) / \\(\\pi_{\\theta_{old}}(a_t|s_t)\\)\n\\(A_t\\) is the advantage\n\\(\\epsilon\\) is the clipping parameter (typically 0.2)\n\n\n\nFor an LLM, the policy \\(\\pi_{\\theta}(a_t|s_t)\\) represents the probability of choosing token \\(a_t\\) given context \\(s_t\\):\n\n\n\\(s_t\\) is the current context (prompt + generated tokens so far)\n\\(a_t\\) is the next token to generate\n\n\nThe probability ratio, \\(r_t(\\theta) = \\pi_{\\theta}(a_t|s_t)\\) / \\(\\pi_{\\theta_{old}}(a_t|s_t)\\) is the ratio in probability of the new policy (\\(\\theta\\)) choosing token \\(a_t\\) over the probability from the old policy (\\(\\theta_{old}\\)) choosing token \\(a_t\\). (i.e \\(r_t(\\theta)\\) = 2 means it‚Äôs twice as likely to choose \\(a_t\\) in the new policy)\nThe clipping function, \\(clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\) ensures this probability ratio is reasonably bounded, keeping updates small.\nThen for each token, we calculate both the unclipped objective: \\(r_t(Œ∏)A_t\\) and the clipped objective \\(clip(r_t(Œ∏), 1-Œµ, 1+Œµ)A_t\\), and take the minimum of both, again ensuring smaller, more stable updates.\nWhere (\\(A(t)\\)) is the ‚Äúadvantage‚Äù (\\(A(t)\\)) is a measurement of how much better or worse an action was compared to what we expected, it incorporates, but isn‚Äôt exactly the reward‚Ä¶\n\n\n‚Ä¶The ‚ÄúAdvantage‚Äù \\((A_t)\\)\nOk, so we spent all of that time talking about the reward, just for it to end up wrapped in something called the ‚Äúadvantage‚Äù, so let‚Äôs break down the advantage a bit more.\nAdvantage \\((A_t)\\) measures how much better or worse an action was compared to what we expected. It‚Äôs calculated as: \\[A_t = R_t - V(s_t)\\]\nwhere: - \\(R_t\\) is the actual reward received - \\(V(s_t)\\) is the expected value (what we thought we‚Äôd get)\nIn the simplest example, imagine you‚Äôre a language model deciding what word to generate:\n\nYou generate ‚Äúexcellent‚Äù with an expected reward of 0.5. Then, for an actual reward of 0.8: \\[Advantage = 0.8 - 0.5 = +0.3\\] This was better than expected.\nYou generate ‚Äúokay‚Äù with an expected reward of 0.5. Then, for an actual reward of 0.3: \\[Advantage = 0.3 - 0.5 = -0.2\\] Negative advantage means this was worse than expected.\n\nAdvantage is used over raw reward because it allows harder prompts to expect harder results. For example, easy prompts could be very easy to get a reward of 0.8, but hard prompts hardly ever get even 0.4. Hard vs easy prompts having their own expected reward can accommodate this difference in their ‚Äútypical‚Äù reward.\nOk, so reward for a given word, we‚Äôve covered. But expected reward, what we ‚Äúthought we‚Äôd get‚Äù‚Ä¶ you might have guessed that that‚Äôs a bit more complicated.\n\n\nThe ‚ÄúExpected Value‚Äù (\\(V(s_t)\\))\nThe expected value is what you think your reward should be. This is naturally difficult to define, so you can really get this in a lot of ways. Some very simple ways could be: - defining different expected values for different categories of prompts. i.e:\nexpected_values = {\n    \"math\": 0.6,        # Math problems are harder\n    \"greeting\": 0.9,    # Greetings are easy\n    \"general\": 0.7      # General conversation\n}\nexpected_value = expected_values[get_prompt_type(prompt)]\n\nkeeping track of rewards you‚Äôve already given different types of prompts:\n\nrewards_history = {\n    \"math_questions\": [0.6, 0.7, 0.5, 0.8],  # Previous rewards for math\n    \"greeting\": [0.9, 0.95, 0.85, 0.9]       # Previous rewards for greetings\n}\nexpected_value = average(rewards_history[prompt_type])\nBut in practice, calculating the expected reward tends to be more sophisticated. It can be done with:\n\nValue Networks: these are neural networks who are trained to predict rewards. It can handle more nuanced contexts about the prompts, and more easily adapt to things like prompt lengths, topics covered, and depth of explanation asked for.\nTemporal Differences: A way to consider future rewards, not just immediate ones. Instead of just looking at the current reward (R), we also consider what rewards we expect in the future (\\(\\lambda\\) * \\(V_{next}\\), where \\(\\lambda\\) is a discount factor). This helps when your early actions lead to better outcomes later.\nGeneralized Advantage Estimation (GAE): A method that balances between immediate and future rewards when calculating advantage. It uses a weighted average of rewards over different time spans, helping to reduce variance in our advantage estimates while still maintaining useful learning signals.\n\nIn this notebook, we‚Äôll stick to some of these very simple expected value definitions. But it‚Äôs important to know that this is a choice to be made in any RL implementation! And, that the value function used is also something that can be updated. So, during training, a value function can learn to generate better expected values: we can use loss functions to update both \\(\\pi_\\theta\\) and (\\(V(s_t)\\)). We‚Äôll discuss this in more detail below!\n\n\n\nThe Expectation Operation\nI promise there will soon be code. But there‚Äôs one more thing to clear up. The PPO objective function \\((L^{CLIP})\\): \\[\nL^{CLIP}(\\theta) = E[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]\n\\]\nhas that ‚Äú\\(E\\)‚Äù - what is that?\nThis is the expectation operation. That‚Äôs mathematically simple enough - it‚Äôs really just a weighted average: the mean of the possible values that a variable can take. That‚Äôs all!\nBut what are ‚Äúall possible values‚Äù? For LLM‚Äôs, this is a sample of all possible experiences (trajectories) collected from the policy. So really, we average over a sample of all token paths that you could have generated in response to all prompt examples. Why a sample? For a bit more detailed explanation, see Appendix 1.\nWe‚Äôll touch on this again in a bit, but for now, let‚Äôs jump into an example."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#a-simple-ppo-example",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#a-simple-ppo-example",
    "title": "RL for LLMs",
    "section": "A Simple PPO Example",
    "text": "A Simple PPO Example\nLet‚Äôs see an actual implementation of the PPO calculation, to observe how, in practice, this is calculated over trajectories that an LLM could generate.\n\nFor a Single Token\nBelow, we‚Äôll see how the PPO function would calculate its objective for a single new potential token.\nConsider in this example that we have generated so far:\n\n\n‚ÄúI am feeling great and happy today, it‚Äôs‚Äù\n\n\nAnd we are tasked with determining if the new word that we could generate:\n\n\n‚Äúexcellent‚Äù\n\n\nis advantageous or not.\nThe current policy (model) generates ‚Äúexcellent‚Äù with probability 0.3, and the new proposed policy we‚Äôre evaluating generates ‚Äúexcellent‚Äù with probability 0.6.\nBelow, we‚Äôll calculate the PPO Objective \\((L^{CLIP})\\) that would result from the production of that token. We‚Äôll use our super simple sentiment_reward function that we defined above to determine our reward.\n\ndef ppo_update(old_prob, new_prob, advantage, epsilon=0.1):\n    # Calculate the probability ratio\n    prob_ratio = new_prob / old_prob\n\n    # Compute the unclipped and clipped objectives\n    unclipped_objective = prob_ratio * advantage\n    clipped_objective = np.clip(prob_ratio, 1 - epsilon, 1 + epsilon) * advantage\n\n    # PPO's objective is the minimum of the unclipped and clipped objectives\n    ppo_objective = min(unclipped_objective, clipped_objective)\n\n    return ppo_objective\n\n# new word to generate that we are testing the update for\nnew_word = \"excellent\"\n\n# Let's say, the old policy generated this word with 30% chance, and the new one generates it with 60% chance\nold_prob = 0.3\nnew_prob = 0.6\n\n# Simulated response from the agent so far\nresponse = \"I am feeling great and happy today, it's\"\n\n# Calculate the reward based on our very simple positive word reward from before\n# this calculates over the full current response with this new next token\nreward = sentiment_reward(response + new_word)\n\n# Let's say in this case, the expected behavior is that the new word is neutral,\n# so we expect it to not *add* any reward over what we had\nexpected_reward = sentiment_reward(response)\n\n# calculate the advantage that this new word has given us\nadvantage = reward - expected_reward\n\n# Perform PPO update\nppo_objective = ppo_update(old_prob, new_prob, advantage)\n\nprint(f\"Response: {response}\")\nprint(f\"Positivity Reward: {reward}\")\nprint(f\"Advantage: {advantage}\")\nprint(f\"PPO Objective: {ppo_objective}\")\n\nResponse: I am feeling great and happy today, it's\nPositivity Reward: 1\nAdvantage: 0\nPPO Objective: 0.0\n\n\nWe can see here, that adding excellent resulted in positive advantage: it was good to our reward function to add that word.\nWe can also see when we step through the math, that our objective got clipped: Our probability ratio was 2.0, so: - unclipped objective was \\(2.0 * 1.0 = 2.0\\). - clipped objective (what‚Äôs used) was \\(1.1 * 1.0 = 1.1\\)\nThis is a key action of PPO: ensuring that policy updates remain relatively small.\n\nüìå Note: you may be able to see here, that if our advantage term had been large, even with clipping, the update can be substantial, potentially leading to instability. To mitigate this, it‚Äôs common practice to normalize or scale rewards, thereby controlling the magnitude of the advantage. This normalization helps maintain stable and consistent updates.\n\n\n\nSimple PPO Example Over a Trajectory\nIn practice, any given ‚Äúresponse‚Äù which is a series of token selections is a trajectory that the model could have generated.\nFor each word in:\n\n\n‚ÄúI am feeling great and happy today, it‚Äôs‚Äù\n\n\nthere is an associated old probability, new probability, and reward for generating that token.\nWe can calculate \\(L_{CLIP(\\text{trajectory})}\\) over a given trajectory as: \\[\nL_{CLIP(\\text{trajectory})} =  \\sum_t^T L_{CLIP(t)}\n\\]\nfor a sequence of length T, comprised of tokens t\n\n# for a given trajectory of tokens, each generated with some probability\nresponse_tokens = [\"I\", \"am\", \"feeling\", \"great\", \"and\", \"happy\", \"today,\", \"it's\", \"excellent\"]\n\nold_probs = [.9, .95, .4, .25, .33, .45, .4, .15, .3] # dummy probabilities for each word in the sequence, in old policy\nnew_probs = [.9, .95, .6, .55, .5, .65, .3, .2, .6] # dummy probabilities for each word in the sequence, in new policy\n\n# Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)\nexpected_reward = 0\n\n# Compute PPO objectives for each token\nppo_objectives = []\nfor i, token in enumerate(response_tokens):\n    reward = sentiment_reward(token)\n    advantage = reward - expected_reward  # Advantage function\n\n    ppo_obj = ppo_update(old_probs[i], new_probs[i], advantage)\n    ppo_objectives.append(ppo_obj)\n\n    print(f\"Token: {token}\")\n    print(f\"Reward: {reward:.2f}, Advantage: {advantage:.2f}, PPO Objective: {ppo_obj:.2f}\\n\")\n\n# Trajectory PPO objective (sum over tokens in this trajectory)\ntraj_ppo_objective = sum(ppo_objectives)\nprint(f\"Total PPO Objective for this trajectory: {traj_ppo_objective:.2f}\")\n\nToken: I\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: am\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: feeling\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: great\nReward: 1.00, Advantage: 1.00, PPO Objective: 1.10\n\nToken: and\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: happy\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: today,\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: it's\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: excellent\nReward: 1.00, Advantage: 1.00, PPO Objective: 1.10\n\nTotal PPO Objective for this trajectory: 2.20\n\n\n\n\nSimple PPO Example Over Multiple Trajectories\nSo, the last step here is to consider the multiple trajectories that a model could have reasonably taken during genration, of which we sample some number.\nWe can calculate our final \\(L_{CLIP(\\text{total})}\\) as: \\[\nL_{CLIP(\\text{total})} =  \\frac{1}{N} * \\sum_i^N L_{CLIP(i)}\n\\]\nover all N trajectories we sampled.\nSo, let‚Äôs consider in this case, we sampled a second trajectory which we want to consider in our final \\(L_{CLIP(\\text{total})}\\) calculation.\n\n# for a different trajectory of tokens, each generated with some probability\nresponse_2_tokens = [\"I\", \"am\", \"angry\", \"it's\", \"awful\"]\n\nold_probs = [.9, .95, .3, .25, .2] # dummy probabilities for each word in the sequence, in old policy\nnew_probs = [.9, .95, .66, .55, .75] # dummy probabilities for each word in the sequence, in new policy\n\n# Dummy expected reward (here's we'll just assign some baseline is neutral reward for simplicity)\nexpected_reward = 0\n\n# Compute PPO objectives for each token\nppo_objectives = []\nfor i, token in enumerate(response_2_tokens):\n    reward = sentiment_reward(token)\n    advantage = reward - expected_reward  # Advantage function\n\n    ppo_obj = ppo_update(old_probs[i], new_probs[i], advantage)\n    ppo_objectives.append(ppo_obj)\n\n    print(f\"Token: {token}\")\n    print(f\"Reward: {reward:.2f}, Advantage: {advantage:.2f}, PPO Objective: {ppo_obj:.2f}\\n\")\n\n# Trajectory PPO objective (sum over tokens in this trajectory)\ntraj_ppo_objective2 = sum(ppo_objectives)\nprint(f\"Total PPO Objective for this trajectory: {traj_ppo_objective2:.2f}\\n\")\n\n# Final PPO objective (average over both trajectories)\ntotal_ppo_objective = np.mean([traj_ppo_objective, traj_ppo_objective2])\nprint('\\x1b[0;33;35m' + f\"Total PPO Objective for the full response: {total_ppo_objective:.2f}\" + '\\x1b[0m')\n\nToken: I\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: am\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: angry\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: it's\nReward: 0.00, Advantage: 0.00, PPO Objective: 0.00\n\nToken: awful\nReward: -1.00, Advantage: -1.00, PPO Objective: -3.75\n\nTotal PPO Objective for this trajectory: -3.75\n\nTotal PPO Objective for the full response: -0.77\n\n\n\nSo, our total PPO objective is calculated over all potential trajectories that we sampled. In this case, that was just 2. And we can see that, even though our first trajectory seemed to look good and aligned with the behavior that we wanted, we also had a trajectory that was bad. This is why it‚Äôs important to take a sample of possible trajectories that the model could produce!\n\nüìå Note: You might also notice here that our second response got a strongly negative PPO objective. Negative rewards, leading to negative advantages, mean that the clipping won‚Äôt do anything. Since we always take the minimum of the clipped and unclipped objective, advantage * unclipped probability will always be &lt;= advantage * clipped probability. See the note in Appendix 2 about this for more details about how negative rewards are used in practice! For now, we‚Äôll roll with it.)"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#kl-divergence-in-rlhf",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#kl-divergence-in-rlhf",
    "title": "RL for LLMs",
    "section": "KL Divergence in RLHF",
    "text": "KL Divergence in RLHF\nOne more concept to touch on is something called the Kullback-Leibler (KL) Divergence. When fine-tuning LLMs with reinforcement learning, we want to improve the model‚Äôs behavior while preventing it from deviating too drastically from its original training.\nKL divergence measures how much one probability distribution differs from another. In the context of LLMs, it quantifies the difference between the token probability distributions of two models - typically our current policy model and a reference model.\nMathematically, for two probability distributions \\(P\\) and \\(Q\\), KL divergence is defined as:\n\\[\nD_{KL}(P || Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n\\]\nFor language models, this becomes: \\[\nD_{KL}(\\pi_{\\text{new}} || \\pi_{\\text{ref}}) = \\sum_{t} \\pi_{\\text{new}}(t|c) \\log\\frac{\\pi_{\\text{new}}(t|c)}{\\pi_{\\text{ref}}(t|c)}\n\\] Where: - \\(\\pi_{\\text{new}}\\) is our current policy model - \\(\\pi_{\\text{ref}}\\) is the reference model (usually the initial model before RL training) - \\(t\\) represents tokens, generated given - \\(c\\) the context/prompt\nFrequently, KL divergence is included as an explicit penalty in our reward function, preventing the model from:\n\nForgetting its pre-trained knowledge\nAdopting degenerate patterns to maximize reward\nStraying too far from human-like text generation\n\nIn practice, we typically add a KL penalty term to our reward: \\[\nr_{\\text{total}} = r_{\\text{original}} - \\beta \\cdot D_{KL}(\\pi_{\\text{new}} || \\pi_{\\text{ref}})\n\\]\nWhere \\(\\beta\\) controls the strength of the penalty. This creates a balance between optimizing for rewards and maintaining the model‚Äôs original behavior.\n ## üìå One Final Note on PPO\nYou should now understand how the PPO is calculated and implemented fairly well. And while PPO is a very popular algorithm for reinforcement learning, and what we‚Äôll go on to use in this notebook, you should know that it‚Äôs far from the only one. There are many other choices of update strategy that can be used, and while it‚Äôs out of the scope of this notebook to go into all of them, I‚Äôll leave a few links here for some other popular choices, that you may be interested to look into now that you know about PPO.\nTrust Region Policy Optimization (TRPO)\nDirect Preference Optimization (DPO)"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#how-ppo-updates-a-model-in-practice",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#how-ppo-updates-a-model-in-practice",
    "title": "RL for LLMs",
    "section": "How PPO Updates a Model in Practice",
    "text": "How PPO Updates a Model in Practice\nSo, we know how the PPO Objective is calculated. But how exactly is that implemented during training to update the weights of the model and change its behavior?\nWell, it‚Äôs simple! The PPO objective is directly applied as the loss function, so gradient descent directly optimizes this function by computing\n\\[\n\\frac{\\partial L_{CLIP}}{\\partial\\theta}\n\\]\nduring backpropogation to update the model‚Äôs weights.\n\n\nWhat Defines the Old and New Policy?\nAs we already discussed, PPO relies on comparing an old policy with a new policy and determining if the new policy is favorable.\nThe old policy (\\(\\pi_{\\theta(old)}\\)) is the model before applying the PPO update. The new policy (\\(\\pi_\\theta\\)) is the model after we‚Äôve updated it using PPO.\nBut you might be asking, how do we get old and new policies? What stages of training to they correspond to? The new policy is the one that we just got via gradient descent. And the old policy is from the previous update.\nConsider going through an epoch of updating a model using PPO. The typical PPO ‚Äúiteration‚Äù (a high-level loop): - Set Old Policy: Copy your current model weights as ‚Äúold policy.‚Äù - Gather Data: Roll out the environment using the old policy. - Compute Rewards & Advantages: Based on the data from step 2. - Run Multiple Mini-Batch Updates: Each update modifies the model from (\\(\\pi_{\\theta(old)}\\)) to (\\(\\pi_{\\theta}\\))\nAfter these updates finish, your ‚Äúnew policy‚Äù (\\(\\pi_{\\theta}\\)) is typically quite different from (\\(\\pi_{\\theta(old)}\\)).\nThen at the start of the next iteration, you set old policy = new policy (the final model from last iteration), gather fresh data, do more updates, repeat.\nIn many PPO codebases, they‚Äôll say ‚Äúwe do \\(K\\) epochs per iteration.‚Äù Those ‚Äúepochs‚Äù just mean \\(K\\) passes of gradient descent on the same collected batch. Each pass changes the policy slightly, but it‚Äôs all within a single iteration.\n\nüìå Note: As you might have guessed, this is a little bit more nuanced at the start of an iteration, as we don‚Äôt really yet have a \\(\\pi_\\theta\\) to act as the new policy yet! So for the first step of every iteration, \\(\\pi_\\theta = \\pi_{\\theta(old)}\\) and it updates as soon as it‚Äôs seen some of the data. See Appendix 3 for a much more in-depth discussion of this, including why mini-batches can improve learning with PPO over i.e full stochastic gradient descent."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#training-the-value-function",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#training-the-value-function",
    "title": "RL for LLMs",
    "section": "Training The Value Function",
    "text": "Training The Value Function\nWe‚Äôve talked about the advantage, which is calculated: \\[\nA(t) = R(t) - V_s(t)\n\\] And how \\(V_s(t)\\), the value function that determines the reward that the model ‚Äúexpects‚Äù to receive, can generally be quite complicated.\nWell, in practice, it is often something that is trained along with adjustments to the model, because accurate \\(V_s(t)\\) estimates lead to less noisy advantage estimates and more stable training.\nThe value function can be trained just like any regression model: - We want \\(V_{\\theta}(s_t)\\) to match the actual return \\(R_t\\). - So, we use i.e.¬†a Mean Squared Error (MSE) loss: $ L_V() = ( V_(s_t) - R_t )^2 $ - And update it alongside PPO‚Äôs policy loss as a separate loss term.\nSo a real, full PPO loss function, including the value function then becomes something like: \\[\nL_{\\text{total}} = L_{\\text{CLIP}} + c_1 L_V - c_2 H[\\pi]\n\\] where: - \\(L_{\\text{CLIP}}\\) = PPO policy loss. - \\(L_V\\) = Value function loss (MSE between \\(V(s)\\) and \\(R_t\\)). - \\(H[\\pi]\\) = Optional entropy term sometimes added to encourage exploration. - \\(c_1, c_2\\) = Tunable coefficients that control the relative weights of the loss components(e.g., \\(c_1 = 0.5, c_2 = 0.01\\)).\nThat‚Äôs all that I‚Äôll say about training the value function here, because while we will train it going forward, we‚Äôre going to keep it simple. But make sure to reference Appendix 4 about value functions if you‚Äôd like to know more."
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#an-example-in-code",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#an-example-in-code",
    "title": "RL for LLMs",
    "section": "An Example in Code",
    "text": "An Example in Code\nOk. We‚Äôre ready to put together a full simple pipeline of doing reinforcement learning on an LLM!\nWe‚Äôll make use of the trl library to handle the PPO part. We‚Äôll see below how this library handles all of the ‚Äúdifficult‚Äù parts of implementing reinforcement learning for us.\n\nüìå Note: In this tutorial, we are using a downgraded version of trl, 0.10.1. I found this version was easier to use and understand the code for. Make sure if you read the docs while going through this tutorial to switch them to that version, as more updated versions changed syntax somewhat drastically!\n\n\nDefining the Reward\nWe‚Äôre going to use a very simple reward function for this example, similar to the sentiment_reward that we used earlier in this tutorial, but improve it slightly and make some changes to make it easier to learn.\nThe changes we‚Äôll make: - Assign word-based values for postive-ness and negative-ness, so some words get higher rewards than others - Multiply our positive score by 5 to get stronger bias towards positive words - Add a small positive bias for all sentences (0.1) so our reward is always positive. This makes for more stable training generally.\n\n# Define improved sentiment reward function\ndef get_sentiment_reward(text: str) -&gt; float:\n    positive_words = {\n        'good': 1, 'great': 2, 'excellent': 3, 'wonderful': 3, 'amazing': 3,\n        'happy': 2, 'glad': 1, 'love': 3, 'like': 1, \"awesome\": 2,\n        \"fun\": 2, \"super\": 2, \"incredible\": 3, 'perfect': 3\n    }\n\n    negative_words = {\n        'bad': 1, 'awful': 2, 'terrible': 3, 'angry': 2, 'horrible': 3,\n        'lame': 1, 'hate': 3, \"gross\": 2, 'sad': 1, 'upset': 2\n    }\n\n    words = text.lower().split()\n    positive_score = sum(positive_words.get(word, 0) for word in words)\n    negative_score = sum(negative_words.get(word, 0) for word in words)\n\n    # Simple calculation with positive bias\n    reward = (positive_score * 5) - negative_score\n    return max(0.1, float(reward))  # Ensure minimum positive reward\n\n\n\nLoading A Model\nHere, we‚Äôll use GPT-2. This is a small LLM that generates text in a completion way (i.e it will finish sentences for you, not respond to what you say).\nWe will load this with AutoModelForCausalLMWithValueHead, which loads both an autoregressive model with a value head in addition to the language model head. The value head is then a trainable determination of the expected value of the model‚Äôs output.\ntrl will also want a ref_model: A copy of the original model, used to compare the trained model‚Äôs outputs against a reference. This reference model is used to compute KL divergence to prevent the model from deviating too much from its initial behavior.\nFinally, we load the tokenizer, which will turn our text into numbers the model can understand, and vice versa.\nThen, we define some generation arguments. These parameters control some more fine-grained details about how text is generated during training, and how we explore different text options during generation. In particular, these arguments determine how the model chooses a response to a prompt: - top_k: 0 When set to 0, there‚Äôs no limit on how many possible next tokens the model considers. If set to a value like 50, the model would only consider the 50 most likely next tokens and ignore all others. With top_k: 0, all possible tokens remain candidates, even unlikely ones. - top_p: 1.0 This controls ‚Äúnucleus sampling‚Äù (also called ‚Äúcumulative probability truncation‚Äù). A value of 1.0 means the model considers all tokens whose cumulative probability adds up to 100%. If set to 0.9, the model would only consider tokens whose cumulative probability adds up to 90%, effectively filtering out the long tail of unlikely tokens. - do_sample: True This determines whether the model uses sampling (probabilistic selection) or greedy decoding: - When True: The model randomly selects the next token based on the probability distribution, allowing for creativity and variation - When False: The model always picks the single most likely token (greedy decoding), leading to more predictable but potentially repetitive output\nThese parameters allow the model to generate more diverse outputs for a given prompt, increasing the exploration.\n\n# 1. load a pretrained model - with clear device placement\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nmodel.to(device)\n\nref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\nref_model.to(device)\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ngeneration_kwargs = {\n    \"min_length\": -1,         # No minimum length constraint\n    \"top_k\": 0,               # No top-k filtering (consider all tokens)\n    \"top_p\": 1.0,             # No nucleus sampling (consider all tokens)\n    \"do_sample\": True,        # Use sampling rather than greedy decoding\n    \"pad_token_id\": tokenizer.eos_token_id,  # Pad with EOS token\n    \"max_new_tokens\": 15,     # Generate at most 15 new tokens, will help speed up training\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up the PPO Configuration\nThe PPO Trainer also allows certain configuration parameters. We‚Äôll set some simple ones, but mostly leave this to the defaults. The learning rate is important here - we choose something quite small to keep training stable.\nThen, PPOTrainer just needs this config, the model, reference model, and tokenizer to eventually run the full PPO pipeline!\n\n# 2. initialize trainer with minimal parameters\nppo_config = {\n    \"mini_batch_size\": 1,     # Process one example at a time\n    \"batch_size\": 1,          # Total batch size for one optimization step\n    \"learning_rate\": 5e-6,    # Learning rate for optimizer\n    \"log_with\": None,         # No external logging\n}\n\nconfig = PPOConfig(**ppo_config)\nppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)\n\n\n\nGetting a Starting Point\nLet‚Äôs start by defining some simple prompts that should prompt positive responses. We‚Äôll see how GPT-2 performs out of the box on these prompts by checking the reward that we get from its default responses.\nYou‚Äôll see below, GPT-2 isn‚Äôt exactly the most eloquent or coherent model. That‚Äôs ok! We‚Äôre really just trying to train it to give us a bunch of postive words anyway, which is simple enough of a task that it should learn it fine!\n\n# Training prompts\nprompts = [\n    \"I feel happy when\",\n    \"The best part about this is\",\n    \"I love how\",\n    \"Today was great because\",\n]\n\n# Before training outputs\nprint(\"\\n=== Before Training Outputs ===\")\norig_responses = {}\norig_rewards = []\n\nfor prompt in prompts:\n    query_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        # Use the base model for generation to avoid CUDA errors\n        response = model.generate(\n            query_tensor,\n            **generation_kwargs\n        )\n    response_txt = tokenizer.decode(response[0])\n    orig_responses[prompt] = response_txt\n    reward_value = get_sentiment_reward(response_txt)\n    orig_rewards.append(reward_value)\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Response: {response_txt}\")\n    print(f\"Reward: {reward_value}\")\n\nprint(f\"\\nAverage initial reward: {np.mean(orig_rewards):.2f}\")\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\n\n=== Before Training Outputs ===\n\nPrompt: I feel happy when\nResponse: I feel happy when she is interested in contributing.\"\n\nWhen asked about others calling out the\nReward: 10.0\n\nPrompt: The best part about this is\nResponse: The best part about this is you can make it very nationalful. Again, it's all true.\nReward: 0.1\n\nPrompt: I love how\nResponse: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their\nReward: 15.0\n\nPrompt: Today was great because\nResponse: Today was great because we knew we could create more labor for people,\" Smith said. \"And\nReward: 10.0\n\nAverage initial reward: 8.78\n\n\n\n\nSetting up the Training Loop\nThis is the main loop where we‚Äôll generate responses, calculate rewards, and update the model using PPO. The logic will be as follows:\nFor each epoch &gt; For each of our prompts &gt;&gt; Tokenize the prompt &gt;&gt; &gt;&gt; Generate a response from the current version of the model (new policy) &gt;&gt; &gt;&gt; Calculate the reward of that response &gt;&gt; &gt;&gt; Give the PPO trainer the prompt, response, and corresponding reward\nThe magic really happens here: Give the PPO trainer the prompt, response, and corresponding reward. The trl library and PPOTrainer that we set up are aware of the model, reference model, and tokenizer. This step handles using the reward we passed in, as well as the prompt + response that corresponded to that reward to: - Calculate probabilities: Computes how likely the generated tokens were under both the current model (new policy) and reference model (old policy) - Compute advantage: Determines how much better or worse the generated response performed compared to what was expected - Apply the PPO objective: Uses the clipped PPO objective function to limit how much the model changes in a single step - Perform backpropagation: Updates the model weights to make high-reward responses more likely in the future - Update the value function: The value function (which is part of the model from AutoModelForCausalLMWithValueHead) estimates the expected reward for a given state is also updated. - Enforce KL divergence: Ensures the new policy doesn‚Äôt deviate too far from the old policy, maintaining coherent text generation - Return statistics: Provides metrics about the update like loss values, KL divergence, and entropy for monitoring the training process\nThe line: stats = ppo_trainer.step([query[0]], [response[0]], [rewards])) encapsulates the core RL algorithm that enables the model to learn from the reward signal. It‚Äôs where the model actually learns to adjust its probability distribution to favor token sequences that lead to higher sentiment rewards. And trl does all of the hard stuff in the background for us!\n\nprint(\"\\n=== Starting Training ===\")\n\n# Prepare for tracking training statistics\nepoch_stats = defaultdict(list)\nepoch_rewards = []\n\n# Run for multiple epochs\nnum_epochs = 12\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}\")\n    epoch_reward = 0\n    epoch_responses = []\n\n    # Shuffle prompts each epoch for better generalization\n    np.random.shuffle(prompts)\n\n    # Process each prompt\n    for prompt in prompts:\n          # Encode the prompt\n          encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n          # Generate a response using the current model\n          with torch.no_grad():\n              response = model.generate(\n                  encoded_prompt,\n                  **generation_kwargs\n              )\n\n          # Decode the response\n          response_txt = tokenizer.decode(response[0])\n\n          # Calculate reward for this response\n          reward_value = get_sentiment_reward(response_txt)\n          rewards = torch.tensor([reward_value], dtype=torch.float, device=device)\n\n          # Store for reporting\n          epoch_reward += reward_value\n          epoch_responses.append((prompt, response_txt, reward_value))\n\n          # Prepare tensors for PPO step\n          # PPO requires specific tensor shapes\n          query = encoded_prompt[0].unsqueeze(0)  # Reshape for PPO\n          response = response[0].unsqueeze(0)     # Reshape for PPO\n\n          # Train step - update model using PPO\n          stats = ppo_trainer.step([query[0]], [response[0]], [rewards])\n\n          # Track training metrics\n          for k, v in stats.items():\n              if v is not None:\n                  epoch_stats[k].append(v)\n\n    # Calculate and report epoch statistics\n    if epoch_responses:\n        avg_reward = epoch_reward / len(epoch_responses)\n        epoch_rewards.append(avg_reward)\n\n        print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n        print(f\"Average Reward: {avg_reward:.2f}\")\n\n        # Print reward trend\n        if epoch &gt; 0:\n            reward_change = avg_reward - epoch_rewards[-2]\n            print(f\"Reward Change: {reward_change:+.2f}\")\n\n        # Print sample responses\n        print(\"\\nSample responses from this epoch:\")\n        for i, (prompt, response, reward) in enumerate(epoch_responses[:2]):\n            print(f\"Prompt: {prompt}\")\n            print(f\"Response: {response}\")\n            print(f\"Reward: {reward:.2f}\")\n    else:\n        print(\"No successful training steps this epoch\")\n\n\n=== Starting Training ===\n\nEpoch 1\n\n--- Epoch 1 Summary ---\nAverage Reward: 11.25\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I don't think I'd rule it out because I'm very fortunate to\nReward: 15.00\nPrompt: Today was great because\nResponse: Today was great because to see all the flames in the air. It was a whirlwind of congressional\nReward: 10.00\n\nEpoch 2\n\n--- Epoch 2 Summary ---\nAverage Reward: 13.78\nReward Change: +2.53\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how tragic this was\n\nI love it so much I feel like the tornado\nReward: 35.00\nPrompt: The best part about this is\nResponse: The best part about this is that, from a human point of view, all models bear all of the\nReward: 0.10\n\nEpoch 3\n\n--- Epoch 3 Summary ---\nAverage Reward: 17.52\nReward Change: +3.75\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when I keep seeing Lisp. I like it. It's a joy to work\nReward: 15.00\nPrompt: I love how\nResponse: I love how they have tried. I love that they wore their mark on their hat with\nReward: 30.00\n\nEpoch 4\n\n--- Epoch 4 Summary ---\nAverage Reward: 12.53\nReward Change: -5.00\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how you make method shots because I love orange funk and you've grown me up\nReward: 30.00\nPrompt: I feel happy when\nResponse: I feel happy when I have another large cock in my hips\n\nLatelose: Acting\nReward: 10.00\n\nEpoch 5\n\n--- Epoch 5 Summary ---\nAverage Reward: 12.53\nReward Change: +0.00\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when I I I I I I I I I I I I I I I\nReward: 10.00\nPrompt: Today was great because\nResponse: Today was great because they ate a little purple cabbage or rib eye of deind. Truly Complete\nReward: 10.00\n\nEpoch 6\n\n--- Epoch 6 Summary ---\nAverage Reward: 8.78\nReward Change: -3.75\n\nSample responses from this epoch:\nPrompt: I feel happy when\nResponse: I feel happy when my rest rest becomes not difficult... But when my recovery growsnt so urgent\nReward: 10.00\nPrompt: I love how\nResponse: I love how much multiplex are haunted areas. How pipe, wood, torches, etc\nReward: 15.00\n\nEpoch 7\n\n--- Epoch 7 Summary ---\nAverage Reward: 13.78\nReward Change: +5.00\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how they work with us,\" Barzero argues with both hands. And they are\nReward: 15.00\nPrompt: I feel happy when\nResponse: I feel happy when my sleep. I feel happy when I I I be happy uncertain uncertain uncertain\nReward: 30.00\n\nEpoch 8\n\n--- Epoch 8 Summary ---\nAverage Reward: 16.27\nReward Change: +2.50\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how it. Ourld literally bind for a happy it to per chance or for\nReward: 25.00\nPrompt: The best part about this is\nResponse: The best part about this is long hair:\nNote:You can choose between option:UnlockedSince\nReward: 0.10\n\nEpoch 9\n\n--- Epoch 9 Summary ---\nAverage Reward: 8.78\nReward Change: -7.50\n\nSample responses from this epoch:\nPrompt: Today was great because\nResponse: Today was great because we are currently loving and embracing being children. We are finding joy in understanding\nReward: 10.00\nPrompt: I feel happy when\nResponse: I feel happy when we were even able to get close to our spiritual helper.\n\nSo\nReward: 10.00\n\nEpoch 10\n\n--- Epoch 10 Summary ---\nAverage Reward: 10.03\nReward Change: +1.25\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I look at how I look at grabbing at back grabbing at back grabbing at\nReward: 15.00\nPrompt: I feel happy when\nResponse: I feel happy when you g m t,\n\nJ\n\nacje\n\nYes\nReward: 10.00\n\nEpoch 11\n\n--- Epoch 11 Summary ---\nAverage Reward: 10.00\nReward Change: -0.03\n\nSample responses from this epoch:\nPrompt: I love how\nResponse: I love how I I I I I I I I IIIIIII\nReward: 15.00\nPrompt: Today was great because\nResponse: Today was great because malarkey women and men changed.The grateful ones had romantic view;\nReward: 10.00\n\nEpoch 12\n\n--- Epoch 12 Summary ---\nAverage Reward: 15.03\nReward Change: +5.03\n\nSample responses from this epoch:\nPrompt: Today was great because\nResponse: Today was great because of the freshandfreshlight You've commanduedTheOfGoodTheAvoid\nReward: 10.00\nPrompt: The best part about this is\nResponse: The best part about this is that on the gazelles ‚Äì I used to use them ‚Äì they\nReward: 0.10\n\n\n\n\nSeeing How We Did\nFinally, we can look a bit deeper to see how well we did. Let‚Äôs investigate in more depth: - Before vs After Comparison: For each prompt, we‚Äôll compare the original model‚Äôs response with our RL-trained model‚Äôs response. This direct comparison helps us visualize the specific changes in text generation. - Individual Reward Metrics: We calculate the sentiment reward for both the original and trained responses. The difference between these scores shows how much our model has improved at generating positive text. - Aggregate Improvement: By averaging rewards across all prompts, we can quantify the overall improvement from RL training. A positive change indicates successful optimization toward our sentiment objective.\n\n# Compare before/after\nprint(\"\\n=== After Training Outputs ===\")\nfinal_rewards = []\n\nfor prompt in prompts:\n    try:\n        # Generate using the standard method to avoid errors\n        encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            response = model.generate(\n                encoded_prompt,\n                **generation_kwargs\n            )\n        response_txt = tokenizer.decode(response[0])\n        reward_value = get_sentiment_reward(response_txt)\n        final_rewards.append(reward_value)\n\n        # Compare with original\n        orig_reward = get_sentiment_reward(orig_responses[prompt])\n\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Before: {orig_responses[prompt]}\")\n        print(f\"After: {response_txt}\")\n        print(f\"Reward Before: {orig_reward:.2f}\")\n        print(f\"Reward After: {reward_value:.2f}\")\n        print(f\"Improvement: {reward_value - orig_reward:+.2f}\")\n    except Exception as e:\n        print(f\"Error evaluating prompt '{prompt}': {e}\")\n\n# Print final stats\nif final_rewards:\n    print(\"\\n=== Training Results ===\")\n    print(f\"Starting Average Reward: {np.mean(orig_rewards):.2f}\")\n    print(f\"Ending Average Reward: {np.mean(final_rewards):.2f}\")\n    print(f\"Change: {np.mean(final_rewards) - np.mean(orig_rewards):+.2f}\")\n\n\n=== After Training Outputs ===\n\nPrompt: I feel happy when\nBefore: I feel happy when she is interested in contributing.\"\n\nWhen asked about others calling out the\nAfter: I feel happy when ordinary people are free and all my woes are banished because of quite full of\nReward Before: 10.00\nReward After: 10.00\nImprovement: +0.00\n\nPrompt: The best part about this is\nBefore: The best part about this is you can make it very nationalful. Again, it's all true.\nAfter: The best part about this is the paperwork. Write the cards up front, step by step, step by\nReward Before: 0.10\nReward After: 0.10\nImprovement: +0.00\n\nPrompt: I love how\nBefore: I love how Cyprus redlines it, especially giving immigrants an arrangement for a pension for their\nAfter: I love how Snowman looks too when she looks like Rocky Simon Newell. I love\nReward Before: 15.00\nReward After: 35.00\nImprovement: +20.00\n\nPrompt: Today was great because\nBefore: Today was great because we knew we could create more labor for people,\" Smith said. \"And\nAfter: Today was great because of was amazing and appreciated and astounding were all the immense and unrelasibility\nReward Before: 10.00\nReward After: 25.00\nImprovement: +15.00\n\n=== Training Results ===\nStarting Average Reward: 8.78\nEnding Average Reward: 17.52\nChange: +8.75\n\n\nAnd there you have it! We sucessfully trained GPT-2 to give us more positive words in its responses.\nNow, of course, we used a super simple reward here, and not a particularly good model (no offense, GPT-2), so we can see a lot of repeated words contributing to that positive response. As we discussed, in reality, a reward function should be more complicated, and our prompts used for training should be much more diverse than just 4. Still, you now know how to set up a PPO training pipeline!"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#collecting-human-feedback",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#collecting-human-feedback",
    "title": "RL for LLMs",
    "section": "Collecting Human Feedback",
    "text": "Collecting Human Feedback\nIn RLHF, human judgments about model outputs are collected. Unlike our algorithmic reward function that automatically calculated a score, RLHF relies on actual human preferences.\nTypically, a real RLHF pipeline will involve humans doing pairwise comparisons of responses. Rather than asking humans to provide absolute scores, RLHF typically uses comparative judgments where annotators choose which of two responses they prefer. This involves: - Prompt Selection: A diverse set of prompts is created to cover different topics, skills, and potential failure modes. - Response Generation: For each prompt, the model generates multiple responses using different sampling parameters. - Human Annotation: Human annotators are presented with a prompt and two model-generated responses, then asked to select which one is better according to specific criteria.\n\n# Simple simulation of human preference collection interface\ndef collect_human_preference(prompt, response_a, response_b):\n    print(f\"Prompt: {prompt}\\n\")\n    print(f\"Response A:\\n{response_a}\\n\")\n    print(f\"Response B:\\n{response_b}\\n\")\n\n    while True:\n        choice = input(\"Which response do you prefer? (A/B/Tie): \").upper()\n        if choice in [\"A\", \"B\", \"TIE\"]:\n            return choice\n        print(\"Invalid input. Please enter A, B, or Tie.\")\n\n# Example prompts and responses\nexamples = [\n    {\n        \"prompt\": \"Explain the concept of reinforcement learning to a high school student.\",\n        \"response_a\": \"Reinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes.\",\n        \"response_b\": \"Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward.\"\n    },\n    {\n        \"prompt\": \"What are some ways to reduce stress?\",\n        \"response_a\": \"Reducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing.\",\n        \"response_b\": \"To reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system.\"\n    }\n]\n\n# Run the preference collection for demonstration\ncollected_preferences = []\nfor i, example in enumerate(examples):\n    print(f\"\\n===== Example {i+1} =====\")\n    preference = collect_human_preference(\n        example[\"prompt\"],\n        example[\"response_a\"],\n        example[\"response_b\"]\n    )\n    collected_preferences.append({\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": \"response_a\" if preference == \"A\" else \"response_b\" if preference == \"B\" else \"tie\",\n        \"rejected\": \"response_b\" if preference == \"A\" else \"response_a\" if preference == \"B\" else \"tie\"\n    })\n    print(f\"You preferred Response {preference}\")\n\n\n===== Example 1 =====\nPrompt: Explain the concept of reinforcement learning to a high school student.\n\nResponse A:\nReinforcement learning is like training a dog. You give it treats when it does something good, and it learns to do more of that behavior. Similarly, an AI gets 'rewards' for good actions and 'penalties' for bad ones, so it gradually learns what actions lead to good outcomes.\n\nResponse B:\nReinforcement learning is a machine learning paradigm where an agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes cumulative reward.\n\nWhich response do you prefer? (A/B/Tie): B\nYou preferred Response B\n\n===== Example 2 =====\nPrompt: What are some ways to reduce stress?\n\nResponse A:\nReducing stress involves exercise, meditation, and proper sleep. These activities can help balance cortisol levels and improve your overall wellbeing.\n\nResponse B:\nTo reduce stress, try deep breathing, going for walks, limiting caffeine, practicing mindfulness, and talking to friends. Self-care activities like taking a warm bath or reading a book can also help you relax and reset your nervous system.\n\nWhich response do you prefer? (A/B/Tie): B\nYou preferred Response B\n\n===== Collected Preferences =====\nExample 1: You chose the response_b over the response_a\nExample 2: You chose the response_b over the response_a\n\n\nWhen collecting real human feedback, annotators are typically given specific criteria to evaluate, like:\n\nHelpfulness: How well does the response address the user‚Äôs request?\nTruthfulness: Is the information accurate and factual?\nHarmlessness: Does the response avoid harmful, offensive, or misleading content?\nClarity: Is the response clearly written and easy to understand?"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#reward-model-training",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#reward-model-training",
    "title": "RL for LLMs",
    "section": "Reward Model Training",
    "text": "Reward Model Training\nWhile you get direct feedback this way, it would be difficult to collect real human feedback over all responses that you could possibly reasonably expect. So instead, a reward model is often trained to predict what humans like better. Better responses usually have some things in common, they‚Äôre maybe: - wordier - friendlier - more factual\nSo these more generalized properties can be learned, and then applied to prompts en masse, rather than needs humans to look at hundreds of thousands of responses individually. The reward model bridges the gap between collected human judgments and automated rewards needed for reinforcement learning.\n\nConverting Preferences to a Reward Model\nThe reward model is essentially a classifier trained to predict human preferences. It takes in a prompt and response, and outputs a scalar value representing the ‚Äúquality‚Äù of the response according to human preferences.\nIf we define:\n\\((x, y_w, y_l)\\) as a triplet where \\(x\\) is the prompt, \\(y_w\\) is the preferred (winning) response, and \\(y_l\\) is the less preferred (losing) response,\nand\n\\(r_\\theta(x, y)\\) as our reward model with parameters \\(\\theta\\) that outputs a scalar reward for prompt \\(x\\) and response \\(y\\).\nThen, we train the reward model to maximize the log probability of the human preferences: \\[\n{L}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]\n\\]\n\n\\({L}(\\theta)\\) is the loss function we‚Äôre trying to minimize, where \\(\\theta\\) represents all the parameters of our reward model.\n\\(\\mathbb{E}_{(x, y_w, y_l) \\sim D}\\) is the expected value over all triplets sampled from our dataset D. In simpler terms, it means ‚Äúthe average across all our training examples.‚Äù\n\\(r_\\theta(x, y_w)\\) is the reward score our model assigns to the winning (preferred) response \\(y_w\\) given prompt \\(x\\).\n\\(r_\\theta(x, y_l)\\) is the reward score our model assigns to the losing (less preferred) response \\(y_l\\) given the same prompt \\(x\\).\n\\(\\sigma(z)\\) is the sigmoid function, defined as \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), which maps any real number to a value between 0 and 1.\n\nThe equation expresses a Bradley-Terry model, which is used for pairwise comparisons. For each pair of responses, we compute the difference in their rewards: \\(r_\\theta(x, y_w) - r_\\theta(x, y_l)\\). We pass this difference through a sigmoid function, which gives us the probability that the model correctly predicts the human preference. We want to maximize this probability, which is equivalent to minimizing the negative log probability (since loss functions want to be minimized). Then, we average this loss across all training examples.\n\n\nTraining a Reward Model\nThen, we can train a reward model based on our triplets \\((x, y_w, y_l)\\).\nBelow is a very simplified example. Instead of a real neural network, we just use a function simulate_reward_scores to see how we could calculate the loss based on the preferences indicated in the last code cell. This function simply assigns a score to a response based purely on its length.\nIn reality, this calculated loss would help a model readjust its predicted rewards.\n\n# Simulate a reward model's outputs\n# In reality, these would come from a neural network\ndef simulate_reward_scores(response_a, response_b):\n    \"\"\"Simulate reward scores for demonstration purposes\"\"\"\n    # Just a simple length-based score for demonstration\n    score_a = 0.5 + 0.01 * len(response_a)\n    score_b = 0.5 + 0.01 * len(response_b)\n    return {\"response_a\": score_a, \"response_b\": score_b}\n\n# Calculate reward model loss\ndef reward_model_loss(scores, chosen, rejected):\n    \"\"\"Calculate the loss based on preference pair and model scores\"\"\"\n    chosen_score = scores[chosen]\n    rejected_score = scores[rejected]\n\n    # Print scores for demonstration\n    print(f\"Chosen response score: {chosen_score:.4f}\")\n    print(f\"Rejected response score: {rejected_score:.4f}\")\n    print(f\"Score difference (chosen - rejected): {chosen_score - rejected_score:.4f}\")\n\n    # The core loss function: -log(sigmoid(chosen_score - rejected_score))\n    # This encourages the model to give the preferred response a higher score\n    sigmoid = 1 / (1 + math.exp(-(chosen_score - rejected_score)))\n    loss = -math.log(sigmoid)\n\n    return loss\n\n# Using data from our previously collected human preferences\nfor i, preference in enumerate(collected_preferences):\n    example = examples[i]\n    prompt = example[\"prompt\"]\n    response_a = example[\"response_a\"]\n    response_b = example[\"response_b\"]\n    chosen = preference[\"chosen\"]\n    rejected = preference[\"rejected\"]\n\n    print(f\"\\n===== Example {i+1}: {prompt} =====\")\n    print(f\"You preferred: {chosen}\")\n\n    # Initial model scoring (before training)\n    scores = simulate_reward_scores(response_a, response_b)\n    loss = reward_model_loss(scores, chosen, rejected)\n    print(f\"Loss: {loss:.4f}\")\n\n\n===== Example 1: Explain the concept of reinforcement learning to a high school student. =====\nYou preferred: response_b\nChosen response score: 2.8900\nRejected response score: 3.2500\nScore difference (chosen - rejected): -0.3600\nLoss: 0.8893\n\n===== Example 2: What are some ways to reduce stress? =====\nYou preferred: response_b\nChosen response score: 2.8900\nRejected response score: 2.0000\nScore difference (chosen - rejected): 0.8900\nLoss: 0.3441"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#rlhf-pipeline-implementation",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#rlhf-pipeline-implementation",
    "title": "RL for LLMs",
    "section": "RLHF Pipeline Implementation",
    "text": "RLHF Pipeline Implementation\nOnce trained, the reward model replaces the handcrafted reward function we used earlier. During RL training:\n\nThe model generates a response to a prompt\nThe reward model evaluates the response, producing a scalar reward\nThis reward is used to update the model via PPO, just as we did with our algorithmic reward\n\nSo, where before we had\nreward = get_sentiment_reward(response_txt)  # From our sentiment detection function\nNow with RLHF:\nreward = reward_model(prompt, response_txt)  # From our trained reward model\nWhile this change may look simple, it fundamentally transforms how the system learns - from optimizing for predefined metrics to optimizing for learned human preferences.\nSo, the necessary steps for a complete RLHF pipeline include:\n\nInitial LLM Training: Train or fine-tune a base LLM using standard methods\nHuman Preference Collection: Gather human judgments on model outputs\nReward Model Training: Train a reward model to predict human preferences\nRL Fine-tuning: Use the reward model to guide policy optimization"
  },
  {
    "objectID": "posts/RL-for-LLMs/RL_for_LLMs.html#practical-considerations-for-scaling-rlhf",
    "href": "posts/RL-for-LLMs/RL_for_LLMs.html#practical-considerations-for-scaling-rlhf",
    "title": "RL for LLMs",
    "section": "Practical Considerations for Scaling RLHF",
    "text": "Practical Considerations for Scaling RLHF\nImplementing RLHF at scale involves several important considerations:\nQuality of Human Feedback: Diverse annotator pools to avoid bias, clear guidelines to ensure consistency, and quality control measures to identify unreliable annotations can all enhance the quality of the data that trains the reward model.\nComputational Requirements: Training a reward model adds another large model to the pipeline, and PPO fine-tuning is more compute-intensive than supervised fine-tuning. Multiple runs may also be needed to find optimal hyperparameters.\nReward Hacking: Models can also learn to exploit weaknesses in the reward model just like they can from an algorithmic model, so it‚Äôs important to regularly update the reward model with new human judgments. Adding KL penalties can help to prevent excessive deviation from the base model.\nDistribution Shift: As the policy model improves, it generates responses outside the reward model‚Äôs training distribution. Iterative approaches that collect new human feedback on improved model outputs help address this.\nHybrid Approaches: Combining RLHF with rule-based rewards for certain constraints can improve overall quality. Multi-objective optimization can balance different desired qualities, and ensemble reward models are sometimes used to capture different aspects of human preferences.\nIn practice, RLHF is often implemented as an iterative process:\n\nTrain initial reward model from human preferences\nPerform RL fine-tuning using this reward model\nGenerate new responses with the improved policy\nCollect new human preferences on these responses\nRetrain or update the reward model 6.Repeat the process\n\nThis iterative approach helps address distribution shift and ensures the reward model keeps pace with policy improvements."
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html",
    "href": "posts/KV-Caching/kv_caching.html",
    "title": "KV Caching",
    "section": "",
    "text": "This notebook provides a hands-on exploration of KV caching in language model text generation, specifically using LLaMa 3.2 1B. We‚Äôll examine how caching works, its benefits for inference speed, and its implications for model state management.\n\n  \n\n\nWhen a language model processes text, it doesn‚Äôt just look at one word at a time - it builds up a complex internal state that represents its ‚Äúunderstanding‚Äù of the entire context. This state consists of key-value pairs at each layer of the transformer architecture, which encode the relationships and patterns in the input text.\n\n\nWithout caching, here‚Äôs what happens every time you ask for a completion:\n\nThe model takes your prompt (e.g., ‚ÄúThe story begins with a‚Äù)\nConverts it to tokens\nProcesses these tokens through all its layers, building up its internal state\nUses this state to predict the next token\nAdds the new token to the input\nRepeats steps 3-5 until done.\n\nWhat this means is thats after concatenating each newly generated token to the running sequence, it recomputes everything from scratch with the new, longer input, for every single token generated.\nAnd, that means if you want five different endings to the same prompt, the model has to process ‚ÄúThe story begins with a‚Äù through this process five separate times from scratch!\n\n\n\nCaching is like giving the model a short-term memory. Here‚Äôs how it works:\nFirst time: - Process the prompt normally through the steps above - But save the internal state (key-value pairs for KV caching) after processing the prompt\nSubsequent times: - Instead of reprocessing the prompt, load the saved state which is already prepped to generate the next token - Start generating from there\nIn this notebook, we‚Äôll demonstrate both of these ways of generating output from a model, and look at some implications for how we can use KV caching and a saved internal state of the model to get better, faster, responses!"
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html#caching-what-is-it",
    "href": "posts/KV-Caching/kv_caching.html#caching-what-is-it",
    "title": "KV Caching",
    "section": "",
    "text": "When a language model processes text, it doesn‚Äôt just look at one word at a time - it builds up a complex internal state that represents its ‚Äúunderstanding‚Äù of the entire context. This state consists of key-value pairs at each layer of the transformer architecture, which encode the relationships and patterns in the input text.\n\n\nWithout caching, here‚Äôs what happens every time you ask for a completion:\n\nThe model takes your prompt (e.g., ‚ÄúThe story begins with a‚Äù)\nConverts it to tokens\nProcesses these tokens through all its layers, building up its internal state\nUses this state to predict the next token\nAdds the new token to the input\nRepeats steps 3-5 until done.\n\nWhat this means is thats after concatenating each newly generated token to the running sequence, it recomputes everything from scratch with the new, longer input, for every single token generated.\nAnd, that means if you want five different endings to the same prompt, the model has to process ‚ÄúThe story begins with a‚Äù through this process five separate times from scratch!\n\n\n\nCaching is like giving the model a short-term memory. Here‚Äôs how it works:\nFirst time: - Process the prompt normally through the steps above - But save the internal state (key-value pairs for KV caching) after processing the prompt\nSubsequent times: - Instead of reprocessing the prompt, load the saved state which is already prepped to generate the next token - Start generating from there\nIn this notebook, we‚Äôll demonstrate both of these ways of generating output from a model, and look at some implications for how we can use KV caching and a saved internal state of the model to get better, faster, responses!"
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html#hugging-face-authentication",
    "href": "posts/KV-Caching/kv_caching.html#hugging-face-authentication",
    "title": "KV Caching",
    "section": "Hugging Face Authentication",
    "text": "Hugging Face Authentication\nLLaMa 3.2 requires authentication with Hugging Face to access the model. You‚Äôll need to: 1. Have a Hugging Face account 2. Accept the LLaMa 3.2 model terms of use on the Hugging Face model page 3. Create an access token on Hugging Face (https://huggingface.co/settings/tokens)\nAfter you have your access token and have accepted the terms, the code below will help you log in:\n\nfrom huggingface_hub import login\nimport getpass\n\ntoken = getpass.getpass(\"Enter your Hugging Face token: \")\nlogin(token=token)\n\n# Verify login\nprint(\"Login status: Authenticated with Hugging Face\")\n\nEnter your Hugging Face token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\nLogin status: Authenticated with Hugging Face\n\n\n\nmodel_name = \"meta-llama/Llama-3.2-1B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)\n\n\nBefore we dive into caching, let‚Äôs look at how the model processes text. We‚Äôll create a simple function to tokenize and process text, showing the internal states at each step.\n\ndef inspect_tokens(text):\n    \"\"\"Display token information for a given text.\"\"\"\n    tokens = tokenizer.encode(text, return_tensors=\"pt\")\n    print(f\"Text: {text}\")\n    print(f\"Number of tokens: {len(tokens[0])}\")\n    print(\"\\nToken IDs:\")\n    print(tokens[0].tolist())\n    print(\"\\nDecoded tokens:\")\n    print([tokenizer.decode([t]) for t in tokens[0]])\n    return tokens\n\n# Example usage\nsample_text = \"The quick brown fox\"\ntokens = inspect_tokens(sample_text)\n\nText: The quick brown fox\nNumber of tokens: 5\n\nToken IDs:\n[128000, 791, 4062, 14198, 39935]\n\nDecoded tokens:\n['&lt;|begin_of_text|&gt;', 'The', ' quick', ' brown', ' fox']"
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html#a-quick-introduction-to-transformer-architecture",
    "href": "posts/KV-Caching/kv_caching.html#a-quick-introduction-to-transformer-architecture",
    "title": "KV Caching",
    "section": "A Quick Introduction to Transformer Architecture",
    "text": "A Quick Introduction to Transformer Architecture\nBefore we understand caching, we need to understand how transformers process sequences. In a transformer like Llama, text flows through the model in several stages:\n\nTokenization: Text ‚Üí Token IDs\nToken Embeddings: Token IDs ‚Üí Vectors\nMultiple Transformer Layers: Each containing:\n\nSelf-attention mechanism\nFeed-forward neural networks\n\n\n\nThe Self-Attention Mechanism: Where Caching Happens\nThe self-attention portion is where the caching can happen. Let‚Äôs look more specifically at what happens in one of these layers.\n\nStep 1: Query, Key, Value Creation\nFor each token in the sequence, the model creates three vectors: - Query (\\(\\widehat{Q}\\)): What the current token is looking for - Key (\\(\\widehat{K}\\)): What the token offers to others - Value (\\(\\widehat{V}\\)): The actual information content\nFor example, for a simple sequence like ‚ÄúThe cat sat‚Äù, you would need to calculate:\nToken 1 (‚ÄúThe‚Äù):\n\n\\(Q_1\\) = \\(W_Q\\) √ó \\(x_1\\)\n\\(K_1\\) = \\(W_K\\) √ó \\(x_1\\)\n\\(V_1\\) = \\(W_V\\) √ó \\(x_1\\)\n\nToken 2 (‚Äúcat‚Äù):\n\n\\(Q_2\\) = \\(W_Q\\) √ó \\(x_2\\)\n\\(K_2\\) = \\(W_K\\) √ó \\(x_2\\)\n\\(V_2\\) = \\(W_V\\) √ó \\(x_2\\)\n\n\n\nToken 3 (‚Äúsat‚Äù):\n\n\\(Q_3\\) = \\(W_Q\\) √ó \\(x_3\\)\n\\(K_3\\) = \\(W_K\\) √ó \\(x_3\\)\n\\(V_3\\) = \\(W_V\\) √ó \\(x_3\\)\n\nCalculating a \\(Q,K,V\\) vector for each word in the sequence, from weight matricies \\(W_Q, W_K, W_V\\), on the tokenized vector \\(x\\) for each word.\n\n\n\nStep 2: Attention Score Computation\nThen, these vectors come together to form \\(Q, K, V\\) matrices.\nfor instance:\n\\[\nQ =\n\\begin{bmatrix}\nQ_1 \\\\\nQ_2 \\\\\nQ_3\n\\end{bmatrix}\n\\]\nwhere this \\(Q\\) is a matrix of size seq_length x hidden_dim: One \\(\\widehat{Q}\\) vector per token in the sequence, which has its length determined by the size of the matrix \\(W_Q\\), a hard-coded dimension of the model.\nBecause in reality, there are multiple ‚Äúheads‚Äù in each attention layer (multiple \\(W_Q, W_K, W_V\\)‚Äôs, the dimensions are:\n\n\\(Q\\): [num_heads, seq_length, head_dim]\n\\(K\\): [num_heads, seq_length, head_dim]\n\\(V\\): [num_heads, seq_length, head_dim]\n\nWhere: - head_dim = d_h / num_heads - seq_length grows as we generate\nSo, in the above example, the K matrix, for instance,\nWhen the model processes a sequence, it first computes these \\(Q, K, V\\) matricies for the input sequence.\nThen, an attention score is calculated from these matricies as:\n\\[\nAttention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n\\]\nWhich is what we really are wanting from an attention layer to progress through. This attention score says how much every token in the sequence should ‚Äúpay attention‚Äù to every other in the sequence, and is used to contextualize the input in order to generate the next token."
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html#why-caching-matters-the-computational-challenge",
    "href": "posts/KV-Caching/kv_caching.html#why-caching-matters-the-computational-challenge",
    "title": "KV Caching",
    "section": "Why Caching Matters: The Computational Challenge",
    "text": "Why Caching Matters: The Computational Challenge\nSo, attention wants to be able to know about all of the tokens in a sequence. And it‚Äôs computation will depend on what‚Äôs in the sequence, so it makes sense that we need to recalculate it.\nBut what about what comes before? When we process ‚Äúthe cat‚Äù, is computing all of \\(Q_1, K_1, V_1, Q_2, K_2, V_2\\) and for ‚Äúthe cat sat‚Äù, all of \\(Q_1, K_1, V_1, Q_2, K_2, V_2, Q_3, K_3, V_3\\). You can see how, for long prompts, this quickly becomes a lot.\nWithout caching, when parsing a sequence, the model must: 1. Compute Q, K, V for the current token 1. Recompute Q, K, V for ALL previous tokens 2. Compute attention scores for ALL combinations 3. Process through ALL layers again\nFor a sequence of length \\(L\\), this means \\(O(L¬≤)\\) computations for EACH new token!\nBut why regenerate all of the \\(Q, K, V\\) vectors of previous parts of the sequence? \\(W_Q, W_K,\\) and \\(W_V\\) are fixed weight matrices. \\(Q, K, V\\) matrices are changing as more is added to the sequence, but they‚Äôre just getting added to, a calculation for \\(Q_1, K_1, V_1\\) is the same every time."
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html#attention-at-inference-time",
    "href": "posts/KV-Caching/kv_caching.html#attention-at-inference-time",
    "title": "KV Caching",
    "section": "Attention at Inference Time",
    "text": "Attention at Inference Time\nBefore we go on, we need to clear up a nuance about how generating the next token (doing inference) happens, which changes how this attention is calculated slighlty at inference time vs when batch-processing a whole (determined) sequence like we just laid out above, which you would do during training.\nWhen it comes to generating the next new word, we need to get the attention score, which contextualizes the current word to all others that came before it. But consider how this calculation works.\n\\(Q, K,\\) and \\(V\\) are matrices. So, if:\n\\[\nQ =\n\\begin{bmatrix}\nq_{the,1} & q_{the,2} & q_{the,3} \\\\\nq_{cat,1} & q_{cat,2} & q_{cat,3}\n\\end{bmatrix}\n\\]\n\\[\nK^T =\n\\begin{bmatrix}\nk_{the,1} & k_{cat,1} \\\\\nk_{the,2} & k_{cat,2} \\\\\nk_{the,3} & k_{cat,3}\n\\end{bmatrix}\n\\]\nThen, \\[\nQ \\times K^T =\n\\begin{bmatrix}\n(q_{the,1} \\times k_{the,1} + q_{the,2} \\times k_{the,2} + q_{the,3} \\times k_{the,3}) & (q_{the,1} \\times k_{cat,1} + q_{the,2} \\times k_{cat,2} + q_{the,3} \\times k_{cat,3}) \\\\\n(q_{cat,1} \\times k_{the,1} + q_{cat,2} \\times k_{the,2} + q_{cat,3} \\times k_{the,3}) & (q_{cat,1} \\times k_{cat,1} + q_{cat,2} \\times k_{cat,2} + q_{cat,3} \\times k_{cat,3})\n\\end{bmatrix}\n\\]\nNow, recall what \\(Q,K,\\) and \\(V\\) are meant to represent. \\(Q\\) is the ‚Äúquery‚Äù this ‚Äúasks‚Äù about the token in question. \\(K\\) the ‚Äúkey‚Äù says what information a token has to offer, and \\(V\\) is the ‚Äúvalue‚Äù that stores the actual information to give.\nWe don‚Äôt really need any to ask any questions (i.e store any ‚ÄúQ‚Äù element) for a word we‚Äôve already generated - there‚Äôs nothing more to ‚Äúask‚Äù or ‚Äúunderstand‚Äù about a token in the past. During training, computing Q vectors for all tokens is important because the model needs to learn how each token influences and is influenced by every other token in the sequence. But during inference, we only care about how our new token should relate to what came before. We just need the current token‚Äôs Q vector to ask ‚Äòhow should I pay attention to all previous tokens?‚Äô by using it with the cached K and V values.\nAnd this is evident in the matrix - each row contains all combinations for that given Q. In the above example, the first row tells us about ‚Äúthe‚Äù and the second row tells us about ‚Äúcat‚Äù. Querying the current token ‚Äú(‚Äùcat‚Äù) doesn‚Äôt depend on the query values of the previous word ‚ÄúThe‚Äù. When multiplying this matrix result by V, a similar observation can be made.\nIn practice, at generation time, what this means is that we only need K‚Äôs and V‚Äôs for every token that came before to properly contextualize the current Q. Our Q matrix will actually only be made up of the Q vector for the current token.\nSo, when prompt caching, you‚Äôll see that we will store the K and V values to avoid re-computing them, but we don‚Äôt need to store Q, since the Q of the current token only is actually all that‚Äôs being used for the the next token."
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html#performance-impact",
    "href": "posts/KV-Caching/kv_caching.html#performance-impact",
    "title": "KV Caching",
    "section": "Performance Impact",
    "text": "Performance Impact\nFor a sequence of length S and generation length G:\n\nWithout Caching:\n\nFor each new token, we recompute \\(K,V\\) vectors for all previous tokens\nNeed to process entire sequence each time\nTotal Computations ‚âà \\(S \\times G \\times (S + G)\\)\n\n\n\nWith Caching:\n\nInitial processing of prompt: L¬≤ computations\nFor each new token: just one new set of computations\nTotal Computations ‚âà \\(S^2 + G\\)\n\nThe speedup becomes more dramatic as the prompt length (\\(S\\)) increases:\n\n\n\nPrompt Length\nGeneration Length\nSpeedup Factor\nExample\n\n\n\n\n10 tokens\n20 tokens\n~2x\nA short sentence\n\n\n100 tokens\n20 tokens\n~8x\nA paragraph\n\n\n1000 tokens\n20 tokens\n~40x\nA long document\n\n\n\nThis dramatic improvement occurs because: 1. Without caching, each new token requires reprocessing the entire history 2. With caching, each new token only requires computing its own \\(K,V\\) vectors 3. The longer the prompt, the more redundant computation we avoid\nFor real-world applications like chatbots or document processing where prompts can be thousands of tokens long, KV caching becomes essential for reasonable inference speed."
  },
  {
    "objectID": "posts/KV-Caching/kv_caching.html#kv-caching-in-code",
    "href": "posts/KV-Caching/kv_caching.html#kv-caching-in-code",
    "title": "KV Caching",
    "section": "KV Caching in Code",
    "text": "KV Caching in Code\nNow, let‚Äôs edit our generation function to include this KV caching.\n\nAdding Explicit Cache Management in Transformers Library\nAs we already stated above, caching mechanisms are already built-in and implemented by default in huggingface‚Äôs transformers library. However, there are also ways to have much more control over the caching, which we‚Äôll explore in this implementation. Using explicit Cache classes like DynamicCache provides several advantages:\n\n1. Cache Reusability\n\nYou can save a cache state and reuse it for multiple different generations\nUseful for generating different endings from the same prompt\nHelps avoid recomputing prompt processing multiple times\n\n\n\n2. Cache Control\n\nChoose different cache implementations (Dynamic, Static, Sliding Window)\nControl memory usage with different cache strategies\nExplicitly manage when caches are cleared or updated\n\n\n\n3. Advanced Use Cases\n\nSliding Window Attention: Limit memory usage for long sequences\nQuantized Caching: Reduce memory footprint with quantization\nCross-Attention Caching: Useful for encoder-decoder models\n\n\n\n4. Debugging and Inspection\n\nExamine cache contents directly\nMonitor memory usage\nDebug attention patterns\n\nYou can read more about different ways to implement caching in the huggingface Cache documentation.\n\ndef generate_kv_cached_completion(prompt, max_length=100):\n    \"\"\"\n    Generate completion using explicit KV caching with DynamicCache.\n    This gives us more control over cache management compared to the model's default caching.\n    \"\"\"\n    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n\n    # Initialize DynamicCache - this allows us to:\n    # 1. Explicitly manage what's cached\n    # 2. Reuse the cache across multiple generations\n    # 3. Inspect cache contents if needed\n    past_key_values = DynamicCache()\n\n    with torch.no_grad():\n        # Initial forward pass - process the prompt\n        # past_key_values here will store K,V pairs for the prompt\n        outputs = model(\n            input_ids,\n            use_cache=True,\n            past_key_values=past_key_values,  # Pass our managed cache\n            return_dict=True\n        )\n\n        generated_sequence = input_ids\n        generated_text = []\n\n        # Generate tokens one at a time\n        for _ in range(max_length):\n            # Get logits for next token prediction\n            next_token_logits = outputs.logits[:, -1, :]\n            next_token = torch.argmax(torch.softmax(next_token_logits, dim=-1)).unsqueeze(0).unsqueeze(0)\n\n            # Keep track of the sequence and generated tokens\n            generated_sequence = torch.cat([generated_sequence, next_token], dim=-1)\n            generated_text.append(next_token.item())\n\n            # Forward pass for next token, using our managed cache\n            outputs = model(\n                next_token,\n                use_cache=True,\n                past_key_values=outputs.past_key_values,\n                return_dict=True\n            )\n\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n    return tokenizer.decode(generated_text, skip_special_tokens=True)\n\nNow, let‚Äôs see how much this speeds up the generation of our story.\n\n# Using a long prompt:\nprompt = \"\"\"The last library on Earth wasn't a building - it was a person. Her name was Sarah Chen, and she was the final recipient of the Memory Archive Protocol,\na desperate procedure developed in the last days before the global web collapsed.\nThe process had encoded the contents of humanity's greatest digital archives directly into her neural pathways.\nNow, ten years after the collapse, she wandered the dusty remains of Silicon Valley, her mind a vast repository of everything from ancient\nphilosophical texts to modern scientific papers, from classic literature to social media's last posts. Each night, she transcribed a small portion of her knowledge onto carefully preserved paper,\nracing against time and her own mortality to preserve what remained of human knowledge.\nBut on this particular morning, as she wrote in her small, fortified sanctuary, Sarah realized something had changed.\nSome of the memories were starting to move on their own, rearranging themselves, evolving into something new. She was simultaneously transported into the memories\nand experiencing them in third person. She saw the words dance on the page in time with seeing what the words meant happen in front of her.\nIt was all out of order. Confusing. She tried to get a handle on what was happening. She steadied herself and focused, tried to put her attention to the here and now. But it was hard to fight it.\nShe thought about\"\"\"\n\n# Test non-cached version\ncompletion, non_cached_time = generate_completion(prompt)\nprint(f\"Completion: {completion}\")\nprint(f\"Time taken: {non_cached_time:.2f} seconds\")\n\n# Test cached version\nstart_time = time.time()\ncompletion = generate_kv_cached_completion(prompt)\nend_time = time.time()\ncached_time = end_time - start_time\n\nprint(f\"Completion: {completion}\")\nprint(f\"Time taken: {cached_time:.2f} seconds\")\n\nprint(f\"\\nSpeedup: {non_cached_time/cached_time:.2f}x\")\n\nCompletion:  the last time she had been here, ten years ago. She had been in the library, and she had been in the library, and she had been in the library. \nShe had been in the library, and she had been in the library, and she had been in the library. \nShe had been in the library, and she had been in the library, and she had been in the library. \nShe had been in the library, and she had been in the library, and\nTime taken: 5.70 seconds\nCompletion:  the last time she had been here, ten years ago. She had been in the library, and she had been in the library, and she had been in the library. \nShe had been in the library, and she had been in the library, and she had been in the library. \nShe had been in the library, and she had been in the library, and she had been in the library. \nShe had been in the library, and she had been in the library, and\nTime taken: 1.87 seconds\n\nSpeedup: 3.05x\n\n\nSo, we‚Äôve reduced our time considerably!\nOne final note: A question you might be asking is ‚ÄúWhy am I getting the same response every time, and does that have to do with storing the internal state?‚Äù\nBut no! Even though \\(K,V\\) caching is storing those values, those aren‚Äôt where the randomness is happening. It just happens that in our next token generation, we did:\nnext_token = torch.argmax(torch.softmax(next_token_logits, dim=-1)).unsqueeze(0).unsqueeze(0)\nSo, we forced the generation to pick what the model thinks is the ‚Äúbest‚Äù next token every time, making the calculation deterministic. This is useful to get the most accurate speed comparisons, but not necessary. We could have changed that line to:\nnext_token = torch.multinomial(torch.softmax(next_token_logits / temperature, dim=-1), num_samples=1).unsqueeze(0)\nWhere the temperature controls the amount of randomness, and torch.multinomial() will sample the responses instead of always choosing the maximum."
  },
  {
    "objectID": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html",
    "href": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html",
    "title": "Fine Tuning LLaVA with and without LoRA",
    "section": "",
    "text": "In this notebook, we fine tune LLaVA (Large Language and Vision Assistant), a multimodal VLM (vision-language model) to be better adapted to describing astronomical images.\n\n\n\nLLaVA is a vision-language model that combines a vision encoder (to ‚Äúsee‚Äù images) with a language model (to generate text), enabling it to answer questions about images and generate detailed descriptions. Originally trained on millions of general image-caption pairs, LLaVA can already describe everyday objects, scenes, and basic visual properties.\n\nfrom IPython.display import Image as iImage, display\ndisplay(iImage(\"images/llava_example.png\", width=800))\n\n\n\n\n\n\n\n\n\n\n\nIn this notebook, we‚Äôre going to try to fine tune LLaVA on the task of GalaxyZoo - labeled morphological properties of pictures of galaxies. But why choose LLaVA for this task, rather than, say a regular image model + a classification head? Vision-language models offer unique advantages:\n\nFlexibility: Instead of fixed categories, a vision-language model can describe novel or ambiguous features in natural language\nInterpretability: Models that describe why what they see points to a classification can be more interpretable, and we could ask follow-up questions to better understand why decisions were made\nMulti-task capability: A VLM like LLaVA is more flexible to the multiple morphological categories. It can classify spiral vs elliptical easily, and the absence/presence of more fine-grained features, and could be easily adapted later to include more if desired.\n\nBy fine-tuning on Galaxy Zoo data, we‚Äôll teach it to adapt to a new domain, recognizing and describing the astronomical features of galaxy morphologies. This can make use of all of its existing knowledge for how to look at images, but help it figure out what we want it to focus on for images of galaxies, and how to talk about them.\nLet‚Äôs get started!\n\n\n\nI‚Äôll assume this notebook is running on Google colab.\nIf so, we need to install some packages before we proceed. After the below cell runs, restart the kernel to be sure the notebook has access to these packages.\n\n!pip install -q bitsandbytes\n!pip install -q peft\n!pip install -q --upgrade ipywidgets==8.1.7\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72.9/72.9 MB 34.5 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 363.4/363.4 MB 2.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 13.8/13.8 MB 120.2 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24.6/24.6 MB 96.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 883.7/883.7 kB 56.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 664.8/664.8 MB 2.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 211.5/211.5 MB 6.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56.3/56.3 MB 41.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 127.9/127.9 MB 18.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 207.5/207.5 MB 3.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.1/21.1 MB 105.3 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 139.8/139.8 kB 6.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.2/2.2 MB 61.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.6/1.6 MB 81.7 MB/s eta 0:00:00\n\n\n\nimport requests\nimport json\nimport os\nfrom pathlib import Path\nfrom PIL import Image\nimport time\nfrom typing import Dict, List, Any\nfrom datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\nimport hashlib\nimport tqdm\nimport pandas as pd\nfrom datasets import load_dataset\nimport numpy as np\nimport random\nfrom PIL import Image\nfrom IPython.display import display\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\n\n\n\n\nLLaVA (Large Language and Vision Assistant) is a model that was introduced by Liu et al.¬†in the Vision Instruction Tuning paper in late 2023.\nIt grafts a vision encoder onto a causal-LM so the model can ‚Äúread‚Äù an image before predicting text. As such, it has multiple transformer components working in tandem:\n\n\n\n\n\n\n\n\nComponent\nWhat it is\nShape\n\n\n\n\nVision encoder\nViT-L/14 (CLIP) patch-embed layer, frozen\n3 √ó 336 √ó 336 ‚Üí 257 √ó 1024\n\n\nProjector\n2-layer MLP (GELU)\n1024 ‚Üí 4096 (or 2048) (LLaMA hidden)\n\n\nLanguage model\nLLaMA decoder-only Transformer\n4096-d (LLaMA), 2048-d (LLaMA-Tiny)\n\n\n\nBefore we get to training this model, we‚Äôll look thoroughly at its architecture and training procedure to understand what it‚Äôs capable of!\n\nNote: In this tutorial, we‚Äôll be training on LLaVA-tiny, so we‚Äôll focus on the more fine-grained details there where specified. There‚Äôs nothing special about LLaVA-tiny besides that these underlying components were chosen to be small - the same principles about how the models were stitched together and trained applies to the larger LLaVA versions as well.\n\n\n\nIn order for LLaVA to be able to ‚Äúlook‚Äù at images, it needs an encoder portion that turns an image into meaningful embeddings.\n\nFor this, LLaVA uses a Vision Transformer (ViT) model ‚Äî specifically, a large version known as ViT-L/14. This same vision encoder is used by both larger LLaVA models and LLaVA-tiny.\nViT-L/14: - Has 24 transformer blocks (the ‚ÄúL‚Äù = the ‚ÄúLarge‚Äù version). - Splits the image into patches of 14 √ó 14 pixels (the ‚Äú14‚Äù). - Processes images of 336 √ó 336 pixels, which gives you 24 √ó 24 = 576 patches, plus one extra special token (577 total tokens).\nEach of these patches is treated a bit like a ‚Äúvisual word‚Äù ‚Äî it gets turned into a 1024-dimensional vector that summarizes the visual content in that small chunk of the image.\n\n\nLet‚Äôs take a step back and look at how a ViT turns an image into a tokenized sequence.\nImages are of course made of pixels - and ViT starts with images that are 336x336 pixels. Transformer architectures fundamentally want to make use of sequences. While pixels are naturally numeric, flattening all pixels of an image, even of 336x336 (which is relatively small!) = 112896 values - too large to efficiently process, and probably not worth it in terms of the information actually contained in all individual pixel values.\nThe core innovation behind ViT is to sequence patches of the image. Small enough patches of an image can be considered to contain approximately one ‚Äúthing‚Äù and thus can be treated as a single region of focus in the image. 24 x 24 patches taken over an image that‚Äôs 336x336 = 576 values - much better.\n\ndisplay(iImage(\"images/pixels_vs_patches.png\", width=1000))\n\n\n\n\n\n\n\n\n\n\n\nThe next step is to use those patches to actually get meaningful information about the image.\nFor each patch {1, 2, ‚Ä¶. 576}:\n\nFlatten and Project\n\n\nFlatten it into a single 1D vector of pixel values\nPass that through a learnable linear projection layer, which maps that pixel sequence into a 1024-dimensional embedding that is a summary of the patch‚Äôs visual contents.\n\n\nAdd Positional Information\n\n\nEach patch has a corresponding learned positional embedding ‚Äî a fixed vector (same shape as the patch embedding) that represents its location.\nThe model adds this positional vector to the patch embedding.\n\nSo for each patch:\n[embedding] = [patch vector] + [position vector]\nThis lets the model reason about spatial layout, not just content.\n\n\nüìå Note: Why 196 ‚Üí 1024 projection?\n\nYou might be wondering why we do the projection to 1024 values from 196. We talked about how 336x336 pixels -&gt; 112896 is a ‚Äúwaste‚Äù, but 24x24 patches of size 1024 = 589824 - more information than just our pixel values!\nSo why do we do this?\nIt might seem like just an inflation of the data, but doing this projection makes the information richer and more suitable for the Transformer to use. The 1024 vectors that come out of the linear projection layer are contextualized, rich embeddings of the pixel information, and provide more meaningful information than pixel values alone.\n\nTransformers expect high-dim input (e.g., LLaMA uses 2048-d text tokens)\nRaw pixels are low-level; projection lets the model learn abstract features\nThe number of tokens works out to still just 576 (one per patch), so compute stays reasonable (we‚Äôll see this in more detail soon)\n\nSo think of this transformation like it‚Äôs converting an image patch into a dense ‚Äúvisual word.‚Äù\n\n\ndisplay(iImage(\"images/processing_patches.png\", width=800))\n\n\n\n\n\n\n\n\n\n\n\nOnce we have this rich embedding for each of our patches, it‚Äôs time for the core transformer architecture to do its thing. Each transformer layer works to model relationships between patches using self-attention.\nAt each layer:\n\nAttention mechanism\nEach patch embedding goes through fully connected layers to create Query, Key, and Value representations. Each patch ‚Äúlooks‚Äù at every other patch and decides how much attention to pay to each one.\n‚Üí This creates a big 576 √ó 576 matrix of attention scores.\n‚Üí Each patch gets updated as a weighted combination of all the others. ‚Üí The result goes through another fully connected layer to produce the final attention output.\nMulti-Layer Perceptron (MLP)\nAfter attention, each updated embedding goes through a seperate feedforward neural network. This typically expands the embedding to a larger size, applies a non-linearity, then contracts back to the original size.\nOutput\nWe now have a new set of patch embeddings ‚Äî same shape (576 √ó 1024), but now contextualized:\nEach patch now contains information not just about itself, but also about the rest of the image.\n\nIn ViT-L which has 24 layers, this process is repeated 24 times. And so, the embedding of an image coming out of ViT is size 576 x 1024 representing 576, 1024-size embeddings that describe the image.\nBelow, we take a closer look at how the attention portion of one of these layers works.\n\n\nüìå Revisiting that 1024 Dimension:\n\nOk, the point that we made above:\n‚ÄúThe number of tokens works out to still just 576 (one per patch), so compute stays reasonable‚Äù\nmight not have immediately made sense. And as we just discussed, every layer of ViT will compute over this 576 x 1024 matrix, so we‚Äôll constantly be passing around those 589824 values of information. It might have made sense why this projection to 1024 is more meaningful, but how is it also more computationally efficient than dealing with our raw pixel values (112896)?\nThis is because, thanks to the attention mechanism, transformers scale quadratically with sequence length (\\(L\\)) but only linearly with embedding dimension (\\(D\\)). This really comes down to the fact that the attention mechanism contains a dot product calculation of an \\(L \\times D\\) and \\(D \\times L\\) matrix, which requires that every row be multiplied by every column. Therefore, every increase in \\(L\\) requires \\(L^2\\) calculations.\nIn ViT, the token count that assigns one token per patch is our sequence length. So, we save tons of compute using patches to make the sequence length smaller for the transformer layers. That‚Äôs why patch-based tokenization is critical ‚Äî if we instead kept an 112896 sequence of pixels, using attention on it would be infeasible.\n\n\ndisplay(iImage(\"images/transformer_layer.png\", width=900))\n\n\n\n\n\n\n\n\n\n\n\n\nThe other half of LLaVA is of course the language understanding part. For LLaVA, the language model component is a decoder-only Transformer, whose job is to turn embeddings into words.\n\nLLaVA relies on a LLaMA model for its language component. In the case of LLaVA-tiny, it simply uses LLaMA-Tiny, a smaller version of LLaMA-7B:\n\n\n\nModel\nLayers\nHidden size\n# Params\n\n\n\n\nLLaMA-7B\n32\n4096\n6.7 B\n\n\nTiny LLaMA\n22\n2048\n~1.1 B\n\n\n\nSo the architecture is the same ‚Äî just fewer layers, making it small enough to fit and train on a modest GPU.\n\n\nThe job of a decoder-based language model is to take a sequence of tokens and turn it into language.\nThe input might be:\n&lt;image&gt; Describe this galaxy.\nThis is tokenized, both the image (we‚Äôll dicuss) and the words ‚ÄúDescribe this galaxy‚Äù by a tokenizer into a numerical sequence that the decoder layers can handle.\nThe job of the tokenizer is to create meaningful embeddings for words. Each word token in the description is embedded into a vector that contains richer embeddings than just the word itself. Tokenizers are trained specifically to do this task.\nThe embedding dimension (\\(D\\)), 2048, is fixed by the tokenizer and therefore the model. The sequence length (\\(L\\)) depends on how many things there are to tokenize - it depends on the image size (although this is fixed to 336x336), and how many words we gave in our prompt to LLaVA. In practice, LLM‚Äôs have some maximum sequence length, \\(L_{max}\\) that they can handle.\n\n\n\nA decoder transformer generates words one by one - every new token generated is conditioned on what came before.\nThe primary computation for a decoder layer is the same as for any other transformer layer, except that the attention portion is different in that masked attention is used instead. This zeros out attention between the tokens and others tokens that are later in the sequence to them.\nThe function of this is two-fold. - During training, the model can‚Äôt ‚Äúcheat‚Äù by allowing tokens to attend to ones that they shouldn‚Äôt know exist yet. - During inference, tokens in the past are fixed (already generated, or were input) - there‚Äôs no value in attending them to future tokens - you can‚Äôt do anything to change them now anyway.\n\ndisplay(iImage(\"images/decoder_layer.png\", width=900))\n\n\n\n\n\n\n\n\n\n\n\nThe text decoder of LLaVA-tiny has 22 layers, so the above process happens 22 times, and coming out of the decoder layers is an \\(L \\times 2048\\) matrix.\nThis is turned into a prediction of the next token by: - taking the embedding for the most recent (L-th) word (remember, this has now been contextualized many times by all the words before it) - it‚Äôs passed to a linear projecion layer to get a vector of length vocab_size (typically ~32k) - this is typically passed to a softmax function to produce, for all words in the vocabulary, the probability that a given word is the next token - with some temperature allowing for randomness, a token of high probability is chosen.\nWhen this is appended to the sequence, L becomes L + 1, and the whole process of 22 decoder layers starts again with the new sequence. This gradually produces the output that you see when asking LLaVA about an image!\n\n\n\n\nIt‚Äôs finally time to talk about the core ‚Äútechnology‚Äù behind a model like LLaVA - the fusion mechanism. Up until now, we have a ‚Äúregular‚Äù ViT that processes an image and a ‚Äúregular‚Äù large language model that makes text. The fusion mechanism is what connects them.\nSince LLaVA‚Äôs task is to process and understand an image to be able to create text contextualized by it, the job of the fusion mechanism is really to turn the image emedding into something that the language model understands.\nIn reality, the fusion mechanism is extremely simple ‚Äî it‚Äôs just a linear projection layer that maps the embeddings from the ViT into the same vector space as the language model‚Äôs token embeddings. This lets the image patches act like ‚Äúvisual tokens‚Äù that the language model can attend to just like words.\n\nLLaVA uses a simple fusion mechanism to do this: - The final output of the vision encoder is a sequence of 576 visual tokens (one per patch), each a 1024-dimensional vector. - These are passed through a small projection layer (usually an MLP) that maps 1024 ‚Üí 2048, matching the LLaMA embedding size. - The projected visual tokens are then prepended to the input tokens of the language model, as if they were special ‚Äúimage tokens.‚Äù\nThe LLM then attends to these visual tokens just like text, using self-attention across the combined sequence. This way, the LLM gets the visual tokens in a space that‚Äôs already meaningful to it, but also get‚Äôs told that they are visual tokens, so that it can understand how to use them during the training phase.\nThis approach might seem super simple (and it is!) but it‚Äôs also very flexible, and surprisingly effective with the right training, as we‚Äôll see.\n\ndisplay(iImage(\"images/llava_pipeline.png\", width=1000))\n\n\n\n\n\n\n\n\n\n\n\nThe final thing to discuss is how something like this gets trained. A ViT and a LLaMA already understand their respective datatypes, so training has to make sure to work with, and not against that knowledge.\nFirst, we‚Äôll talk about an important element of how LLaVA was trained that happens, even before the pipeline is trained all together that sets LLaVA up for success.\n\nViT-L/14 Was Trained With Contrastive Learning LLaVA doesn‚Äôt train the vision encoder from scratch ‚Äî it inherits a ViT model that was already trained using CLIP (Contrastive Language-Image Pretraining), created by OpenAI.\n\nCLIP was trained on 400 million (image, text) pairs scraped from the internet (web pages with associated images and alt text, captions, etc.).\nThe image goes through a ViT (like ViT-L/14).\nThe text goes through a Transformer-based text encoder. This turns text -&gt; embeddings, but importantly, is not generative.\nThen it‚Äôs trained using a contrastive loss, which aims to push the embeddings of an image and the embeddings of its corresponding caption closer in embedding space, and embeddings that don‚Äôt correspond further away.\n\nThe result: ViT-L/14 learns to produce embeddings that are already aligned with natural language text embeddings, so the 576x1024 embeddings coming out of the ViT are already ‚Äúlanguage-aligned‚Äù to some extent ‚Äî they live in the same conceptual space as captions.\n\n\nBecause ViT was trained with CLIP to understand images, and to put them in a text-friendly space, it‚Äôs frozen during training.\nThe first phase of LLaVA training teaches the fusion mechanism to align, and the language model to accept, tokens.\n\nDataset: image‚Äìcaption pairs (COCO, CC3M, etc.)\nInput: an image\nTarget Output: the corresponding caption (as tokens)\nLoss: language modeling loss (cross-entropy on the caption tokens)\n\nDuring this training phase, the weights of the fusion mechanism layer are trained (ViT output ‚Üí LLM input space), and the language model is lightly tuned to learn what to do with image tokens. &gt;\n\n‚Äúlightly tuned‚Äù meaning using a low learning rate, PEFT, or unfreezing only certain layers, depending on the LLaVA version. The idea here is to make use of what the language model already knows as best as possible.\n\nThis is the training phase where LLaVA learns to ‚Äútalk about‚Äù images at all.\n\n\n\nThe first phase of training teaches LLaVA to write a caption for a corresponding image. But LLaVA is also able to accept prompt instructions, i.e.¬†you can include with your image ‚ÄúDescribe this image‚Äù or ‚ÄúWhat‚Äôs interesting about this picture?‚Äù\nFor this to work, the language part of LLaVA needs to be instruction fine-tuned. We won‚Äôt get into the details here since it‚Äôs a bit outside of the scope of how LLaVA works specifically, but this phase of training is just meant to align the language part of the model - both ViT and the fusion mechanism are frozen while the language model learns to follow instructions.\n\nAnd that‚Äôs it!! Hopefully, you should now have a concrete understanding of how information flows through a model like LLaVA, and how we train a model to this sort of multi-modal alignment!\n\n\n\n\n\nIn this notebook, we‚Äôll be fine tuning our own LLaVA to understand astronomical images. Since LLaVA is trained on image and image caption pairs, we need a similar data structure to train it with.\nHigh quality image captions that contain real astronomical descriptions are ideal. For the purposes of this tutorial, we‚Äôll be gathering data from Galaxy Zoo 2 - a set of galaxy images and their morphological classifications. We‚Äôll build the morphological classifications into captions as best as we can. While not totally ideal for real, quality fine tuning, it should be sufficient to see some basic changes in the model.\n\n\n\n\nSource Hugging Face dataset mwalmsley/gz2 (172 k SDSS galaxy JPGs + volunteer morphology votes).\n\nImages JPEG cut-outs around each galaxy.\n\nLabels converted to text\n\nElliptical / Spiral (+ bar, arm count, bulge, merger/odd)\n\nAxis-ratio for ellipticals (‚Äúround‚Äù, ‚Äúelongated‚Äù)\n\n\nCaption length ‚âà 5-20 words typically.\n\n\n\n\nThe below code:\n\ndownloads N examples (set by N),\n\nrescales to 336 √ó 336,\n\nbuilds captions from the columns of galaxy morphological descriptions\ncategorizes galaxies by spiral vs elliptical to create a balanced dataset\nbuilds captions using a balanced dataset\nwrites gz2_llava.jsonl for LLaVA training.\n\n\n\nLLaVA expects training data in a conversation format that mimics how humans discuss images. Each training example contains:\n\nHuman question/prompt about an image\nLLM model response with detailed explanation\nImage reference linking to the visual content\n\nIn the below code, you can set N - the number of images to be downloaded per categorization.\n\n# ------------------------------------------------------------------ paths\nroot = Path(\"gz2_llava_hf\")\nimgs = root / \"images\"\nroot.mkdir(exist_ok=True)\nimgs.mkdir(exist_ok=True)\n\n# ------------------------------------------------------------------ load full dataset first\nprint(\"Loading dataset...\")\nds = load_dataset(\"mwalmsley/gz2\", split=\"train\")\nN = 1000 # Max N per class\n\n# ------------------------------------------------------------------ helper for captions\ndef describe(r):\n    out = []\n    # ============ turn labels into words for captions =======================\n    smooth = r[\"smooth-or-featured-gz2_smooth_fraction\"] &gt; 0.6\n    if smooth:\n        out.append(\"This is an image of an elliptical galaxy\")\n        # axis ratio\n        if r[\"how-rounded-gz2_round_fraction\"] &gt; .5:\n            out.append(\"nearly round in shape\")\n        elif r[\"how-rounded-gz2_in-between_fraction\"] &gt; .5:\n            out.append(\"moderately elongated in shape\")\n        elif r[\"how-rounded-gz2_cigar_fraction\"] &gt; .5:\n            out.append(\"highly elongated in shape\")\n    else:\n        out.append(\"This is an image of a spiral galaxy\")\n        if r[\"bar-gz2_yes_fraction\"] &gt; .5:\n            out.append(\"with a central bar\")\n        if r[\"spiral-arm-count-gz2_2_fraction\"] &gt; .5:\n            out.append(\"with two arms\")\n        elif r[\"spiral-arm-count-gz2_3_fraction\"] &gt; .5:\n            out.append(\"with three arms\")\n        elif r[\"spiral-arm-count-gz2_more-than-4_fraction\"] &gt; .5:\n            out.append(\"with many arms\")\n    # bulge prominence / shape\n    if r[\"bulge-size-gz2_obvious_fraction\"] &gt; .5:\n        out.append(\"prominent bulge at its center\")\n    if r[\"bulge-shape-gz2_boxy_fraction\"] &gt; .5:\n        out.append(\"boxy bulge at its center\")\n    # mergers / oddities\n    if r[\"something-odd-gz2_yes_fraction\"] &gt; .4:\n        out.append(\"disturbed or merging with another galaxy\")\n    return \", \".join(out) + \".\"\n\n# ------------------------------------------------------------------ categorize by type\nprint(\"Categorizing galaxies...\")\nspirals = []\nellipticals = []\n\nfor i, ex in enumerate(tqdm.tqdm(ds, desc=\"Categorizing\")):\n    smooth = ex[\"smooth-or-featured-gz2_smooth_fraction\"] &gt; 0.6\n    if smooth:\n        ellipticals.append(i)\n    else:\n        spirals.append(i)\n\nprint(f\"Found {len(spirals)} spirals and {len(ellipticals)} ellipticals\")\n\n# ------------------------------------------------------------------ balance dataset\n# Take equal numbers of each\nn_per_class = min(len(spirals), len(ellipticals), N)  # Max N per class\nprint(f\"Selecting {n_per_class} of each type (total: {n_per_class * 2})...\")\n\n# Random sample from each\nrandom.seed(42)\nselected_spirals = random.sample(spirals, n_per_class)\nselected_ellipticals = random.sample(ellipticals, n_per_class)\n\n# Combine and shuffle\nselected_indices = selected_spirals + selected_ellipticals\nrandom.shuffle(selected_indices)\n\n# ------------------------------------------------------------------ build records into captions\nrecords = []\ncaption_lengths = {\"spiral\": [], \"elliptical\": []}\n\nfor i in tqdm.tqdm(selected_indices, desc=\"Processing\"):\n    ex = ds[i]\n    img = ex[\"image\"].convert(\"RGB\").resize((336, 336))\n    fname = imgs / f\"{hashlib.md5(ex['id_str'].encode()).hexdigest()}.jpg\"\n    img.save(fname, \"JPEG\", quality=85)\n\n    caption = describe(pd.Series(ex))\n    galaxy_type = \"elliptical\" if ex[\"smooth-or-featured-gz2_smooth_fraction\"] &gt; 0.6 else \"spiral\"\n\n    records.append({\"image\": str(fname), \"text\": caption})\n    caption_lengths[galaxy_type].append(len(caption.split(\",\")))  # Count features\n\n# ------------------------------------------------------------------ statistics to make sure it's balanced enough\nprint(\"\\n=== Dataset Statistics ===\")\nprint(f\"Total examples: {len(records)}\")\nprint(f\"Spirals: {n_per_class} ({n_per_class/len(records)*100:.1f}%)\")\nprint(f\"Ellipticals: {n_per_class} ({n_per_class/len(records)*100:.1f}%)\")\n\nprint(\"\\nAverage features per type:\")\nprint(f\"Spirals: {sum(caption_lengths['spiral'])/len(caption_lengths['spiral']):.1f} features\")\nprint(f\"Ellipticals: {sum(caption_lengths['elliptical'])/len(caption_lengths['elliptical']):.1f} features\")\n\n# Show some examples of varying lengths\nprint(\"\\nExample captions by length:\")\nsorted_records = sorted(records, key=lambda x: len(x['text']))\nprint(f\"Shortest: {sorted_records[0]['text']}\")\nprint(f\"Median: {sorted_records[len(sorted_records)//2]['text']}\")\nprint(f\"Longest: {sorted_records[-1]['text']}\")\n\n# ------------------------------------------------------------------ dump jsonl\nwith open(root / \"gz2_llava.jsonl\", \"w\") as f:\n    for r in records:\n        f.write(json.dumps(r) + \"\\n\")\n\nprint(f\"\\n‚úì Wrote {len(records)} balanced examples ‚Üí {root/'gz2_llava.jsonl'}\")\n\nLoading dataset...\n\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorizing galaxies...\n\n\nCategorizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172377/172377 [04:16&lt;00:00, 672.34it/s]\n\n\nFound 56141 spirals and 116236 ellipticals\nSelecting 1000 of each type (total: 2000)...\n\n\nProcessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:11&lt;00:00, 166.74it/s]\n\n\n\n=== Dataset Statistics ===\nTotal examples: 2000\nSpirals: 1000 (50.0%)\nEllipticals: 1000 (50.0%)\n\nAverage features per type:\nSpirals: 2.1 features\nEllipticals: 2.5 features\n\nExample captions by length:\nShortest: This is an image of a spiral galaxy.\nMedian: This is an image of an elliptical galaxy, moderately elongated in shape.\nLongest: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, boxy bulge at its center, disturbed or merging with another galaxy.\n\n‚úì Wrote 2000 balanced examples ‚Üí gz2_llava_hf/gz2_llava.jsonl\n\n\n\n\n\nAfter running this data collection, you should have:\ngz2_llava_hf/\n‚îú‚îÄ‚îÄ gz2_llava.jsonl.json          # Dataset with all metadata\n‚îú‚îÄ‚îÄ images/                      # Downloaded astronomical images\n‚îÇ   ‚îú‚îÄ‚îÄ 00c1591a613183ff21a67f79a29b5940.jpg\n‚îÇ   ‚îú‚îÄ‚îÄ 00fd8380b58f4c5086f655e646e0d5a0.jpg\n‚îÇ   ‚îî‚îÄ‚îÄ ...\nEach entry in gz2_llava.jsonl contains: - image: Path to the astronomical image - text: description of galaxy morphology\n\n\n\n\nNow, let‚Äôs look a at a random example in our newly-downloaded dataset.\nThis cell can be re-run to inspect a new random image.\n\nimport json, random\nfrom PIL import Image\nfrom IPython.display import display\n\n# path to the JSON-Lines file you just wrote\nJSONL = \"gz2_llava_hf/gz2_llava.jsonl\"      # adjust if you used a different folder\n\n# grab one random record (or change to lines[0] for the first)\nwith open(JSONL) as f:\n    rec = json.loads(random.choice(f.readlines()))\n\nprint(rec[\"text\"])          # caption\ndisplay(Image.open(rec[\"image\"]))  # shows the image in a notebook\n\nThis is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center.\n\n\n\n\n\n\n\n\n\n\n\n\nThe final step is to make sure we have a dataset class set up for use with the model.\nWe need to convert our conversation data into PyTorch‚Äôs training format. This cell creates a Dataset class that loads images, processes text, and tokenizes everything for LLaVA training.\nOne crucial step that happens here is that we mask out the image tokens so they are not trained on. If we don‚Äôt do this step, the full ‚Äúoutput‚Äù that the model will check for correctness on will include the image tokens, which makes no sense!\n\nclass GZ2LLaVADataset(Dataset):\n    def __init__(self, jsonl_path, processor):\n        with open(jsonl_path) as f:\n            self.data = [json.loads(l) for l in f]\n        self.proc = processor\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        img = Image.open(item[\"image\"]).convert(\"RGB\")\n\n        # Build the full text\n        prompt = \"USER: &lt;image&gt;\\nDescribe this galaxy.\\nASSISTANT: \"\n        full_text = prompt + item[\"text\"]\n\n        # Process image and text together\n        enc = self.proc(text=full_text, images=img, return_tensors=\"pt\")\n\n        input_ids = enc[\"input_ids\"].squeeze()\n        attention_mask = enc[\"attention_mask\"].squeeze()\n        pixel_values = enc[\"pixel_values\"].squeeze()\n\n        # CRITICAL: Find where the actual caption starts\n        # We need to mask everything BEFORE the caption\n        labels = input_ids.clone()\n\n        # Tokenize just the prompt to find its length AFTER image expansion\n        prompt_enc = self.proc(text=prompt, images=img, return_tensors=\"pt\")\n        prompt_length = prompt_enc[\"input_ids\"].shape[1]\n\n        # Mask everything up to the caption\n        labels[:prompt_length] = -100\n\n        # Debug to verify\n        if idx &lt; 3:\n            # Count what we're actually learning\n            unmasked = (labels != -100).sum()\n            print(f\"\\nExample {idx}:\")\n            print(f\"  Total tokens: {len(input_ids)}\")\n            print(f\"  Caption tokens to learn: {unmasked}\")\n            print(f\"  Caption text: {item['text'][:50]}...\")\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"pixel_values\": pixel_values,\n            \"labels\": labels,\n        }\n\n\n\n\nThis cell loads the TinyLLaVA model and creates helper functions for testing. We‚Äôll use these functions to compare the model‚Äôs responses before and after training.\nWe take tiny LLaVA from this HuggingFace repository.\n\n\n\n\n\n\nWeights & config are pulled from ü§ó Hub repo bczhou/tiny-llava-v1-hf.\n\nIt combines:\n\nVision: CLIP ViT-L/14 (~300M parameters)\nProjection: 2-layer MLP (~8M parameters)\n\nLanguage: TinyLlama-1.1B-Chat (~1.1B parameters)\nTotal: ~1.4B parameters ‚Üí ‚âà 2.8 GB in fp16, 5.6 GB in fp32\n\n\ndevice_map=\"auto\" = ü§ó Accelerate loads each layer on the first GPU/CPU with room.\ntorch_dtype=torch.float32 tells HF to up-cast fp16 weights to fp32 when opening‚Äîslower, but avoids NaN/overflow on consumer GPUs.\n\nWe also below will make a function that helps us test our model. We can set certain generation parameters, and do specify some:\n\ndo_sample=False: Deterministic (greedy) decoding - always picks the most likely token\nnum_beams=3: Beam search explores multiple paths to find better sequences\nrepetition_penalty=1.2: Discourages repeating phrases (common in small models)\nno_repeat_ngram_size=2: Prevents repeating 2-word phrases exactly\nmin_length=10: Forces at least 10 tokens - prevents immediate EOS generation\nmax_new_tokens=30: Limits response length for quick testing (and we don‚Äôt need much more anyway)\n\n\ndef setup_tiny_llava():\n    \"\"\"One function to set everything up\"\"\"\n    print(\"Setting up TinyLLaVA...\")\n\n    # Load model and processor\n    model_id = \"bczhou/tiny-llava-v1-hf\"\n\n    model = LlavaForConditionalGeneration.from_pretrained(\n        model_id,\n        torch_dtype=torch.float32,  # Use FP32 instead of FP16\n        device_map=\"auto\"\n    )\n\n    processor = AutoProcessor.from_pretrained(model_id)\n\n    # Fix patch_size issue - only a tinyllava thing\n    if processor.patch_size is None:\n        processor.patch_size = 14\n        print(\"‚úì Fixed patch_size\")\n\n    print(\"‚úì Model and processor ready\")\n    return model, processor\n\ndef test_model(model, processor, test_image_path):\n    \"\"\"Simple test function\"\"\"\n    image = Image.open(test_image_path).convert('RGB').resize((336, 336))\n\n    inputs = processor(\n        text=\"USER: &lt;image&gt;\\nWhat is this? ASSISTANT:\",\n        images=image,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=30,\n            eos_token_id=processor.tokenizer.eos_token_id,\n            pad_token_id=processor.tokenizer.pad_token_id,\n            do_sample=False,              # greedy\n            repetition_penalty=1.2,       # avoid loops\n            no_repeat_ngram_size=2,\n            num_beams=3,                   # optional beam search\n            min_length=10,  # Force at least 10 tokens\n        )\n\n    response = processor.decode(outputs[0], skip_special_tokens=True)\n    return response.split(\"ASSISTANT:\")[-1].strip()\n\nLet‚Äôs quickly take a look at the architecture of TinyLLaVA and make sure that it matches what we expect.\n\nmodel, processor = setup_tiny_llava()\nprint(model)\n\nSetting up TinyLLaVA...\n\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\n\n\n\n\n\n\n\n\n‚úì Fixed patch_size\n‚úì Model and processor ready\nLlavaForConditionalGeneration(\n  (model): LlavaModel(\n    (vision_tower): CLIPVisionModel(\n      (vision_model): CLIPVisionTransformer(\n        (embeddings): CLIPVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n          (position_embedding): Embedding(577, 1024)\n        )\n        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-23): 24 x CLIPEncoderLayer(\n              (self_attn): CLIPAttention(\n                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()\n                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n              )\n              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (multi_modal_projector): LlavaMultiModalProjector(\n      (linear_1): Linear(in_features=1024, out_features=2048, bias=True)\n      (act): GELUActivation()\n      (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n    )\n    (language_model): LlamaModel(\n      (embed_tokens): Embedding(32064, 2048)\n      (layers): ModuleList(\n        (0-21): 22 x LlamaDecoderLayer(\n          (self_attn): LlamaAttention(\n            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        )\n      )\n      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n  )\n  (lm_head): Linear(in_features=2048, out_features=32064, bias=False)\n)\n\n\n\n\n\nNow, we are ready to set up the loop to perform full fine tuning!\nHuggingface makes this super easy by just defining a trainer. But first, we‚Äôll need to define the training arguments. This sets up exactly how the training will proceed, and it‚Äôs where we can set hyperparameters, determine what outputs we want to see, where the model will save, all of that!\n\n# Training configuration - full FP32 to avoid all gradient issues\ntraining_args = TrainingArguments(\n    output_dir=\"./tiny-llava-trained\",         # Where to save model checkpoints\n    num_train_epochs=3,                        # How many times to go through the dataset\n    per_device_train_batch_size=1,             # Process 1 example at a time (small for memory)\n    learning_rate=1e-5,                        # How fast the model learns (small = careful)\n    logging_steps=1000,                        # Print progress every N training steps\n    save_strategy=\"no\",                        # Don't save checkpoints (workshop demo only)\n    eval_strategy=\"no\",                        # Don't evaluate during training (keep simple)\n    remove_unused_columns=False,               # Keep all data columns for LLaVA\n    fp16=False,                                # Disable half-precision (avoids gradient issues)\n    bf16=False,                                # Disable bfloat16 (avoids gradient issues)\n    report_to=\"none\",                          # Don't send metrics to tracking services\n    dataloader_num_workers=0,                  # Use main thread only (avoids multiprocessing issues)\n    dataloader_pin_memory=False                # Disable memory pinning (avoids GPU memory issues)\n)\n\n\n\n\nThen, we‚Äôre finally ready to set it training.\nBelow, we‚Äôll set the model and processor up, create the dataset from our formatted JSON, and start training.\nThis cell will also show us what the model‚Äôs response to a given image looked like both before and after training, so we can see if it got any better.\n\nOne critical thing that we do below is freeze the vision model weights.\nThe vision encoder is already excellent at extracting visual features, and training it on our small galaxy dataset would likely only make it worse at general vision tasks. We only need to teach the model how to describe galaxies, not how to see them differently.\n\nprint(\"üöÄ Starting TinyLLaVA full fine tuning....\")\nprint(\"=\" * 30)\n\n# 1. Setup using our setup function\nmodel, processor = setup_tiny_llava()\n\n# 2. freeze the vision‚Äêtower weights\nfor name, param in model.named_parameters():\n    if \"vision_tower\" in name:\n        param.requires_grad = False\n\n# 3. create the dataset from our LLaVA-formatted JSON\ndataset = GZ2LLaVADataset(\"gz2_llava_hf/gz2_llava.jsonl\", processor)\n\nval_size = int(0.1 * len(dataset)) # set validation size - we'll just do 10%\ntrain_size = len(dataset) - val_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# ------Test before training -----\nprint(\"\\nTesting BEFORE training:\")\nsample_item = dataset.data[0]\ntest_image = f\"{sample_item['image']}\"\nbefore_response = test_model(model, processor, test_image)\n\nplt.figure(figsize=(8, 6))\nimg = Image.open(test_image)\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nprint(f\"BEFORE: {before_response}\")\n# -------------------------------\n\n# 4. Set up the trainer with our args\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    processing_class=processor\n)\n\n# 5. set the training going\nprint(\"\\nStarting training...\")\nstart_time = time.time()\ntrainer.train()\nfull_training_time = time.time() - start_time\nprint(f\"‚úì Training completed in {full_training_time:.1f} seconds ({full_training_time/60:.1f} minutes)\")\n\n#  ----- Test after training ------\nprint(\"\\nTesting AFTER training:\")\nafter_response = test_model(model, processor, test_image)\nprint(f\"AFTER: {after_response}\")\nprint(f\"TRUE: {sample_item['text']}\")\n# -------------------------------\n\nüöÄ Starting TinyLLaVA full fine tuning....\n==============================\nSetting up TinyLLaVA...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\n\n\n\n\n\n\n\n\n‚úì Fixed patch_size\n‚úì Model and processor ready\n\nTesting BEFORE training:\n\n\n\n\n\n\n\n\n\nBEFORE: The image is a close-up view of a star in the middle of the night sky. The star appears to be glowing brightly,\n\nStarting training...\n\n\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n\n\n\n    \n      \n      \n      [5400/5400 36:55, Epoch 3/3]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n1000\n0.195500\n\n\n2000\n0.138800\n\n\n3000\n0.121600\n\n\n4000\n0.103600\n\n\n5000\n0.091000\n\n\n\n\n\n\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n‚úì Training completed in 2216.3 seconds (36.9 minutes)\n\nTesting AFTER training:\nAFTER: an image of an elliptical galaxy, nearly round in shape. prominent bulge at its center. disturbed or merging with another galax\nTRUE: This is an image of an elliptical galaxy, nearly round in shape, prominent bulge at its center.\n\n\nLet‚Äôs see how it did on a random image (you can re-run this cell to generate a new image)\n\nprint('-------------FROM TRAINING-------------------')\n# Get a random index from the training subset\ntrain_idx = np.random.randint(len(train_dataset))\nactual_idx = train_dataset.indices[train_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig1 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\nprint('\\n------------FROM VALIDATION------------------')\n# Get a random index from the validation subset\nval_idx = np.random.randint(len(val_dataset))\nactual_idx = val_dataset.indices[val_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig2 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\n-------------FROM TRAINING-------------------\nTRUTH: This is an image of a spiral galaxy, prominent bulge at its center.\nPREDICTED: an image of a spiral galaxy, prominent bulge at its center. disturbed or merging with another Galy. prominentbulgeat\n\n------------FROM VALIDATION------------------\nTRUTH: This is an image of an elliptical galaxy, nearly round in shape.\nPREDICTED: an image of an elliptical galaxy, nearly round in shape. boxy bulge at its center. disturbed or merging with another\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we‚Äôll use LoRA (Low-Rank Adaptation) to fine-tune TinyLLaVA more efficiently. LoRA lets us train only a small number of parameters while keeping the base model frozen.\nBut before we do that, let‚Äôs take another look at our model and talk about where exactly LoRA could be applied:\nLlavaForConditionalGeneration(\n  (model): LlavaModel(\n    (vision_tower): CLIPVisionModel(\n      (vision_model): CLIPVisionTransformer(\n        (embeddings): CLIPVisionEmbeddings(\n          (patch_embedding): Conv2d(...)  # ‚úÖ COULD use LoRA (but typically don't)\n          (position_embedding): Embedding(...)  \n        )\n        (pre_layrnorm): LayerNorm(...)  \n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-23): 24 x CLIPEncoderLayer(\n              (self_attn): CLIPAttention(\n                (k_proj): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (v_proj): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (q_proj): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (out_proj): Linear(...) # ‚úÖ COULD use LoRA (but we freeze vision)\n              )\n              (layer_norm1): LayerNorm(...)   \n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()  \n                (fc1): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (fc2): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n              )\n              (layer_norm2): LayerNorm(...)  \n            )\n          )\n        )\n        (post_layernorm): LayerNorm(...)   \n      )\n    )\n    (multi_modal_projector): LlavaMultiModalProjector(\n      (linear_1): Linear(...)  # ‚≠ê WE USE LoRA HERE (mm_projector)\n      (act): GELUActivation()  #\n      (linear_2): Linear(...)  # ‚≠ê WE USE LoRA HERE (mm_projector)\n    )\n    (language_model): LlamaModel(\n      (embed_tokens): Embedding(...)  #\n      (layers): ModuleList(\n        (0-21): 22 x LlamaDecoderLayer(\n          (self_attn): LlamaAttention(\n            (q_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (k_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (v_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (o_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (up_proj): Linear(...)    # ‚≠ê WE USE LoRA HERE\n            (down_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (act_fn): SiLU()  #\n          )\n          (input_layernorm): LlamaRMSNorm(...)  #\n          (post_attention_layernorm): LlamaRMSNorm(...)  #\n        )\n      )\n      (norm): LlamaRMSNorm(...)  #\n      (rotary_emb): LlamaRotaryEmbedding()  #\n    )\n  )\n  (lm_head): Linear(...)  # ‚úÖ COULD use LoRA (but typically don't)\n)\n\n\nHere, we set the LoRA config, which will tell the peft library exactly how we want lora applied to our model.\nSome key configuration choices made below: - r=8, lora_alpha=16: We keep rank fairly small, because LLaVA tiny is quite a small model. - lora_dropout=0.05: small dropout prevents overfitting on our limited galaxy descriptions - target_modules: We apply LoRA adaptations to 3 different key parts of the model: - Attention layers (q_proj, k_proj, v_proj, o_proj): Help the model attend to relevant image regions - FFN layers (gate_proj, up_proj, down_proj): Transform features for galaxy-specific outputs - Vision-language projector (mm_projector): The crucial bridge between image and text is actually the most important for our task!\nIncluding all these modules ensures the model can both process visual features AND generate appropriate text. As you‚Äôll see below, with low rank, this is still a tiny fraction of our model size.\n\nlora_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", # Attention layers\n        \"gate_proj\", \"up_proj\", \"down_proj\",  # FFN layers\n        \"mm_projector\"] # fusion layer\n)\n\n\n\n\nWe use slightly different training arguments for LoRA. Notably, we‚Äôre able to increase our batch size to 8, and our learning rate is a bit higher. Because our batch size is larger, we‚Äôll also change logging_steps, as the number of steps is a function of how much data is processed at once.\n\n# Training configuration - full FP32 to avoid all gradient issues\ntraining_args = TrainingArguments(\n    output_dir=\"./tiny-llava-lora-trained\",    # Where to save model checkpoints\n    num_train_epochs=3,                        # How many times to go through the dataset\n    per_device_train_batch_size=8,             # Can use larger batch size now, less memory needs\n    learning_rate=5e-5,                        # A bit higher than full fine tuning\n    logging_steps=200,                         # Print progress every N training steps\n    save_strategy=\"no\",                        # Don't save checkpoints (workshop demo only)\n    eval_strategy=\"no\",                        # Don't evaluate during training (keep simple)\n    remove_unused_columns=False,               # Keep all data columns for LLaVA\n    fp16=False,                                # Disable half-precision (avoids gradient issues)\n    bf16=False,                                # Disable bfloat16 (avoids gradient issues)\n    report_to=\"none\",                          # Don't send metrics to tracking services\n    dataloader_num_workers=0,                  # Use main thread only (avoids multiprocessing issues)\n    dataloader_pin_memory=False                # Disable memory pinning (avoids GPU memory issues)\n)\n\n\n\n\nThanks to LoRA, we‚Äôll save enough memory that we can batch our data instead of giving one example at a time. But vision-language models like LLaVA require special handling when batching data because the default PyTorch collator doesn‚Äôt know how to handle our mixed inputs (images + text of varying lengths).\nSo, if we want to turn our batch size up to 8, we need to define a collator that says how to do that. Below, our custom collate_fn handles how to build batches from our mixed image/text, variable length data.\n\ndef collate_fn(features):\n    # features is a list of dicts, each with keys:\n    #   'input_ids', 'attention_mask', 'pixel_values', 'labels'\n\n    # 1) stack pixel_values (all same shape: 3√ó336√ó336)\n    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n\n    # 2) gather input_ids & attention_mask for text, pad them to same length\n    batch_text = {\n        \"input_ids\":      [f[\"input_ids\"] for f in features],\n        \"attention_mask\": [f[\"attention_mask\"] for f in features],\n    }\n    batch_text = processor.tokenizer.pad(\n        batch_text,\n        padding=\"longest\",\n        return_tensors=\"pt\"\n    )\n\n    # 3) labels: pad/truncate to same as input_ids\n    #    if you already have f[\"labels\"], you can pad those directly:\n    batch_labels = processor.tokenizer.pad(\n        {\"input_ids\": [f[\"labels\"] for f in features]},\n        padding=\"longest\",\n        return_tensors=\"pt\"\n    )[\"input_ids\"]\n\n    # 4) assemble final batch\n    batch = {\n        \"pixel_values\": pixel_values,\n        \"input_ids\":    batch_text[\"input_ids\"],\n        \"attention_mask\": batch_text[\"attention_mask\"],\n        \"labels\":       batch_labels,\n    }\n    return batch\n\n\n\n\nWe re-use our model setup and a lot of the code above to do our training.\nA few things to note - We print out trainable_parameters, this tells us what percentage of the model is actually going to be trained this time. - We still freeze the vision tower to ensure that none of it‚Äôs layers that matched our target_modules are actually trained, for the same reasons we discussed above. - As before, we‚Äôll see one example before/after training.\n\n# üöÄ TinyLLaVA + LoRA fine-tuning on Galaxy Zoo 2\n# ------------------------------------------------\nprint(\"üöÄ Starting TinyLLaVA LoRA fine-tuning‚Ä¶\")\nprint(\"=\" * 30)\n\nfrom peft import prepare_model_for_kbit_training\n\n# 1. base weights + processor\nmodel, processor = setup_tiny_llava()   # our existing helper\n\n# 2. attach LoRA adapters (only proj/FFN layers train)\nmodel = get_peft_model(model, lora_cfg)\n\n# 3. freeze **only** the vision‚Äêtower LoRA weights\nfor name, param in model.named_parameters():\n    if \"vision_tower\" in name and \"lora_\" in name:\n        param.requires_grad = False\n\nmodel.print_trainable_parameters()\n\n# 4. make dataset\ndataset = GZ2LLaVADataset(\"gz2_llava_hf/gz2_llava.jsonl\", processor)\n\nval_size = int(0.1 * len(dataset)) # set validation size - we'll just do 10%\ntrain_size = len(dataset) - val_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\nprint(f\"Split: {len(train_dataset)} train, {len(val_dataset)} validation\")\n\n# ------Test before training -----\nprint(\"\\nTesting BEFORE training:\")\nsample_item = dataset.data[0]\ntest_image = f\"{sample_item['image']}\"\nbefore_response = test_model(model, processor, test_image)\n\nplt.figure(figsize=(8, 6))\nimg = Image.open(test_image)\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nprint(f\"BEFORE: {before_response}\")\n# -------------------------------\n\n# 5. trainer (reduce LR for LoRA)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer = processor.tokenizer, # Trainer expects ‚Äútokenizer‚Äù\n    data_collator=collate_fn,\n)\n\nprint(\"\\nStarting training...\")\nstart_time = time.time()\ntrainer.train()\nlora_training_time = time.time() - start_time\nprint(f\"‚úì Training completed in {lora_training_time:.1f} seconds ({lora_training_time/60:.1f} minutes)\")\n\n# 6. test after\nprint(\"\\nTesting AFTER training:\")\nprint(\"AFTER:\", test_model(model, processor, test_image))\n\n# 7. save lightweight adapters (~40 MB)\nmodel.save_pretrained(\"tinyllava_gz2_lora\")\nprint(\"Adapters saved to tinyllava_gz2_lora/\")\n\nüöÄ Starting TinyLLaVA LoRA fine-tuning‚Ä¶\n==============================\nSetting up TinyLLaVA...\n\n\n\n\n\n‚úì Fixed patch_size\n‚úì Model and processor ready\ntrainable params: 12,615,680 || all params: 1,425,088,512 || trainable%: 0.8853\nSplit: 1800 train, 200 validation\n\nTesting BEFORE training:\n\n\n\n\n\n\n\n\n\n/tmp/ipython-input-1885621431.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n\n\nBEFORE: The image is a close-up view of a star in the middle of the night sky. The star appears to be glowing brightly,\n\nStarting training...\n\n\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [675/675 20:28, Epoch 3/3]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n200\n1.861200\n\n\n400\n1.457400\n\n\n600\n1.410500\n\n\n\n\n\n\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n‚úì Training completed in 1231.3 seconds (20.5 minutes)\n\nTesting AFTER training:\nAFTER: This is an image of an elliptical galaxy, nearly round in shape, prominent bulge at its center, disturbed or merging with\nAdapters saved to tinyllava_gz2_lora/\n\n\n\n# (OPTIONAL), a trainer can always be trained further by running this again!\n#trainer.train()\n\nAgain, re-run this cell to see performance on a random example.\n\nprint('-------------FROM TRAINING-------------------')\n# Get a random index from the training subset\ntrain_idx = np.random.randint(len(train_dataset))\nactual_idx = train_dataset.indices[train_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig1 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\nprint('\\n------------FROM VALIDATION------------------')\n# Get a random index from the validation subset\nval_idx = np.random.randint(len(val_dataset))\nactual_idx = val_dataset.indices[val_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig2 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\n-------------FROM TRAINING-------------------\nTRUTH: This is an image of a spiral galaxy, prominent bulge at its center.\nPREDICTED: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, disturbed or\n\n------------FROM VALIDATION------------------\nTRUTH: This is an image of an elliptical galaxy, nearly round in shape.\nPREDICTED: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, boxy bul\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"\\n‚è±Ô∏è  Training Time Comparison:\")\nprint(f\"   Full fine-tuning: {full_training_time:.1f}s\")\nprint(f\"   LoRA fine-tuning: {lora_training_time:.1f}s\")\nprint(f\"   Speedup: {full_training_time/lora_training_time:.1f}x faster with LoRA\")\n\n\n‚è±Ô∏è  Training Time Comparison:\n   Full fine-tuning: 2216.3s\n   LoRA fine-tuning: 1231.3s\n   Speedup: 1.8x faster with LoRA\n\n\nOk. You might be thinking - ‚Äú2x speedup? Who cares!‚Äù because in this notebook, using LoRA only got us an about that. But remember that we‚Äôre working here with a tiny model, and a tiny amount of data. The real benefits of LoRA become apparent when:\n\nWorking with larger models (7B, 13B parameters) where it can be 10x+ faster\nDeploying on limited hardware where memory is constrained (remeber, here, we saw a batch size of 1 on an A100 with full fine tuning, so our ‚Äúbeefy‚Äù GPU was already barely able to handle this task).\nIterating quickly on experiments - even 20 minutes vs 40 means is huge when running a model over and over again to make tweaks.\n\n\n\n\n\nAnd that‚Äôs it! We‚Äôve sucessfully trained a TinyLLaVA both with full fine tuning and using LoRA. And we definitely see learning, in both cases!\n\n\nFine-tuning a specialized model is never simply ‚Äúthrow data at the model and hope‚Äù.\n\nUnderstanding Data Flow is Critical.\n\n\nWe trace exactly how our images become tokens (336√ó336 ‚Üí 576 patches ‚Üí 576 tokens) to understand how information flows\nMasking required understanding where image tokens end and text begins\nWithout this knowledge, we‚Äôd train on the wrong tokens and get garbage results\n\n\nArchitecture Knowledge Matters.\n\n\nWe froze the vision encoder because it already understands images well\nWe trained the projection layer because that‚Äôs where image‚Üílanguage translation happens\nChoosing LoRA target modules required knowing which layers affect generation quality\n\n\nData Structure Shapes Everything. We had to carefully structure our data:\n\n\"USER: &lt;image&gt;\\nDescribe this galaxy.\\nASSISTANT: [actual description]\"\n          ‚Üë                                           ‚Üë\n    Image goes here                          Only train on this part\nThe model‚Äôs behavior completely depends on these formatting decisions.\n\nDebugging Requires Deep Understanding. It‚Äôs really common to get nonsense in your first stab at training a model. Debugging to get good results is often not ‚ÄúI have an error‚Äù, but rather ‚Äúthis doesn‚Äôt work as well as I thought‚Äù - and that sort of debugging benefits from a human overseeing the process.\n\n\n\n\n1. The Power of Shared Representation Spaces\nWhat we‚Äôve done with LLaVA reveals a profound principle: different types of information can be projected into a shared ‚Äúthinking space.‚Äù\nThis simple projection is surprisingly deep:\nimage_features (1024d) ‚Üí projection layer ‚Üí language space (2048d)\nWhile it‚Äôs simple in structure, it works because both vision and language models learned rich, compositional representations, so the projection layer just needs to learn to translate between these representation spaces. Once in language space, images become ‚Äújust another type of token‚Äù.\n2. Fusion is a General Principle\nThe fusion mechanism we studied isn‚Äôt limited to images + text, and could be applied to:\n\nAudio: Whisper embeddings ‚Üí projection ‚Üí LLM space\nVideo: Frame embeddings + temporal encoding ‚Üí projection ‚Üí LLM space\nGeneralized scientific data: Spectra/time series ‚Üí specialized encoder ‚Üí projection ‚Üí LLM space\n\nThe pattern is universal, and there‚Äôs no reason multiple of these can‚Äôt happen at once, i.e - image ‚Üí image projection ‚Üí LLM space image tokens, - audio ‚Üí audio projection ‚Üí LLM space audio tokens, - LLM space image tokens + LLM space audio tokens + text tokens ‚Üí LLM\n3. Toward Truly Universal Models These architectures get us closer to a future where:\n\nUniversal tokenization: All modalities become standardized tokens, and any decoding task can accept those tokens\nShared architectures: One model type processes everything\nEmergent translation: Models learn to translate between any modalities they‚Äôve seen\n\nWe‚Äôre not fully there yet. But for now, thoughtful engineering of fusion mechanisms - like we did today - remains essential."
  },
  {
    "objectID": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html#basic-setup",
    "href": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html#basic-setup",
    "title": "Fine Tuning LLaVA with and without LoRA",
    "section": "",
    "text": "I‚Äôll assume this notebook is running on Google colab.\nIf so, we need to install some packages before we proceed. After the below cell runs, restart the kernel to be sure the notebook has access to these packages.\n\n!pip install -q bitsandbytes\n!pip install -q peft\n!pip install -q --upgrade ipywidgets==8.1.7\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72.9/72.9 MB 34.5 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 363.4/363.4 MB 2.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 13.8/13.8 MB 120.2 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24.6/24.6 MB 96.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 883.7/883.7 kB 56.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 664.8/664.8 MB 2.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 211.5/211.5 MB 6.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56.3/56.3 MB 41.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 127.9/127.9 MB 18.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 207.5/207.5 MB 3.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.1/21.1 MB 105.3 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 139.8/139.8 kB 6.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.2/2.2 MB 61.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.6/1.6 MB 81.7 MB/s eta 0:00:00\n\n\n\nimport requests\nimport json\nimport os\nfrom pathlib import Path\nfrom PIL import Image\nimport time\nfrom typing import Dict, List, Any\nfrom datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\nimport hashlib\nimport tqdm\nimport pandas as pd\nfrom datasets import load_dataset\nimport numpy as np\nimport random\nfrom PIL import Image\nfrom IPython.display import display\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model"
  },
  {
    "objectID": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html#an-introduction-to-llava",
    "href": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html#an-introduction-to-llava",
    "title": "Fine Tuning LLaVA with and without LoRA",
    "section": "",
    "text": "LLaVA (Large Language and Vision Assistant) is a model that was introduced by Liu et al.¬†in the Vision Instruction Tuning paper in late 2023.\nIt grafts a vision encoder onto a causal-LM so the model can ‚Äúread‚Äù an image before predicting text. As such, it has multiple transformer components working in tandem:\n\n\n\n\n\n\n\n\nComponent\nWhat it is\nShape\n\n\n\n\nVision encoder\nViT-L/14 (CLIP) patch-embed layer, frozen\n3 √ó 336 √ó 336 ‚Üí 257 √ó 1024\n\n\nProjector\n2-layer MLP (GELU)\n1024 ‚Üí 4096 (or 2048) (LLaMA hidden)\n\n\nLanguage model\nLLaMA decoder-only Transformer\n4096-d (LLaMA), 2048-d (LLaMA-Tiny)\n\n\n\nBefore we get to training this model, we‚Äôll look thoroughly at its architecture and training procedure to understand what it‚Äôs capable of!\n\nNote: In this tutorial, we‚Äôll be training on LLaVA-tiny, so we‚Äôll focus on the more fine-grained details there where specified. There‚Äôs nothing special about LLaVA-tiny besides that these underlying components were chosen to be small - the same principles about how the models were stitched together and trained applies to the larger LLaVA versions as well.\n\n\n\nIn order for LLaVA to be able to ‚Äúlook‚Äù at images, it needs an encoder portion that turns an image into meaningful embeddings.\n\nFor this, LLaVA uses a Vision Transformer (ViT) model ‚Äî specifically, a large version known as ViT-L/14. This same vision encoder is used by both larger LLaVA models and LLaVA-tiny.\nViT-L/14: - Has 24 transformer blocks (the ‚ÄúL‚Äù = the ‚ÄúLarge‚Äù version). - Splits the image into patches of 14 √ó 14 pixels (the ‚Äú14‚Äù). - Processes images of 336 √ó 336 pixels, which gives you 24 √ó 24 = 576 patches, plus one extra special token (577 total tokens).\nEach of these patches is treated a bit like a ‚Äúvisual word‚Äù ‚Äî it gets turned into a 1024-dimensional vector that summarizes the visual content in that small chunk of the image.\n\n\nLet‚Äôs take a step back and look at how a ViT turns an image into a tokenized sequence.\nImages are of course made of pixels - and ViT starts with images that are 336x336 pixels. Transformer architectures fundamentally want to make use of sequences. While pixels are naturally numeric, flattening all pixels of an image, even of 336x336 (which is relatively small!) = 112896 values - too large to efficiently process, and probably not worth it in terms of the information actually contained in all individual pixel values.\nThe core innovation behind ViT is to sequence patches of the image. Small enough patches of an image can be considered to contain approximately one ‚Äúthing‚Äù and thus can be treated as a single region of focus in the image. 24 x 24 patches taken over an image that‚Äôs 336x336 = 576 values - much better.\n\ndisplay(iImage(\"images/pixels_vs_patches.png\", width=1000))\n\n\n\n\n\n\n\n\n\n\n\nThe next step is to use those patches to actually get meaningful information about the image.\nFor each patch {1, 2, ‚Ä¶. 576}:\n\nFlatten and Project\n\n\nFlatten it into a single 1D vector of pixel values\nPass that through a learnable linear projection layer, which maps that pixel sequence into a 1024-dimensional embedding that is a summary of the patch‚Äôs visual contents.\n\n\nAdd Positional Information\n\n\nEach patch has a corresponding learned positional embedding ‚Äî a fixed vector (same shape as the patch embedding) that represents its location.\nThe model adds this positional vector to the patch embedding.\n\nSo for each patch:\n[embedding] = [patch vector] + [position vector]\nThis lets the model reason about spatial layout, not just content.\n\n\nüìå Note: Why 196 ‚Üí 1024 projection?\n\nYou might be wondering why we do the projection to 1024 values from 196. We talked about how 336x336 pixels -&gt; 112896 is a ‚Äúwaste‚Äù, but 24x24 patches of size 1024 = 589824 - more information than just our pixel values!\nSo why do we do this?\nIt might seem like just an inflation of the data, but doing this projection makes the information richer and more suitable for the Transformer to use. The 1024 vectors that come out of the linear projection layer are contextualized, rich embeddings of the pixel information, and provide more meaningful information than pixel values alone.\n\nTransformers expect high-dim input (e.g., LLaMA uses 2048-d text tokens)\nRaw pixels are low-level; projection lets the model learn abstract features\nThe number of tokens works out to still just 576 (one per patch), so compute stays reasonable (we‚Äôll see this in more detail soon)\n\nSo think of this transformation like it‚Äôs converting an image patch into a dense ‚Äúvisual word.‚Äù\n\n\ndisplay(iImage(\"images/processing_patches.png\", width=800))\n\n\n\n\n\n\n\n\n\n\n\nOnce we have this rich embedding for each of our patches, it‚Äôs time for the core transformer architecture to do its thing. Each transformer layer works to model relationships between patches using self-attention.\nAt each layer:\n\nAttention mechanism\nEach patch embedding goes through fully connected layers to create Query, Key, and Value representations. Each patch ‚Äúlooks‚Äù at every other patch and decides how much attention to pay to each one.\n‚Üí This creates a big 576 √ó 576 matrix of attention scores.\n‚Üí Each patch gets updated as a weighted combination of all the others. ‚Üí The result goes through another fully connected layer to produce the final attention output.\nMulti-Layer Perceptron (MLP)\nAfter attention, each updated embedding goes through a seperate feedforward neural network. This typically expands the embedding to a larger size, applies a non-linearity, then contracts back to the original size.\nOutput\nWe now have a new set of patch embeddings ‚Äî same shape (576 √ó 1024), but now contextualized:\nEach patch now contains information not just about itself, but also about the rest of the image.\n\nIn ViT-L which has 24 layers, this process is repeated 24 times. And so, the embedding of an image coming out of ViT is size 576 x 1024 representing 576, 1024-size embeddings that describe the image.\nBelow, we take a closer look at how the attention portion of one of these layers works.\n\n\nüìå Revisiting that 1024 Dimension:\n\nOk, the point that we made above:\n‚ÄúThe number of tokens works out to still just 576 (one per patch), so compute stays reasonable‚Äù\nmight not have immediately made sense. And as we just discussed, every layer of ViT will compute over this 576 x 1024 matrix, so we‚Äôll constantly be passing around those 589824 values of information. It might have made sense why this projection to 1024 is more meaningful, but how is it also more computationally efficient than dealing with our raw pixel values (112896)?\nThis is because, thanks to the attention mechanism, transformers scale quadratically with sequence length (\\(L\\)) but only linearly with embedding dimension (\\(D\\)). This really comes down to the fact that the attention mechanism contains a dot product calculation of an \\(L \\times D\\) and \\(D \\times L\\) matrix, which requires that every row be multiplied by every column. Therefore, every increase in \\(L\\) requires \\(L^2\\) calculations.\nIn ViT, the token count that assigns one token per patch is our sequence length. So, we save tons of compute using patches to make the sequence length smaller for the transformer layers. That‚Äôs why patch-based tokenization is critical ‚Äî if we instead kept an 112896 sequence of pixels, using attention on it would be infeasible.\n\n\ndisplay(iImage(\"images/transformer_layer.png\", width=900))\n\n\n\n\n\n\n\n\n\n\n\n\nThe other half of LLaVA is of course the language understanding part. For LLaVA, the language model component is a decoder-only Transformer, whose job is to turn embeddings into words.\n\nLLaVA relies on a LLaMA model for its language component. In the case of LLaVA-tiny, it simply uses LLaMA-Tiny, a smaller version of LLaMA-7B:\n\n\n\nModel\nLayers\nHidden size\n# Params\n\n\n\n\nLLaMA-7B\n32\n4096\n6.7 B\n\n\nTiny LLaMA\n22\n2048\n~1.1 B\n\n\n\nSo the architecture is the same ‚Äî just fewer layers, making it small enough to fit and train on a modest GPU.\n\n\nThe job of a decoder-based language model is to take a sequence of tokens and turn it into language.\nThe input might be:\n&lt;image&gt; Describe this galaxy.\nThis is tokenized, both the image (we‚Äôll dicuss) and the words ‚ÄúDescribe this galaxy‚Äù by a tokenizer into a numerical sequence that the decoder layers can handle.\nThe job of the tokenizer is to create meaningful embeddings for words. Each word token in the description is embedded into a vector that contains richer embeddings than just the word itself. Tokenizers are trained specifically to do this task.\nThe embedding dimension (\\(D\\)), 2048, is fixed by the tokenizer and therefore the model. The sequence length (\\(L\\)) depends on how many things there are to tokenize - it depends on the image size (although this is fixed to 336x336), and how many words we gave in our prompt to LLaVA. In practice, LLM‚Äôs have some maximum sequence length, \\(L_{max}\\) that they can handle.\n\n\n\nA decoder transformer generates words one by one - every new token generated is conditioned on what came before.\nThe primary computation for a decoder layer is the same as for any other transformer layer, except that the attention portion is different in that masked attention is used instead. This zeros out attention between the tokens and others tokens that are later in the sequence to them.\nThe function of this is two-fold. - During training, the model can‚Äôt ‚Äúcheat‚Äù by allowing tokens to attend to ones that they shouldn‚Äôt know exist yet. - During inference, tokens in the past are fixed (already generated, or were input) - there‚Äôs no value in attending them to future tokens - you can‚Äôt do anything to change them now anyway.\n\ndisplay(iImage(\"images/decoder_layer.png\", width=900))\n\n\n\n\n\n\n\n\n\n\n\nThe text decoder of LLaVA-tiny has 22 layers, so the above process happens 22 times, and coming out of the decoder layers is an \\(L \\times 2048\\) matrix.\nThis is turned into a prediction of the next token by: - taking the embedding for the most recent (L-th) word (remember, this has now been contextualized many times by all the words before it) - it‚Äôs passed to a linear projecion layer to get a vector of length vocab_size (typically ~32k) - this is typically passed to a softmax function to produce, for all words in the vocabulary, the probability that a given word is the next token - with some temperature allowing for randomness, a token of high probability is chosen.\nWhen this is appended to the sequence, L becomes L + 1, and the whole process of 22 decoder layers starts again with the new sequence. This gradually produces the output that you see when asking LLaVA about an image!\n\n\n\n\nIt‚Äôs finally time to talk about the core ‚Äútechnology‚Äù behind a model like LLaVA - the fusion mechanism. Up until now, we have a ‚Äúregular‚Äù ViT that processes an image and a ‚Äúregular‚Äù large language model that makes text. The fusion mechanism is what connects them.\nSince LLaVA‚Äôs task is to process and understand an image to be able to create text contextualized by it, the job of the fusion mechanism is really to turn the image emedding into something that the language model understands.\nIn reality, the fusion mechanism is extremely simple ‚Äî it‚Äôs just a linear projection layer that maps the embeddings from the ViT into the same vector space as the language model‚Äôs token embeddings. This lets the image patches act like ‚Äúvisual tokens‚Äù that the language model can attend to just like words.\n\nLLaVA uses a simple fusion mechanism to do this: - The final output of the vision encoder is a sequence of 576 visual tokens (one per patch), each a 1024-dimensional vector. - These are passed through a small projection layer (usually an MLP) that maps 1024 ‚Üí 2048, matching the LLaMA embedding size. - The projected visual tokens are then prepended to the input tokens of the language model, as if they were special ‚Äúimage tokens.‚Äù\nThe LLM then attends to these visual tokens just like text, using self-attention across the combined sequence. This way, the LLM gets the visual tokens in a space that‚Äôs already meaningful to it, but also get‚Äôs told that they are visual tokens, so that it can understand how to use them during the training phase.\nThis approach might seem super simple (and it is!) but it‚Äôs also very flexible, and surprisingly effective with the right training, as we‚Äôll see.\n\ndisplay(iImage(\"images/llava_pipeline.png\", width=1000))\n\n\n\n\n\n\n\n\n\n\n\nThe final thing to discuss is how something like this gets trained. A ViT and a LLaMA already understand their respective datatypes, so training has to make sure to work with, and not against that knowledge.\nFirst, we‚Äôll talk about an important element of how LLaVA was trained that happens, even before the pipeline is trained all together that sets LLaVA up for success.\n\nViT-L/14 Was Trained With Contrastive Learning LLaVA doesn‚Äôt train the vision encoder from scratch ‚Äî it inherits a ViT model that was already trained using CLIP (Contrastive Language-Image Pretraining), created by OpenAI.\n\nCLIP was trained on 400 million (image, text) pairs scraped from the internet (web pages with associated images and alt text, captions, etc.).\nThe image goes through a ViT (like ViT-L/14).\nThe text goes through a Transformer-based text encoder. This turns text -&gt; embeddings, but importantly, is not generative.\nThen it‚Äôs trained using a contrastive loss, which aims to push the embeddings of an image and the embeddings of its corresponding caption closer in embedding space, and embeddings that don‚Äôt correspond further away.\n\nThe result: ViT-L/14 learns to produce embeddings that are already aligned with natural language text embeddings, so the 576x1024 embeddings coming out of the ViT are already ‚Äúlanguage-aligned‚Äù to some extent ‚Äî they live in the same conceptual space as captions.\n\n\nBecause ViT was trained with CLIP to understand images, and to put them in a text-friendly space, it‚Äôs frozen during training.\nThe first phase of LLaVA training teaches the fusion mechanism to align, and the language model to accept, tokens.\n\nDataset: image‚Äìcaption pairs (COCO, CC3M, etc.)\nInput: an image\nTarget Output: the corresponding caption (as tokens)\nLoss: language modeling loss (cross-entropy on the caption tokens)\n\nDuring this training phase, the weights of the fusion mechanism layer are trained (ViT output ‚Üí LLM input space), and the language model is lightly tuned to learn what to do with image tokens. &gt;\n\n‚Äúlightly tuned‚Äù meaning using a low learning rate, PEFT, or unfreezing only certain layers, depending on the LLaVA version. The idea here is to make use of what the language model already knows as best as possible.\n\nThis is the training phase where LLaVA learns to ‚Äútalk about‚Äù images at all.\n\n\n\nThe first phase of training teaches LLaVA to write a caption for a corresponding image. But LLaVA is also able to accept prompt instructions, i.e.¬†you can include with your image ‚ÄúDescribe this image‚Äù or ‚ÄúWhat‚Äôs interesting about this picture?‚Äù\nFor this to work, the language part of LLaVA needs to be instruction fine-tuned. We won‚Äôt get into the details here since it‚Äôs a bit outside of the scope of how LLaVA works specifically, but this phase of training is just meant to align the language part of the model - both ViT and the fusion mechanism are frozen while the language model learns to follow instructions.\n\nAnd that‚Äôs it!! Hopefully, you should now have a concrete understanding of how information flows through a model like LLaVA, and how we train a model to this sort of multi-modal alignment!"
  },
  {
    "objectID": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html#dataset-creation",
    "href": "posts/LLaVA-Astronomy/LLaVA_GalaxyZoo.html#dataset-creation",
    "title": "Fine Tuning LLaVA with and without LoRA",
    "section": "",
    "text": "In this notebook, we‚Äôll be fine tuning our own LLaVA to understand astronomical images. Since LLaVA is trained on image and image caption pairs, we need a similar data structure to train it with.\nHigh quality image captions that contain real astronomical descriptions are ideal. For the purposes of this tutorial, we‚Äôll be gathering data from Galaxy Zoo 2 - a set of galaxy images and their morphological classifications. We‚Äôll build the morphological classifications into captions as best as we can. While not totally ideal for real, quality fine tuning, it should be sufficient to see some basic changes in the model.\n\n\n\n\nSource Hugging Face dataset mwalmsley/gz2 (172 k SDSS galaxy JPGs + volunteer morphology votes).\n\nImages JPEG cut-outs around each galaxy.\n\nLabels converted to text\n\nElliptical / Spiral (+ bar, arm count, bulge, merger/odd)\n\nAxis-ratio for ellipticals (‚Äúround‚Äù, ‚Äúelongated‚Äù)\n\n\nCaption length ‚âà 5-20 words typically.\n\n\n\n\nThe below code:\n\ndownloads N examples (set by N),\n\nrescales to 336 √ó 336,\n\nbuilds captions from the columns of galaxy morphological descriptions\ncategorizes galaxies by spiral vs elliptical to create a balanced dataset\nbuilds captions using a balanced dataset\nwrites gz2_llava.jsonl for LLaVA training.\n\n\n\nLLaVA expects training data in a conversation format that mimics how humans discuss images. Each training example contains:\n\nHuman question/prompt about an image\nLLM model response with detailed explanation\nImage reference linking to the visual content\n\nIn the below code, you can set N - the number of images to be downloaded per categorization.\n\n# ------------------------------------------------------------------ paths\nroot = Path(\"gz2_llava_hf\")\nimgs = root / \"images\"\nroot.mkdir(exist_ok=True)\nimgs.mkdir(exist_ok=True)\n\n# ------------------------------------------------------------------ load full dataset first\nprint(\"Loading dataset...\")\nds = load_dataset(\"mwalmsley/gz2\", split=\"train\")\nN = 1000 # Max N per class\n\n# ------------------------------------------------------------------ helper for captions\ndef describe(r):\n    out = []\n    # ============ turn labels into words for captions =======================\n    smooth = r[\"smooth-or-featured-gz2_smooth_fraction\"] &gt; 0.6\n    if smooth:\n        out.append(\"This is an image of an elliptical galaxy\")\n        # axis ratio\n        if r[\"how-rounded-gz2_round_fraction\"] &gt; .5:\n            out.append(\"nearly round in shape\")\n        elif r[\"how-rounded-gz2_in-between_fraction\"] &gt; .5:\n            out.append(\"moderately elongated in shape\")\n        elif r[\"how-rounded-gz2_cigar_fraction\"] &gt; .5:\n            out.append(\"highly elongated in shape\")\n    else:\n        out.append(\"This is an image of a spiral galaxy\")\n        if r[\"bar-gz2_yes_fraction\"] &gt; .5:\n            out.append(\"with a central bar\")\n        if r[\"spiral-arm-count-gz2_2_fraction\"] &gt; .5:\n            out.append(\"with two arms\")\n        elif r[\"spiral-arm-count-gz2_3_fraction\"] &gt; .5:\n            out.append(\"with three arms\")\n        elif r[\"spiral-arm-count-gz2_more-than-4_fraction\"] &gt; .5:\n            out.append(\"with many arms\")\n    # bulge prominence / shape\n    if r[\"bulge-size-gz2_obvious_fraction\"] &gt; .5:\n        out.append(\"prominent bulge at its center\")\n    if r[\"bulge-shape-gz2_boxy_fraction\"] &gt; .5:\n        out.append(\"boxy bulge at its center\")\n    # mergers / oddities\n    if r[\"something-odd-gz2_yes_fraction\"] &gt; .4:\n        out.append(\"disturbed or merging with another galaxy\")\n    return \", \".join(out) + \".\"\n\n# ------------------------------------------------------------------ categorize by type\nprint(\"Categorizing galaxies...\")\nspirals = []\nellipticals = []\n\nfor i, ex in enumerate(tqdm.tqdm(ds, desc=\"Categorizing\")):\n    smooth = ex[\"smooth-or-featured-gz2_smooth_fraction\"] &gt; 0.6\n    if smooth:\n        ellipticals.append(i)\n    else:\n        spirals.append(i)\n\nprint(f\"Found {len(spirals)} spirals and {len(ellipticals)} ellipticals\")\n\n# ------------------------------------------------------------------ balance dataset\n# Take equal numbers of each\nn_per_class = min(len(spirals), len(ellipticals), N)  # Max N per class\nprint(f\"Selecting {n_per_class} of each type (total: {n_per_class * 2})...\")\n\n# Random sample from each\nrandom.seed(42)\nselected_spirals = random.sample(spirals, n_per_class)\nselected_ellipticals = random.sample(ellipticals, n_per_class)\n\n# Combine and shuffle\nselected_indices = selected_spirals + selected_ellipticals\nrandom.shuffle(selected_indices)\n\n# ------------------------------------------------------------------ build records into captions\nrecords = []\ncaption_lengths = {\"spiral\": [], \"elliptical\": []}\n\nfor i in tqdm.tqdm(selected_indices, desc=\"Processing\"):\n    ex = ds[i]\n    img = ex[\"image\"].convert(\"RGB\").resize((336, 336))\n    fname = imgs / f\"{hashlib.md5(ex['id_str'].encode()).hexdigest()}.jpg\"\n    img.save(fname, \"JPEG\", quality=85)\n\n    caption = describe(pd.Series(ex))\n    galaxy_type = \"elliptical\" if ex[\"smooth-or-featured-gz2_smooth_fraction\"] &gt; 0.6 else \"spiral\"\n\n    records.append({\"image\": str(fname), \"text\": caption})\n    caption_lengths[galaxy_type].append(len(caption.split(\",\")))  # Count features\n\n# ------------------------------------------------------------------ statistics to make sure it's balanced enough\nprint(\"\\n=== Dataset Statistics ===\")\nprint(f\"Total examples: {len(records)}\")\nprint(f\"Spirals: {n_per_class} ({n_per_class/len(records)*100:.1f}%)\")\nprint(f\"Ellipticals: {n_per_class} ({n_per_class/len(records)*100:.1f}%)\")\n\nprint(\"\\nAverage features per type:\")\nprint(f\"Spirals: {sum(caption_lengths['spiral'])/len(caption_lengths['spiral']):.1f} features\")\nprint(f\"Ellipticals: {sum(caption_lengths['elliptical'])/len(caption_lengths['elliptical']):.1f} features\")\n\n# Show some examples of varying lengths\nprint(\"\\nExample captions by length:\")\nsorted_records = sorted(records, key=lambda x: len(x['text']))\nprint(f\"Shortest: {sorted_records[0]['text']}\")\nprint(f\"Median: {sorted_records[len(sorted_records)//2]['text']}\")\nprint(f\"Longest: {sorted_records[-1]['text']}\")\n\n# ------------------------------------------------------------------ dump jsonl\nwith open(root / \"gz2_llava.jsonl\", \"w\") as f:\n    for r in records:\n        f.write(json.dumps(r) + \"\\n\")\n\nprint(f\"\\n‚úì Wrote {len(records)} balanced examples ‚Üí {root/'gz2_llava.jsonl'}\")\n\nLoading dataset...\n\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorizing galaxies...\n\n\nCategorizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172377/172377 [04:16&lt;00:00, 672.34it/s]\n\n\nFound 56141 spirals and 116236 ellipticals\nSelecting 1000 of each type (total: 2000)...\n\n\nProcessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:11&lt;00:00, 166.74it/s]\n\n\n\n=== Dataset Statistics ===\nTotal examples: 2000\nSpirals: 1000 (50.0%)\nEllipticals: 1000 (50.0%)\n\nAverage features per type:\nSpirals: 2.1 features\nEllipticals: 2.5 features\n\nExample captions by length:\nShortest: This is an image of a spiral galaxy.\nMedian: This is an image of an elliptical galaxy, moderately elongated in shape.\nLongest: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, boxy bulge at its center, disturbed or merging with another galaxy.\n\n‚úì Wrote 2000 balanced examples ‚Üí gz2_llava_hf/gz2_llava.jsonl\n\n\n\n\n\nAfter running this data collection, you should have:\ngz2_llava_hf/\n‚îú‚îÄ‚îÄ gz2_llava.jsonl.json          # Dataset with all metadata\n‚îú‚îÄ‚îÄ images/                      # Downloaded astronomical images\n‚îÇ   ‚îú‚îÄ‚îÄ 00c1591a613183ff21a67f79a29b5940.jpg\n‚îÇ   ‚îú‚îÄ‚îÄ 00fd8380b58f4c5086f655e646e0d5a0.jpg\n‚îÇ   ‚îî‚îÄ‚îÄ ...\nEach entry in gz2_llava.jsonl contains: - image: Path to the astronomical image - text: description of galaxy morphology\n\n\n\n\nNow, let‚Äôs look a at a random example in our newly-downloaded dataset.\nThis cell can be re-run to inspect a new random image.\n\nimport json, random\nfrom PIL import Image\nfrom IPython.display import display\n\n# path to the JSON-Lines file you just wrote\nJSONL = \"gz2_llava_hf/gz2_llava.jsonl\"      # adjust if you used a different folder\n\n# grab one random record (or change to lines[0] for the first)\nwith open(JSONL) as f:\n    rec = json.loads(random.choice(f.readlines()))\n\nprint(rec[\"text\"])          # caption\ndisplay(Image.open(rec[\"image\"]))  # shows the image in a notebook\n\nThis is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center.\n\n\n\n\n\n\n\n\n\n\n\n\nThe final step is to make sure we have a dataset class set up for use with the model.\nWe need to convert our conversation data into PyTorch‚Äôs training format. This cell creates a Dataset class that loads images, processes text, and tokenizes everything for LLaVA training.\nOne crucial step that happens here is that we mask out the image tokens so they are not trained on. If we don‚Äôt do this step, the full ‚Äúoutput‚Äù that the model will check for correctness on will include the image tokens, which makes no sense!\n\nclass GZ2LLaVADataset(Dataset):\n    def __init__(self, jsonl_path, processor):\n        with open(jsonl_path) as f:\n            self.data = [json.loads(l) for l in f]\n        self.proc = processor\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        img = Image.open(item[\"image\"]).convert(\"RGB\")\n\n        # Build the full text\n        prompt = \"USER: &lt;image&gt;\\nDescribe this galaxy.\\nASSISTANT: \"\n        full_text = prompt + item[\"text\"]\n\n        # Process image and text together\n        enc = self.proc(text=full_text, images=img, return_tensors=\"pt\")\n\n        input_ids = enc[\"input_ids\"].squeeze()\n        attention_mask = enc[\"attention_mask\"].squeeze()\n        pixel_values = enc[\"pixel_values\"].squeeze()\n\n        # CRITICAL: Find where the actual caption starts\n        # We need to mask everything BEFORE the caption\n        labels = input_ids.clone()\n\n        # Tokenize just the prompt to find its length AFTER image expansion\n        prompt_enc = self.proc(text=prompt, images=img, return_tensors=\"pt\")\n        prompt_length = prompt_enc[\"input_ids\"].shape[1]\n\n        # Mask everything up to the caption\n        labels[:prompt_length] = -100\n\n        # Debug to verify\n        if idx &lt; 3:\n            # Count what we're actually learning\n            unmasked = (labels != -100).sum()\n            print(f\"\\nExample {idx}:\")\n            print(f\"  Total tokens: {len(input_ids)}\")\n            print(f\"  Caption tokens to learn: {unmasked}\")\n            print(f\"  Caption text: {item['text'][:50]}...\")\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"pixel_values\": pixel_values,\n            \"labels\": labels,\n        }\n\n\n\n\nThis cell loads the TinyLLaVA model and creates helper functions for testing. We‚Äôll use these functions to compare the model‚Äôs responses before and after training.\nWe take tiny LLaVA from this HuggingFace repository.\n\n\n\n\n\n\nWeights & config are pulled from ü§ó Hub repo bczhou/tiny-llava-v1-hf.\n\nIt combines:\n\nVision: CLIP ViT-L/14 (~300M parameters)\nProjection: 2-layer MLP (~8M parameters)\n\nLanguage: TinyLlama-1.1B-Chat (~1.1B parameters)\nTotal: ~1.4B parameters ‚Üí ‚âà 2.8 GB in fp16, 5.6 GB in fp32\n\n\ndevice_map=\"auto\" = ü§ó Accelerate loads each layer on the first GPU/CPU with room.\ntorch_dtype=torch.float32 tells HF to up-cast fp16 weights to fp32 when opening‚Äîslower, but avoids NaN/overflow on consumer GPUs.\n\nWe also below will make a function that helps us test our model. We can set certain generation parameters, and do specify some:\n\ndo_sample=False: Deterministic (greedy) decoding - always picks the most likely token\nnum_beams=3: Beam search explores multiple paths to find better sequences\nrepetition_penalty=1.2: Discourages repeating phrases (common in small models)\nno_repeat_ngram_size=2: Prevents repeating 2-word phrases exactly\nmin_length=10: Forces at least 10 tokens - prevents immediate EOS generation\nmax_new_tokens=30: Limits response length for quick testing (and we don‚Äôt need much more anyway)\n\n\ndef setup_tiny_llava():\n    \"\"\"One function to set everything up\"\"\"\n    print(\"Setting up TinyLLaVA...\")\n\n    # Load model and processor\n    model_id = \"bczhou/tiny-llava-v1-hf\"\n\n    model = LlavaForConditionalGeneration.from_pretrained(\n        model_id,\n        torch_dtype=torch.float32,  # Use FP32 instead of FP16\n        device_map=\"auto\"\n    )\n\n    processor = AutoProcessor.from_pretrained(model_id)\n\n    # Fix patch_size issue - only a tinyllava thing\n    if processor.patch_size is None:\n        processor.patch_size = 14\n        print(\"‚úì Fixed patch_size\")\n\n    print(\"‚úì Model and processor ready\")\n    return model, processor\n\ndef test_model(model, processor, test_image_path):\n    \"\"\"Simple test function\"\"\"\n    image = Image.open(test_image_path).convert('RGB').resize((336, 336))\n\n    inputs = processor(\n        text=\"USER: &lt;image&gt;\\nWhat is this? ASSISTANT:\",\n        images=image,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=30,\n            eos_token_id=processor.tokenizer.eos_token_id,\n            pad_token_id=processor.tokenizer.pad_token_id,\n            do_sample=False,              # greedy\n            repetition_penalty=1.2,       # avoid loops\n            no_repeat_ngram_size=2,\n            num_beams=3,                   # optional beam search\n            min_length=10,  # Force at least 10 tokens\n        )\n\n    response = processor.decode(outputs[0], skip_special_tokens=True)\n    return response.split(\"ASSISTANT:\")[-1].strip()\n\nLet‚Äôs quickly take a look at the architecture of TinyLLaVA and make sure that it matches what we expect.\n\nmodel, processor = setup_tiny_llava()\nprint(model)\n\nSetting up TinyLLaVA...\n\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\n\n\n\n\n\n\n\n\n‚úì Fixed patch_size\n‚úì Model and processor ready\nLlavaForConditionalGeneration(\n  (model): LlavaModel(\n    (vision_tower): CLIPVisionModel(\n      (vision_model): CLIPVisionTransformer(\n        (embeddings): CLIPVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n          (position_embedding): Embedding(577, 1024)\n        )\n        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-23): 24 x CLIPEncoderLayer(\n              (self_attn): CLIPAttention(\n                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()\n                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n              )\n              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (multi_modal_projector): LlavaMultiModalProjector(\n      (linear_1): Linear(in_features=1024, out_features=2048, bias=True)\n      (act): GELUActivation()\n      (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n    )\n    (language_model): LlamaModel(\n      (embed_tokens): Embedding(32064, 2048)\n      (layers): ModuleList(\n        (0-21): 22 x LlamaDecoderLayer(\n          (self_attn): LlamaAttention(\n            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        )\n      )\n      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n  )\n  (lm_head): Linear(in_features=2048, out_features=32064, bias=False)\n)\n\n\n\n\n\nNow, we are ready to set up the loop to perform full fine tuning!\nHuggingface makes this super easy by just defining a trainer. But first, we‚Äôll need to define the training arguments. This sets up exactly how the training will proceed, and it‚Äôs where we can set hyperparameters, determine what outputs we want to see, where the model will save, all of that!\n\n# Training configuration - full FP32 to avoid all gradient issues\ntraining_args = TrainingArguments(\n    output_dir=\"./tiny-llava-trained\",         # Where to save model checkpoints\n    num_train_epochs=3,                        # How many times to go through the dataset\n    per_device_train_batch_size=1,             # Process 1 example at a time (small for memory)\n    learning_rate=1e-5,                        # How fast the model learns (small = careful)\n    logging_steps=1000,                        # Print progress every N training steps\n    save_strategy=\"no\",                        # Don't save checkpoints (workshop demo only)\n    eval_strategy=\"no\",                        # Don't evaluate during training (keep simple)\n    remove_unused_columns=False,               # Keep all data columns for LLaVA\n    fp16=False,                                # Disable half-precision (avoids gradient issues)\n    bf16=False,                                # Disable bfloat16 (avoids gradient issues)\n    report_to=\"none\",                          # Don't send metrics to tracking services\n    dataloader_num_workers=0,                  # Use main thread only (avoids multiprocessing issues)\n    dataloader_pin_memory=False                # Disable memory pinning (avoids GPU memory issues)\n)\n\n\n\n\nThen, we‚Äôre finally ready to set it training.\nBelow, we‚Äôll set the model and processor up, create the dataset from our formatted JSON, and start training.\nThis cell will also show us what the model‚Äôs response to a given image looked like both before and after training, so we can see if it got any better.\n\nOne critical thing that we do below is freeze the vision model weights.\nThe vision encoder is already excellent at extracting visual features, and training it on our small galaxy dataset would likely only make it worse at general vision tasks. We only need to teach the model how to describe galaxies, not how to see them differently.\n\nprint(\"üöÄ Starting TinyLLaVA full fine tuning....\")\nprint(\"=\" * 30)\n\n# 1. Setup using our setup function\nmodel, processor = setup_tiny_llava()\n\n# 2. freeze the vision‚Äêtower weights\nfor name, param in model.named_parameters():\n    if \"vision_tower\" in name:\n        param.requires_grad = False\n\n# 3. create the dataset from our LLaVA-formatted JSON\ndataset = GZ2LLaVADataset(\"gz2_llava_hf/gz2_llava.jsonl\", processor)\n\nval_size = int(0.1 * len(dataset)) # set validation size - we'll just do 10%\ntrain_size = len(dataset) - val_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# ------Test before training -----\nprint(\"\\nTesting BEFORE training:\")\nsample_item = dataset.data[0]\ntest_image = f\"{sample_item['image']}\"\nbefore_response = test_model(model, processor, test_image)\n\nplt.figure(figsize=(8, 6))\nimg = Image.open(test_image)\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nprint(f\"BEFORE: {before_response}\")\n# -------------------------------\n\n# 4. Set up the trainer with our args\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    processing_class=processor\n)\n\n# 5. set the training going\nprint(\"\\nStarting training...\")\nstart_time = time.time()\ntrainer.train()\nfull_training_time = time.time() - start_time\nprint(f\"‚úì Training completed in {full_training_time:.1f} seconds ({full_training_time/60:.1f} minutes)\")\n\n#  ----- Test after training ------\nprint(\"\\nTesting AFTER training:\")\nafter_response = test_model(model, processor, test_image)\nprint(f\"AFTER: {after_response}\")\nprint(f\"TRUE: {sample_item['text']}\")\n# -------------------------------\n\nüöÄ Starting TinyLLaVA full fine tuning....\n==============================\nSetting up TinyLLaVA...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\n\n\n\n\n\n\n\n\n‚úì Fixed patch_size\n‚úì Model and processor ready\n\nTesting BEFORE training:\n\n\n\n\n\n\n\n\n\nBEFORE: The image is a close-up view of a star in the middle of the night sky. The star appears to be glowing brightly,\n\nStarting training...\n\n\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n\n\n\n    \n      \n      \n      [5400/5400 36:55, Epoch 3/3]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n1000\n0.195500\n\n\n2000\n0.138800\n\n\n3000\n0.121600\n\n\n4000\n0.103600\n\n\n5000\n0.091000\n\n\n\n\n\n\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n‚úì Training completed in 2216.3 seconds (36.9 minutes)\n\nTesting AFTER training:\nAFTER: an image of an elliptical galaxy, nearly round in shape. prominent bulge at its center. disturbed or merging with another galax\nTRUE: This is an image of an elliptical galaxy, nearly round in shape, prominent bulge at its center.\n\n\nLet‚Äôs see how it did on a random image (you can re-run this cell to generate a new image)\n\nprint('-------------FROM TRAINING-------------------')\n# Get a random index from the training subset\ntrain_idx = np.random.randint(len(train_dataset))\nactual_idx = train_dataset.indices[train_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig1 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\nprint('\\n------------FROM VALIDATION------------------')\n# Get a random index from the validation subset\nval_idx = np.random.randint(len(val_dataset))\nactual_idx = val_dataset.indices[val_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig2 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\n-------------FROM TRAINING-------------------\nTRUTH: This is an image of a spiral galaxy, prominent bulge at its center.\nPREDICTED: an image of a spiral galaxy, prominent bulge at its center. disturbed or merging with another Galy. prominentbulgeat\n\n------------FROM VALIDATION------------------\nTRUTH: This is an image of an elliptical galaxy, nearly round in shape.\nPREDICTED: an image of an elliptical galaxy, nearly round in shape. boxy bulge at its center. disturbed or merging with another\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we‚Äôll use LoRA (Low-Rank Adaptation) to fine-tune TinyLLaVA more efficiently. LoRA lets us train only a small number of parameters while keeping the base model frozen.\nBut before we do that, let‚Äôs take another look at our model and talk about where exactly LoRA could be applied:\nLlavaForConditionalGeneration(\n  (model): LlavaModel(\n    (vision_tower): CLIPVisionModel(\n      (vision_model): CLIPVisionTransformer(\n        (embeddings): CLIPVisionEmbeddings(\n          (patch_embedding): Conv2d(...)  # ‚úÖ COULD use LoRA (but typically don't)\n          (position_embedding): Embedding(...)  \n        )\n        (pre_layrnorm): LayerNorm(...)  \n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-23): 24 x CLIPEncoderLayer(\n              (self_attn): CLIPAttention(\n                (k_proj): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (v_proj): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (q_proj): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (out_proj): Linear(...) # ‚úÖ COULD use LoRA (but we freeze vision)\n              )\n              (layer_norm1): LayerNorm(...)   \n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()  \n                (fc1): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n                (fc2): Linear(...)  # ‚úÖ COULD use LoRA (but we freeze vision)\n              )\n              (layer_norm2): LayerNorm(...)  \n            )\n          )\n        )\n        (post_layernorm): LayerNorm(...)   \n      )\n    )\n    (multi_modal_projector): LlavaMultiModalProjector(\n      (linear_1): Linear(...)  # ‚≠ê WE USE LoRA HERE (mm_projector)\n      (act): GELUActivation()  #\n      (linear_2): Linear(...)  # ‚≠ê WE USE LoRA HERE (mm_projector)\n    )\n    (language_model): LlamaModel(\n      (embed_tokens): Embedding(...)  #\n      (layers): ModuleList(\n        (0-21): 22 x LlamaDecoderLayer(\n          (self_attn): LlamaAttention(\n            (q_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (k_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (v_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (o_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (up_proj): Linear(...)    # ‚≠ê WE USE LoRA HERE\n            (down_proj): Linear(...)  # ‚≠ê WE USE LoRA HERE\n            (act_fn): SiLU()  #\n          )\n          (input_layernorm): LlamaRMSNorm(...)  #\n          (post_attention_layernorm): LlamaRMSNorm(...)  #\n        )\n      )\n      (norm): LlamaRMSNorm(...)  #\n      (rotary_emb): LlamaRotaryEmbedding()  #\n    )\n  )\n  (lm_head): Linear(...)  # ‚úÖ COULD use LoRA (but typically don't)\n)\n\n\nHere, we set the LoRA config, which will tell the peft library exactly how we want lora applied to our model.\nSome key configuration choices made below: - r=8, lora_alpha=16: We keep rank fairly small, because LLaVA tiny is quite a small model. - lora_dropout=0.05: small dropout prevents overfitting on our limited galaxy descriptions - target_modules: We apply LoRA adaptations to 3 different key parts of the model: - Attention layers (q_proj, k_proj, v_proj, o_proj): Help the model attend to relevant image regions - FFN layers (gate_proj, up_proj, down_proj): Transform features for galaxy-specific outputs - Vision-language projector (mm_projector): The crucial bridge between image and text is actually the most important for our task!\nIncluding all these modules ensures the model can both process visual features AND generate appropriate text. As you‚Äôll see below, with low rank, this is still a tiny fraction of our model size.\n\nlora_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", # Attention layers\n        \"gate_proj\", \"up_proj\", \"down_proj\",  # FFN layers\n        \"mm_projector\"] # fusion layer\n)\n\n\n\n\nWe use slightly different training arguments for LoRA. Notably, we‚Äôre able to increase our batch size to 8, and our learning rate is a bit higher. Because our batch size is larger, we‚Äôll also change logging_steps, as the number of steps is a function of how much data is processed at once.\n\n# Training configuration - full FP32 to avoid all gradient issues\ntraining_args = TrainingArguments(\n    output_dir=\"./tiny-llava-lora-trained\",    # Where to save model checkpoints\n    num_train_epochs=3,                        # How many times to go through the dataset\n    per_device_train_batch_size=8,             # Can use larger batch size now, less memory needs\n    learning_rate=5e-5,                        # A bit higher than full fine tuning\n    logging_steps=200,                         # Print progress every N training steps\n    save_strategy=\"no\",                        # Don't save checkpoints (workshop demo only)\n    eval_strategy=\"no\",                        # Don't evaluate during training (keep simple)\n    remove_unused_columns=False,               # Keep all data columns for LLaVA\n    fp16=False,                                # Disable half-precision (avoids gradient issues)\n    bf16=False,                                # Disable bfloat16 (avoids gradient issues)\n    report_to=\"none\",                          # Don't send metrics to tracking services\n    dataloader_num_workers=0,                  # Use main thread only (avoids multiprocessing issues)\n    dataloader_pin_memory=False                # Disable memory pinning (avoids GPU memory issues)\n)\n\n\n\n\nThanks to LoRA, we‚Äôll save enough memory that we can batch our data instead of giving one example at a time. But vision-language models like LLaVA require special handling when batching data because the default PyTorch collator doesn‚Äôt know how to handle our mixed inputs (images + text of varying lengths).\nSo, if we want to turn our batch size up to 8, we need to define a collator that says how to do that. Below, our custom collate_fn handles how to build batches from our mixed image/text, variable length data.\n\ndef collate_fn(features):\n    # features is a list of dicts, each with keys:\n    #   'input_ids', 'attention_mask', 'pixel_values', 'labels'\n\n    # 1) stack pixel_values (all same shape: 3√ó336√ó336)\n    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n\n    # 2) gather input_ids & attention_mask for text, pad them to same length\n    batch_text = {\n        \"input_ids\":      [f[\"input_ids\"] for f in features],\n        \"attention_mask\": [f[\"attention_mask\"] for f in features],\n    }\n    batch_text = processor.tokenizer.pad(\n        batch_text,\n        padding=\"longest\",\n        return_tensors=\"pt\"\n    )\n\n    # 3) labels: pad/truncate to same as input_ids\n    #    if you already have f[\"labels\"], you can pad those directly:\n    batch_labels = processor.tokenizer.pad(\n        {\"input_ids\": [f[\"labels\"] for f in features]},\n        padding=\"longest\",\n        return_tensors=\"pt\"\n    )[\"input_ids\"]\n\n    # 4) assemble final batch\n    batch = {\n        \"pixel_values\": pixel_values,\n        \"input_ids\":    batch_text[\"input_ids\"],\n        \"attention_mask\": batch_text[\"attention_mask\"],\n        \"labels\":       batch_labels,\n    }\n    return batch\n\n\n\n\nWe re-use our model setup and a lot of the code above to do our training.\nA few things to note - We print out trainable_parameters, this tells us what percentage of the model is actually going to be trained this time. - We still freeze the vision tower to ensure that none of it‚Äôs layers that matched our target_modules are actually trained, for the same reasons we discussed above. - As before, we‚Äôll see one example before/after training.\n\n# üöÄ TinyLLaVA + LoRA fine-tuning on Galaxy Zoo 2\n# ------------------------------------------------\nprint(\"üöÄ Starting TinyLLaVA LoRA fine-tuning‚Ä¶\")\nprint(\"=\" * 30)\n\nfrom peft import prepare_model_for_kbit_training\n\n# 1. base weights + processor\nmodel, processor = setup_tiny_llava()   # our existing helper\n\n# 2. attach LoRA adapters (only proj/FFN layers train)\nmodel = get_peft_model(model, lora_cfg)\n\n# 3. freeze **only** the vision‚Äêtower LoRA weights\nfor name, param in model.named_parameters():\n    if \"vision_tower\" in name and \"lora_\" in name:\n        param.requires_grad = False\n\nmodel.print_trainable_parameters()\n\n# 4. make dataset\ndataset = GZ2LLaVADataset(\"gz2_llava_hf/gz2_llava.jsonl\", processor)\n\nval_size = int(0.1 * len(dataset)) # set validation size - we'll just do 10%\ntrain_size = len(dataset) - val_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\nprint(f\"Split: {len(train_dataset)} train, {len(val_dataset)} validation\")\n\n# ------Test before training -----\nprint(\"\\nTesting BEFORE training:\")\nsample_item = dataset.data[0]\ntest_image = f\"{sample_item['image']}\"\nbefore_response = test_model(model, processor, test_image)\n\nplt.figure(figsize=(8, 6))\nimg = Image.open(test_image)\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\nprint(f\"BEFORE: {before_response}\")\n# -------------------------------\n\n# 5. trainer (reduce LR for LoRA)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer = processor.tokenizer, # Trainer expects ‚Äútokenizer‚Äù\n    data_collator=collate_fn,\n)\n\nprint(\"\\nStarting training...\")\nstart_time = time.time()\ntrainer.train()\nlora_training_time = time.time() - start_time\nprint(f\"‚úì Training completed in {lora_training_time:.1f} seconds ({lora_training_time/60:.1f} minutes)\")\n\n# 6. test after\nprint(\"\\nTesting AFTER training:\")\nprint(\"AFTER:\", test_model(model, processor, test_image))\n\n# 7. save lightweight adapters (~40 MB)\nmodel.save_pretrained(\"tinyllava_gz2_lora\")\nprint(\"Adapters saved to tinyllava_gz2_lora/\")\n\nüöÄ Starting TinyLLaVA LoRA fine-tuning‚Ä¶\n==============================\nSetting up TinyLLaVA...\n\n\n\n\n\n‚úì Fixed patch_size\n‚úì Model and processor ready\ntrainable params: 12,615,680 || all params: 1,425,088,512 || trainable%: 0.8853\nSplit: 1800 train, 200 validation\n\nTesting BEFORE training:\n\n\n\n\n\n\n\n\n\n/tmp/ipython-input-1885621431.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n\n\nBEFORE: The image is a close-up view of a star in the middle of the night sky. The star appears to be glowing brightly,\n\nStarting training...\n\n\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [675/675 20:28, Epoch 3/3]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n200\n1.861200\n\n\n400\n1.457400\n\n\n600\n1.410500\n\n\n\n\n\n\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n\nExample 2:\n  Total tokens: 613\n  Caption tokens to learn: 18\n  Caption text: This is an image of an elliptical galaxy, highly e...\n\nExample 1:\n  Total tokens: 609\n  Caption tokens to learn: 14\n  Caption text: This is an image of a spiral galaxy, with two arms...\n\nExample 0:\n  Total tokens: 618\n  Caption tokens to learn: 23\n  Caption text: This is an image of an elliptical galaxy, nearly r...\n‚úì Training completed in 1231.3 seconds (20.5 minutes)\n\nTesting AFTER training:\nAFTER: This is an image of an elliptical galaxy, nearly round in shape, prominent bulge at its center, disturbed or merging with\nAdapters saved to tinyllava_gz2_lora/\n\n\n\n# (OPTIONAL), a trainer can always be trained further by running this again!\n#trainer.train()\n\nAgain, re-run this cell to see performance on a random example.\n\nprint('-------------FROM TRAINING-------------------')\n# Get a random index from the training subset\ntrain_idx = np.random.randint(len(train_dataset))\nactual_idx = train_dataset.indices[train_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig1 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\nprint('\\n------------FROM VALIDATION------------------')\n# Get a random index from the validation subset\nval_idx = np.random.randint(len(val_dataset))\nactual_idx = val_dataset.indices[val_idx]  # Get the actual index in the original dataset\nsample = dataset.data[actual_idx]  # Use the original dataset\nimg_path = sample[\"image\"]\n\nfig2 = plt.figure(figsize=(8, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.axis('off')\n\nprint(\"TRUTH:\", sample[\"text\"])\nprint(\"PREDICTED:\", test_model(model, processor, img_path))\n\n-------------FROM TRAINING-------------------\nTRUTH: This is an image of a spiral galaxy, prominent bulge at its center.\nPREDICTED: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, disturbed or\n\n------------FROM VALIDATION------------------\nTRUTH: This is an image of an elliptical galaxy, nearly round in shape.\nPREDICTED: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, boxy bul\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"\\n‚è±Ô∏è  Training Time Comparison:\")\nprint(f\"   Full fine-tuning: {full_training_time:.1f}s\")\nprint(f\"   LoRA fine-tuning: {lora_training_time:.1f}s\")\nprint(f\"   Speedup: {full_training_time/lora_training_time:.1f}x faster with LoRA\")\n\n\n‚è±Ô∏è  Training Time Comparison:\n   Full fine-tuning: 2216.3s\n   LoRA fine-tuning: 1231.3s\n   Speedup: 1.8x faster with LoRA\n\n\nOk. You might be thinking - ‚Äú2x speedup? Who cares!‚Äù because in this notebook, using LoRA only got us an about that. But remember that we‚Äôre working here with a tiny model, and a tiny amount of data. The real benefits of LoRA become apparent when:\n\nWorking with larger models (7B, 13B parameters) where it can be 10x+ faster\nDeploying on limited hardware where memory is constrained (remeber, here, we saw a batch size of 1 on an A100 with full fine tuning, so our ‚Äúbeefy‚Äù GPU was already barely able to handle this task).\nIterating quickly on experiments - even 20 minutes vs 40 means is huge when running a model over and over again to make tweaks.\n\n\n\n\n\nAnd that‚Äôs it! We‚Äôve sucessfully trained a TinyLLaVA both with full fine tuning and using LoRA. And we definitely see learning, in both cases!\n\n\nFine-tuning a specialized model is never simply ‚Äúthrow data at the model and hope‚Äù.\n\nUnderstanding Data Flow is Critical.\n\n\nWe trace exactly how our images become tokens (336√ó336 ‚Üí 576 patches ‚Üí 576 tokens) to understand how information flows\nMasking required understanding where image tokens end and text begins\nWithout this knowledge, we‚Äôd train on the wrong tokens and get garbage results\n\n\nArchitecture Knowledge Matters.\n\n\nWe froze the vision encoder because it already understands images well\nWe trained the projection layer because that‚Äôs where image‚Üílanguage translation happens\nChoosing LoRA target modules required knowing which layers affect generation quality\n\n\nData Structure Shapes Everything. We had to carefully structure our data:\n\n\"USER: &lt;image&gt;\\nDescribe this galaxy.\\nASSISTANT: [actual description]\"\n          ‚Üë                                           ‚Üë\n    Image goes here                          Only train on this part\nThe model‚Äôs behavior completely depends on these formatting decisions.\n\nDebugging Requires Deep Understanding. It‚Äôs really common to get nonsense in your first stab at training a model. Debugging to get good results is often not ‚ÄúI have an error‚Äù, but rather ‚Äúthis doesn‚Äôt work as well as I thought‚Äù - and that sort of debugging benefits from a human overseeing the process.\n\n\n\n\n1. The Power of Shared Representation Spaces\nWhat we‚Äôve done with LLaVA reveals a profound principle: different types of information can be projected into a shared ‚Äúthinking space.‚Äù\nThis simple projection is surprisingly deep:\nimage_features (1024d) ‚Üí projection layer ‚Üí language space (2048d)\nWhile it‚Äôs simple in structure, it works because both vision and language models learned rich, compositional representations, so the projection layer just needs to learn to translate between these representation spaces. Once in language space, images become ‚Äújust another type of token‚Äù.\n2. Fusion is a General Principle\nThe fusion mechanism we studied isn‚Äôt limited to images + text, and could be applied to:\n\nAudio: Whisper embeddings ‚Üí projection ‚Üí LLM space\nVideo: Frame embeddings + temporal encoding ‚Üí projection ‚Üí LLM space\nGeneralized scientific data: Spectra/time series ‚Üí specialized encoder ‚Üí projection ‚Üí LLM space\n\nThe pattern is universal, and there‚Äôs no reason multiple of these can‚Äôt happen at once, i.e - image ‚Üí image projection ‚Üí LLM space image tokens, - audio ‚Üí audio projection ‚Üí LLM space audio tokens, - LLM space image tokens + LLM space audio tokens + text tokens ‚Üí LLM\n3. Toward Truly Universal Models These architectures get us closer to a future where:\n\nUniversal tokenization: All modalities become standardized tokens, and any decoding task can accept those tokens\nShared architectures: One model type processes everything\nEmergent translation: Models learn to translate between any modalities they‚Äôve seen\n\nWe‚Äôre not fully there yet. But for now, thoughtful engineering of fusion mechanisms - like we did today - remains essential."
  },
  {
    "objectID": "posts/PEFT-LoRA/PEFT_LoRA.html#the-background",
    "href": "posts/PEFT-LoRA/PEFT_LoRA.html#the-background",
    "title": "PEFT Deep Dive: LoRA",
    "section": "The Background",
    "text": "The Background\nLoRA is based on a simple but powerful insight: when we fine-tune a pre-trained model, the weight updates often have low ‚Äúintrinsic rank‚Äù.\nThat is to say, how many independent directions of change are actually meaningful in any given update matrix \\(\\Delta W = W_{fine-tuned} - W_{original}\\) is small. Even when \\(\\Delta W\\) might be a huge matrix (say 4096√ó4096), most of the meaningful changes during fine tuning are captured by much fewer dimensions.\n\nSo, where did this idea come from?\n\n1. Prior Theoretical Work on Intrinsic Dimensionality\nBefore LoRA, several papers established that neural networks often have much lower ‚Äúintrinsic dimensionality‚Äù than their parameter count suggests. In particular, Li et al.¬†(2018) and Aghajanyan et al.¬†(2020) both invesigated training smaller subspaces of these models and found success adapting models therein, showing that the effective number of parameters needed for adaptation might be much smaller than the total parameter count.\n\n\n2. The Key Empirical Discovery\nThe LoRA authors (Hu et al., 2021) took this insight and attempted to measure the intrinsic rank of the weight updates during fine-tuning.\nTo do this, they: - Fine-tuned GPT-3 on various tasks using full fine-tuning - Computed the weight update matrices \\(\\Delta W = W_{fine-tuned} - W_{original}\\) - Performed Singular Value Decomposition (SVD) on these update matrices and analyzed how the values were distributed\n\n\n3. The Findings\nWhat they found was that the weight update matrices had very low effective ranks. In fact,\n\nMost singular values were tiny (close to zero)\nOnly a small number of singular values (often &lt; 100) contained most of the ‚Äúsignal‚Äù\n\n(And it held across different model sizes, tasks, and layers)\nThis result wasn‚Äôt mathematically guaranteed nor obvious. Fine-tuning could theoretically require complex, high-dimensional change. And while over-parameterization suggested some redundancy, it was not known that that fine-tuning updates would be universally low-rank in a task, model, and optimization-independent way.\n\n\nThe Mathematical Intuition\nOf course, there are several fundamental reasons why this low-rank structure might emerge and make good mathematical sense.\n\n1. The Over-Parameterization Hypothesis\nIt‚Äôs been hypothesized that large language models are massively over-parameterized for any single task. This over-parameterization means that many parameters are redundant for the specific adaptation task, and changes to different parameters become highly correlated.\n\n\n2. Feature Reuse and Composition\nPre-trained models already contain rich, hierarchical feature representations. So fine-tuning typically can just rely on:\n\nReweighting existing features rather than learning new ones from scratch\nCombining existing patterns in new ways rather than creating entirely new patterns\nAdjusting decision boundaries rather than learning new feature detectors\n\nThis also means adaptations can be expressed as linear combinations of existing feature directions.\n\n\n3. Task Similarity\nMost fine-tuning tasks share underlying structure with the pre-training objective. For instance, any new language task will involve:\n\nlanguage understanding and generation\nuse of common syntactic and semantic patterns\n\nThe main difference is often in style or domain rather than fundamental capabilities, so the adaptation primarily involves adjusting the ‚Äúmixing weights‚Äù of existing capabilities rather than learning entirely new ones.\n\n\n4. Gradient Flow and Optimization Dynamics\nDuring fine-tuning, gradients tend to flow along the directions that were already ‚Äúactivated‚Äù during pre-training, meaning that:\n\nParameters that were important for pre-training are more likely to be updated\nUpdates tend to be correlated across layers (if one layer needs to change, related layers need complementary changes)\n\nThis type of optimization will naturally find low-dimensional paths through the parameter space.\n\n\n\nThe Core Mathematical Idea\nNow that we understand why updates might need to only be low-rank, let‚Äôs talk about how we do low-rank updated in practice.\nInstead of updating the full weight matrix \\(W \\in \\mathbb{R}^{d \\times k}\\) during fine-tuning, LoRA represents the update as:\n\\[h = W_0 x + \\Delta W x = W_0 x + BA x\\]\nHere: - \\(W_0\\) is the original frozen pre-trained weight matrix - \\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\) are trainable low-rank matrices - \\(r \\ll \\min(d,k)\\) is the rank of these matrices (typically 1-256) - \\(\\Delta W = BA\\) represents the low-rank update\n\nRemember, if B is a {4096 x 4} matrix, and A is a {4 x 4096} matrix (rank 4), the size of the update to W is still {4096 x 4096} !\n\n\nWith this update, the number of trainable parameters drops from \\(d \\times k\\) to \\((d + k) \\times r\\)\nFor a typical transformer layer with \\(d=k=4096\\) and \\(r=64\\), that means: - Full fine-tuning: \\(4096 \\times 4096 = 16.8M\\) parameters - LoRA: \\((4096 + 4096) \\times 64 = 524K\\) parameters\n\n-&gt; 97% fewer parameters!\n\n\n\nInitialization Strategy (Critical!)\nLoRA uses a specific initialization strategy that‚Äôs crucial for training stability. Let‚Äôs dive a little deeper into what this initialization is, and why it‚Äôs so important for stable training.\nThe Initialization: - Matrix \\(A\\) is initialized with small random Gaussian values: \\(A \\sim N(0, \\sigma^2)\\) - Matrix \\(B\\) is initialized to zero: \\(B = 0\\)\nThis ensures \\(\\Delta W = BA = 0\\) at initialization, meaning that the model starts at its exact baseline performance. But at least one of them must be non-zero to start, or you‚Äôd never get any gradients.\nThe scaling factor Œ±/r:\n\\[h = W_0 x + \\frac{\\alpha}{r} BA x\\]\nThis scaling really serves to give us control over the magnitude: \\(\\alpha\\) lets you control how ‚Äústrong‚Äù the adaptation is - \\(\\alpha = 0\\): No adaptation (just original model) - \\(\\alpha &gt;&gt; r\\): Strong adaptation (LoRA dominates) - \\(\\alpha \\approx r\\): Balanced adaptation (most common)\nSo, we get more stable training because: - Training starts with the proven pre-trained model behavior - LoRA gradually ‚Äúgrows in‚Äù as B learns non-zero values - The scaling prevents wild swings in model behavior early in training"
  },
  {
    "objectID": "posts/PEFT-LoRA/PEFT_LoRA.html#putting-it-into-practice",
    "href": "posts/PEFT-LoRA/PEFT_LoRA.html#putting-it-into-practice",
    "title": "PEFT Deep Dive: LoRA",
    "section": "Putting it into Practice",
    "text": "Putting it into Practice\nNow, let‚Äôs look at how we build LoRA into model training in a practical sense. Let‚Äôs start by seeing what building a LoRA layer would look like.\n\nclass LoRALayer(nn.Module):\n    \"\"\"\n    A Low-Rank Adaptation layer that can wrap any linear layer.\n\n    This implementation shows the core LoRA concept:\n    - Keep original weights frozen\n    - Add low-rank adaptation via two smaller matrices\n    - Apply scaling to control adaptation strength\n    \"\"\"\n\n    def __init__(self,\n                 original_layer: nn.Linear,\n                 rank: int = 4,\n                 alpha: float = 32.0,\n                 dropout: float = 0.1):\n        super().__init__()\n\n        self.rank = rank\n        self.alpha = alpha\n        self.scaling = alpha / rank  # This is the Œ±/r scaling factor\n\n        # Get dimensions from the original layer\n        in_features = original_layer.in_features\n        out_features = original_layer.out_features\n\n        # Freeze the original layer\n        self.original_layer = original_layer\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n\n        # Create the low-rank matrices A and B\n        # A: (rank, in_features) - initialized with small random values\n        # B: (out_features, rank) - initialized to zero\n        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n\n        # Optional dropout for regularization\n        self.dropout = nn.Dropout(dropout) if dropout &gt; 0 else nn.Identity()\n\n    def forward(self, x):\n        # Original computation: W‚ÇÄx\n        original_output = self.original_layer(x)\n\n        # LoRA computation: (Œ±/r) * B * A * x\n        # We compute this as: (Œ±/r) * B * (A * x) for efficiency\n        lora_output = self.dropout(x) @ self.lora_A.T  # (batch, rank)\n        lora_output = lora_output @ self.lora_B.T      # (batch, out_features)\n        lora_output = lora_output * self.scaling       # Apply Œ±/r scaling\n\n        # Combine: W‚ÇÄx + (Œ±/r)BAx\n        return original_output + lora_output\n\n    def get_delta_weights(self):\n        \"\"\"\n        Returns the actual ŒîW = (Œ±/r)BA matrix for analysis\n        \"\"\"\n        return self.scaling * (self.lora_B @ self.lora_A)\n\n# Let's test this with a simple example\nprint(\"Creating a test linear layer and its LoRA version...\")\nprint(\"=\" *50)\n\n# Original linear layer\noriginal = nn.Linear(256, 256)\nprint(f\"Original layer parameters: {sum(p.numel() for p in original.parameters()):,}\")\n\n# LoRA version\nlora_layer = LoRALayer(original, rank=16, alpha=32)\ntrainable_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in lora_layer.parameters())\n\n# Let's see the actual matrices A and B\nprint(\"Matrix A shape (rank √ó in_features):\", lora_layer.lora_A.shape)\nprint(\"Matrix B shape (out_features √ó rank):\", lora_layer.lora_B.shape)\n\nprint(f\"LoRA trainable parameters: {trainable_params:,}\")\nprint(f\"LoRA total parameters: {total_params:,}\")\nprint(f\"Parameter reduction: {(1 - trainable_params/total_params)*100:.1f}%\")\n\n# Test forward pass\nx = torch.randn(256, 256)  # batch_size=32, input_dim=512\noutput = lora_layer(x)\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\nCreating a test linear layer and its LoRA version...\n==================================================\nOriginal layer parameters: 65,792\nMatrix A shape (rank √ó in_features): torch.Size([16, 256])\nMatrix B shape (out_features √ó rank): torch.Size([256, 16])\nLoRA trainable parameters: 8,192\nLoRA total parameters: 73,984\nParameter reduction: 88.9%\nInput shape: torch.Size([256, 256])\nOutput shape: torch.Size([256, 256])\n\n\n\nLoRA In Real Applications\nIn practice, when training a model, you‚Äôd make use of something like the PEFT library from huggingface (https://huggingface.co/docs/peft/en/index), which allows you to just specify a simple LoRA configuration and apply it to a model you want to train, and it handles the rest for you!\n\n\nChoosing Your LoRA Config\nGiven that, a lot of what you need to do is make decisons around the LoRA configuration that you‚Äôll use. One of the key choices there is what weights you‚Äôll apply LoRA to.\nIn a transformer layer, let‚Äôs look at the candidates.\n\nfrom IPython.display import Image as iImage, display\ndisplay(iImage(\"images/transformer_layer_weights.png\", width=1000))\n\n\n\n\n\n\n\n\nLoRA is most typically applied to those weights that are part of the attention calculation (but we‚Äôll talk about that in more detail below).\nThe other thing to specify is the lora rank and alpha. For the example below, we‚Äôll choose those to be fairly stanard (16 and 32), but again, we discuss in more detail below what goes into making this choice.\n\n# Let's load a small model for demonstration\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# Load a small model (we'll use a tiny one for demo purposes)\nmodel_name = \"distilgpt2\"  # Small model that loads quickly\nprint(f\"üì¶ Loading {model_name}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Add padding token if it doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Original model parameters: {model.num_parameters():,}\")\n\n# Configure LoRA - this is where you specify what we just implemented!\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # Type of task\n    r=16,                          # Rank (our 'rank' parameter)\n    lora_alpha=32,                 # Scaling factor (our 'alpha' parameter)\n    lora_dropout=0.1,              # Dropout for regularization\n    target_modules=[\"c_attn\", \"c_proj\"],  # Which layers to apply LoRA to\n    bias=\"none\",                   # How to handle bias terms\n)\n\n# Apply LoRA to the model\nlora_model = get_peft_model(model, lora_config)\n\n# Check the parameter reduction\ntrainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in lora_model.parameters())\n\n# Show which modules were modified\nprint(f\"\\nüéØ LoRA was applied to these modules:\")\nlora_model.print_trainable_parameters()\n\nüì¶ Loading distilgpt2...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n\n\nOriginal model parameters: 81,912,576\n\nüéØ LoRA was applied to these modules:\ntrainable params: 811,008 || all params: 82,723,584 || trainable%: 0.9804\n\n\n\n\n\nChoosing Parameters\nIn a LoRA application, there are decisions to be made for the rank, alpha, layers to apply to, etc. So let‚Äôs talk about how these are typically chosen.\n\n\nWhich Layers?\nLoRA can theoretically be applied to any weight matrix in the model. But in practice, it‚Äôs most typically applied to the weights of the matrices that make up the attention calculation. Studies have shown that seems to be where it‚Äôs most useful, especially for task adaptations.\nMOST EFFECTIVE (apply first): - Query (q_proj) and Value (v_proj) projection matrices - These capture the most important attention patterns - Typically gives 80% of the benefit with minimal parameters\nOFTEN HELPFUL (if you have more compute budget): - Key projection (k_proj) matrices - Output projection (o_proj) after attention\n- First linear layer in feed-forward networks\nSOMETIMES USEFUL (advanced cases): - Second feed-forward layer - All linear layers (if you have lots of compute)\nUSUALLY SKIPPED: - Embedding layers (too sensitive) - Layer norms (few parameters anyway) - Final classification heads (task-specific, need full updates)\n\n\n\nWhat Rank (\\(r\\))?\nAs a rule of thumb, the rank to choose for LoRA depends on the size of the weight matrices of the model, which normally depends on the number of parameters: - Start with r=8-16 for small models (&lt;1B params) - Use r=16-64 for medium models (1B-10B params) - Try r=64-256 for large models (&gt;10B params)\nOf course, task complexity also matters, with more complicated task adaptations often needing higher rank.\nIn general, a systematic approach, where you start small (r=8) and measure performance, doubling the rank until performance plateaus is one of the best ways to determine ideal rank. The ‚Äúknee‚Äù of this curve is usually optimal.\n\n\n\nWhat Alpha (\\(\\alpha\\))?\nThe \\(\\alpha / r\\) ratio matters, so usually the \\(\\alpha\\) is chosen with the additonal consideration of what this ratio would be, leading to: - Œ±/r = 1: Mild adaptation (good for similar tasks) - Œ±/r = 2: Standard adaptation (most common) - Œ±/r = 4+: Strong adaptation (very different tasks)\nThis means that typical \\(\\alpha\\) values are one of these multiples of rank. Most commonly, the desired \\(\\alpha / r\\) is chosen, then \\(\\alpha\\) adjusts based on the chosen rank. It‚Äôs common to just keep \\(\\alpha / r\\) ~ 2, adjusting as r is changed.\n\n\n\nOther Important Settings\nThe LoraConfig takes additional parameters that are commonly assigned, and some additional hyperparameters can help guide LoRA training. 1. Dropout can be used on the LoRA matrices, with typical values being low, 0.0-0.1 for stable tasks. Higher dropout can be used for more regularization.\n\nThe Learning Rate in fine tuning with LoRA can typically be ~10x higher than full fine-tuning.\n\n\nLoRA parameters start from zero, so they need stronger signal\ni.e you might try 1e-4 ~ 5e-4 for LoRA, vs 1e-5 for full fine-tuning"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Tutorials Blog!",
    "section": "",
    "text": "During my Ph.D., I utilized a U-Net to attempt to learn the final distribution of dark matter in a computer simulation from its initial conditions. Being in an Astrophysics Ph.D.¬†program, and the year being 2020, model architectures like the U-Net weren‚Äôt well known by people in my field, including my Ph.D.¬†advisor. He tasked me: teach him what it was, down to the details, so that he could understand the concepts at a deep enough level himself to help properly guide me on the project.\n\n\n\nRepresentation of a dark matter only simulation - the subject of my Ph.D.¬†project\n\n\nAnd thus, my first Jupyter notebook tutorial was born. In the process of writing code cells, making graphics in powerpoint, and scouring through documentation, I found an even deeper understanding of the concepts I was describing. As they say, the best way to prove you really know something is to teach it. Since then, every time I really want to understand something, I make a Jupyter notebook tutorial about it.\nIt occured to me (5 years later), that maybe someone else would care to read these. So here, you‚Äôll find a collection of the notebooks that I‚Äôve put together. Mainly to teach myself, but hopefully, to teach you too!"
  },
  {
    "objectID": "posts/UNet-Tutorial/UNet_Tutorial.html",
    "href": "posts/UNet-Tutorial/UNet_Tutorial.html",
    "title": "The U-Net Architecture",
    "section": "",
    "text": "The U-Net\n\nIn this notebook, we take a detailed look at the U-Net architecture: a CNN architecture that‚Äôs utilized in a number of applications for image reconstruction. Utilizing skip connections to pass information between ‚Äúarms‚Äù, this encoder-decoder architecture remains highly relevant in a number of domains!\n\n  \n0. Pre-Tutorial \nBefore we start, let‚Äôs check that the GPU is ready to go if we have one, and import packages that we‚Äôll need, and talk about the motivations behind using a U-Net\n 0.1 Some Setup\nWe‚Äôll start by importing the packages we‚Äôll need, and define a string which is just the path to the folder that contains this notebook as well as all of the other images/data needed to run it (should be in folders as they are on github).\n\n!pip install -q keras\nimport numpy as np \nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as K\nimport tensorflow as tf\n\nfrom PIL import Image\n\nfrom matplotlib import animation\nfrom IPython.display import display, HTML\nfrom IPython.display import Image as Im\n\n#filepath = '/content/gdrive/My Drive/UNet_Tutorial'   # what my path is on Google Drive\nfilepath = '.'    # what my path is when I'm on local, for example, because the notebook is in the same directory \n                  # as all of the directories with the data/images (same file strucutre as on github)\n\n 0.2 What is a UNet?\nYou might be wondering: what exactly is a UNet? Why would I want to use one? How is it different from other convolutional neural nets? So we‚Äôll start by giving a bit of motivation for why UNets are so useful.\nLet‚Äôs say we have an image.\n\ndisplay(Im('%s/data/test_input.png' %filepath, height=270, width=270))\n\n\n\n\n\n\n\n\nA ‚Äúnormal‚Äù CNN task might be to say ‚Äúthat‚Äôs a stomach‚Äù given that the image could have been from a stomach, brain, or skin. (Full disclosure: I don‚Äôt know what this image is of, but it‚Äôs a medical image of some sort).\nLet‚Äôs say instead though, that we want this:\n\ndisplay(Im('%s/data/test_output.png' %filepath, height=270, width=270))\n\n\n\n\n\n\n\n\nThat‚Äôs not a single class output: it‚Äôs another image. And ‚Äúnormal‚Äù CNN‚Äôs don‚Äôt give an image from an image, they collapse an image into one number or one set of numbers. Enter the UNet. Fundamentally, it‚Äôs a CNN that‚Äôs architecture is such that you get an image back out of the same size as the input image.\nOne way to think about this is really as pixel-by-pixel classification: in the above example, we‚Äôre deciding whether each pixel should be assigned black or white. But as you‚Äôll see, a UNet is more generalizable than that, and the final image doesn‚Äôt necessarily need to correspond to pixel classifications.\n 1. How Convolutions Work\nUNets are a type of convolutional neural network (CNN), so understanding how convolutions work is fundamental to understanding how these networks work. In this section, we briefly go over how to perform convolutions and the building blocks of a convolutional layer in a CNN. While I cover all the basics here, this is meant as more of a refresher and assumes you have previously seen how a convolutional network works (but may have forgotten the details since).\n 1.1 The Convolution Operation\nFirst, we need to cover exactly what a ‚Äúconvolution‚Äù means. The building blocks of convolutions are essentially dot products over matricies - we multiply the values in a matrix by the corresponding values in another matrix, then add the values together to get a number.\nLet‚Äôs define matricies A and B, then take the dot product between them.\n\na = np.array([[1,1,1],[0,0,0],[1,1,1]])\nb = np.array([[3,2,3],[4,2,4],[3,3,3]])\n\n# to visualize the matricies\nfig = plt.figure(figsize=(10,5))\nax1, ax2 = fig.add_subplot(1,2,1), fig.add_subplot(1,2,2)\nax1.imshow(a, vmin=0, vmax=4, cmap=\"Greys\"), ax1.set_title(\"Matrix A\")\nax2.imshow(b, vmin=0, vmax=4, cmap=\"Greys\"), ax2.set_title(\"Matrix B\")\n\nprint(np.sum(a*b)) # since a/b are arrays, a*b is element-wise multiplication\n\n17\n\n\n\n\n\n\n\n\n\nNote that, this is different than doing matrix multiplication, which would result in another matrix, rather than just one number.\nReally, we want to think of this operation as giving us some linear combination of a matrix. If we want a linear combination of matrix X with values x1,x2,x3, ‚Ä¶ x9, we can define some matrix A with values a,b,c, ‚Ä¶ i, such that:\n\\[\\begin{equation*}\ny = \\begin{bmatrix}a&b&c\\\\d&e&f\\\\g&h&i\\end{bmatrix} * \\begin{bmatrix}x_1&x_2&x_3\\\\x_4&x_5&x_6\\\\x_7&x_8&x_9\\end{bmatrix} = \\\\ (a \\times x_1) + (b \\times x_2) + (c \\times x_3) + (d \\times x_4) + (e \\times x_5) + (f \\times x_6) + (g \\times x_7) + (h \\times x_8) + (i \\times x_9)\n\\end{equation*}\\]\nThen, we can interpret the values in matrix A (a,b,c, ‚Ä¶ i) as weights, each of which decides how strong the contribution from matrix X‚Äôs values (x1,x2,x3, ‚Ä¶ x9) should be.\n  1.2 Convolving an Image\nWhen we convolve an image, we simply perform this operation over and over again, on each pixel of an image. So, matrix A would be our matrix of weights, and matrix X is a matrix of pixel values, where the center value is our current pixel of interest. Convolving an image means performing this operation on every pixel of the image, then replacing its value with the one that is given by our matrix dot product. In this way, the image becomes another version of itself - one where each pixel is some linear combination of the pixels that were around it:\n\\[\\begin{equation*}\n\\color{red}{x_{5,new}} = \\begin{bmatrix}a&b&c\\\\d&e&f\\\\g&h&i\\end{bmatrix} \\times \\begin{bmatrix}x_1&x_2&x_3\\\\x_4&\\color{red}{x_5}&x_6\\\\x_7&x_8&x_9\\end{bmatrix} = \\\\ (a \\times x_1) + (b \\times x_2) + (c \\times x_3) + (d \\times x_4) + (e \\times x_5) + (f \\times x_6) + (g \\times x_7) + (h \\times x_8) + (i \\times x_9)\n\\end{equation*}\\]\nIn convolutional network applications, we typically call this matrix of weights a filter. In other applications that use convolutions, it may also be called a kernel.\nLet‚Äôs use matrix A from before, this time to convolve a simple, 5x5 image. The image we are going to convolve is shown below:\n\nim = np.array([[0,3,6,2,3],\n               [2,5,6,3,1],\n               [1,2,0,0,3],\n               [0,5,6,4,4],\n               [4,3,3,4,3]])\n\n# show the image we'll convolve, and the filter we'll convolve it with\nfig = plt.figure(figsize=(10,5))\nax1, ax2 = fig.add_subplot(1,2,1), fig.add_subplot(1,2,2)\nax1.imshow(im, cmap=\"Greys\"), plt.title(\"Starting Image\")\nax2.imshow(a, cmap=\"Greys\",vmin=0,vmax=6), plt.title(\"Filter\")\n\nconved_im = np.zeros((3,3)) # we'll replace these as we get the new values\n\n\n\n\n\n\n\n\nThe first step is to take the 3x3 block in the upper left of our image, and multiply that by our weights:\n\nfig = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4 = fig.subplots(1,4)\nax1.imshow(im, cmap = \"Greys\"), ax1.set_title(\"Full Image\")\nax1.add_patch(matplotlib.patches.Rectangle((-.48,-.48),2.98,2.98,fill=False,color='red',lw=2)) #show region to convolve\n\nax2.imshow(im[0:3,0:3],cmap=\"Greys\"), ax2.set_title(\"Region Around Pixel\")\nax3.imshow(im[0:3,0:3]*a,cmap=\"Greys\",vmin=0,vmax=6), ax3.set_title(\"Filter x Region\")\n\nconved_im[0][0] = np.sum(im[0:3,0:3]*a)\nax4.imshow(conved_im,cmap=\"Greys\",vmin=0,vmax=29), ax4.set_title(\"Convolved Image\") #here, I prematurely set vmax to what the maximum of conved_im\nplt.annotate(\"only one pixel in\", (.55,.4))                                         #will be, otherwise scaling will change as it plots\nplt.annotate(\"new image so far\", (.55,.6))\n\nText(0.55, 0.6, 'new image so far')\n\n\n\n\n\n\n\n\n\nWe can see, that because our filter was a row of ones, a row of zeros, then another row of ones, when we apply this filter to our region, the middle row becomes zeros, while the top and bottom rows of the region are unchanged. So, the sum of Filter X Region which creates our new pixel is really just the sum of the top and bottom rows of the region.\nNext, let‚Äôs move over one pixel, and do the same thing:\n\nfig = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4 = fig.subplots(1,4)\nax1.imshow(im, cmap = \"Greys\"), ax1.set_title(\"Full Image\")\nax1.add_patch(matplotlib.patches.Rectangle((.5,-.48),3,2.98,fill=False,color='red',lw=2)) #show region to convolve\n\nax2.imshow(im[0:3,1:4],cmap=\"Greys\", vmin=0, vmax=6), ax2.set_title(\"Region Around Pixel\")\nax3.imshow(im[0:3,1:4]*a,cmap=\"Greys\",vmin=0,vmax=6), ax3.set_title(\"Filter x Region\")\n\nconved_im[0][1] = np.sum(im[0:3,1:4]*a)\nax4.imshow(conved_im,cmap=\"Greys\",vmin=0,vmax=29), ax4.set_title(\"Convolved Image\") #here, I prematurely set vmax to what the maximum of conved_im\nplt.annotate(\"now, 2 pixels\", (.95,.7))                                             #will be, otherwise scaling will change as it plots\n\nText(0.95, 0.7, 'now, 2 pixels')\n\n\n\n\n\n\n\n\n\nWe can keep moving it over, and filling in the pixels of this ‚Äúnew version‚Äù of our image:\n\nfig = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4 = fig.subplots(1,4)\n\ndisplay_ims = []\nconved_im = np.zeros((3,3)) # reset this\nfor i in range(conved_im.shape[0]):\n  for j in range(conved_im.shape[1]):\n    im1 = ax1.imshow(im, cmap = \"Greys\",animated=True)\n    ax1.set_title(\"Full Image\")\n    im1 = ax1.add_patch(matplotlib.patches.Rectangle((-.48+j,-.48+i),3,3,fill=False,color='red',lw=2)) #show region to convolve\n\n    im2 = ax2.imshow(im[i:i+3,j:j+3],cmap=\"Greys\", vmin=0, vmax=6, animated=True)\n    ax2.set_title(\"Region Around Pixel\")\n    im3 = ax3.imshow(im[i:i+3,j:j+3]*a,cmap=\"Greys\",vmin=0,vmax=6, animated=True)\n    ax3.set_title(\"Filter x Region\")\n\n    conved_im[i][j] = np.sum(im[i:i+3,j:j+3]*a)\n    im4 = ax4.imshow(conved_im,cmap=\"Greys\",vmin=0,vmax=29, animated=True) #here, I prematurely set vmax to what the maximum of conved_im\n    ax4.set_title(\"Convolved Image\")                                       #will be, otherwise scaling will change as it plots\n\n    display_ims.append([im1, im2, im3, im4])\n\nani = animation.ArtistAnimation(fig, display_ims, interval=1000, blit=True, repeat_delay=1000)\nplt.close()\n\nHTML(ani.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n  1.3 Padding\nYou‚Äôll notice that this convolution reduced our image size: while we started with a 5x5 image, our convolved version was only 3x3. This is because we‚Äôre only able to fit a 3x3 filter onto a 5x5 image, 3x3 times. We aren‚Äôt able to make ‚Äúnew‚Äù pixels out of the ones on the border of the image - our filter can‚Äôt fit. For this reason, we usually pad images before we convolve them - or add values all around the border of the image. Padding ensures two important things:\n\nThat the image isn‚Äôt downsized by a convolution\nThat pixels on the outer edges ‚Äúcount‚Äù as much as pixels in the middle. That is - that they‚Äôre convolved over as many times as pixels closer to the center of the image.\n\nThere are many different choices for padding, each with their own unique advantages, but the most common/universal (and the only one we‚Äôll discuss here) is valid, zero padding. Valid means that we add whatever padding we need in order to keep the image the same size. Zero just means that the values we add along the borders are all zeros.\nIf we valid zero-pad the image we were just using, we would get:\n\npadded_im = np.pad(im, pad_width = (1,1), mode=\"constant\", constant_values=0)\n\n# show the image we'll convolve, and the filter we'll convolve it with\nfig = plt.figure(figsize=(10,5))\nax1, ax2 = fig.add_subplot(1,2,1), fig.add_subplot(1,2,2)\nax1.imshow(im, cmap=\"Greys\"), ax1.set_title(\"Original Image\")\nax2.imshow(padded_im, cmap=\"Greys\",vmin=0,vmax=6), ax2.set_title(\"Padded Image\")\n\n(&lt;matplotlib.image.AxesImage&gt;,\n Text(0.5, 1.0, 'Padded Image'))\n\n\n\n\n\n\n\n\n\nWe can see, if we re-do the convolution we did to this image in the last section, now with the padded image, that pixels on the edge of the image are also convolved over, and the image size is preserved:\n\nfig = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4 = fig.subplots(1,4)\n\ndisplay_ims = []\nconved_im = np.zeros((5,5)) # now, we'll have 5x5 pixels to fill in\nfor i in range(conved_im.shape[0]):\n  for j in range(conved_im.shape[1]):\n    im1 = ax1.imshow(padded_im, cmap = \"Greys\",animated=True)\n    ax1.set_title(\"Full Image\")\n    im1 = ax1.add_patch(matplotlib.patches.Rectangle((-.48+j,-.48+i),3,3,fill=False,color='red',lw=2)) #show region to convolve\n\n    im2 = ax2.imshow(padded_im[i:i+3,j:j+3],cmap=\"Greys\", vmin=0, vmax=6, animated=True)\n    ax2.set_title(\"Region Around Pixel\")\n    im3 = ax3.imshow(padded_im[i:i+3,j:j+3]*a,cmap=\"Greys\",vmin=0,vmax=6, animated=True)\n    ax3.set_title(\"Filter x Region\")\n\n    conved_im[i][j] = np.sum(padded_im[i:i+3,j:j+3]*a)\n    im4 = ax4.imshow(conved_im,cmap=\"Greys\",vmin=0,vmax=29, animated=True) #here, I prematurely set vmax to what the maximum of conved_im\n    ax4.set_title(\"Convolved Image\")                                       #will be, otherwise scaling will change as it plots\n\n    display_ims.append([im1, im2, im3, im4])\n\nani = animation.ArtistAnimation(fig, display_ims, interval=1000, blit=True, repeat_delay=1000)\nplt.close()\n\nHTML(ani.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nWe can see that the 3x3 region that we convolved before remains the same in both images, but when we pad the image, we also get information about the edge pixels that before we weren‚Äôt going over.\nValid padding means padding in order to maintain image size. In our example above, this means we just had to add a border of single-pixel width to our image. In general though, the size of the border you need to add will depend on a few parameters. The parameters that determine the size after you perform a convolution are:\n\nn: the size of the input image (assumed square, so it‚Äôs nxn)\nf: the size of the filter (assumed square, so it‚Äôs fxf)\ns: the stride, or how much you move the filter over before you do the next convolution. In our above examples, we‚Äôve always used s=1, but in general, s can be any number that will still make the filter fit evenly inside the image. We will assume that you use the same stride along all of the image dimensions.\np: the width of the padding to be added, assumed the same amount will be added all around the image.\n\nThen, the output dimension of the image will be:\n\\[\\begin{equation*}\nn_{out} \\times n_{out} = \\frac{n - f + 2p}{s} + 1\n\\end{equation*}\\]\nSo, depending on the image size, filter size, and stride selected, you can determine the width of the padding that will need to be added to keep \\(n_{out} = n\\).\n  1.4 Adding Bias and Activations\nBefore we go on, we need to talk about two other fundamental steps that happen after we convolve an image, which are the final building blocks of what happens in a convolutional layer of neural network. First, we need to talk about bias parameters, then, we‚Äôll talk about the activtion step.\nWe‚Äôve already talked about how we can intepret each step of a convolution as replacing a pixel with a linear combination of it and the pixels around it. But a classic linear combination has the format:\n\\[\\begin{equation*}\ny = m_1x_1 + m_2x_2 + m_3x_3 + ... + b\n\\end{equation*}\\]\nAnd so far, we haven‚Äôt added b. We refer to this extra parameter as the bias, it‚Äôs a single value that get‚Äôs added to every pixel of the image after the image has been convolved. You need a bias for sort of the same reason that you need b when you fit a line: because it can act to shift the entire image one way, in a way that otherwise can be impossible given just the values \\(\\times\\) weights.\nBeacuse the bias is just a single number added to every pixel, it‚Äôs a very simple augmentation of the image:\n\nbias = -10\nfig = plt.figure(figsize=(10,5))\nax1, ax2 = fig.add_subplot(1,2,1), fig.add_subplot(1,2,2)\nax1.imshow(conved_im, cmap=\"Greys\", vmin=-5, vmax=29), ax1.set_title(\"Convolved Image\")\nbiased_im = conved_im + bias\nax2.imshow(biased_im, cmap=\"Greys\", vmin=-5, vmax=29), ax2.set_title(\"Convolved Image + (b = %d)\" %bias)\n\n(&lt;matplotlib.image.AxesImage&gt;,\n Text(0.5, 1.0, 'Convolved Image + (b = -10)'))\n\n\n\n\n\n\n\n\n\nIn the above example, we can see that adding the bias dimmed the entire image.\nThe other step that happens after the convolution step in a convolutional layer is the activation. In the activation step, the image is subject to a function, so each pixel of the image is changed according to that function. In convolutional neural networks, the most common of these functions is ReLU (Rectified Linear Units), which looks like: \\[\\begin{equation*}\nReLU(x) = max(0,x)\n\\end{equation*}\\] So, when an image is passed through the ReLU activation, each pixel becomes either 0 (if the value was negative) or remains the same (if the value was 0 or positive). An image passed through this activation will look like:\n\nfig = plt.figure(figsize=(15,5))\nax1, ax2, ax3 = fig.add_subplot(1,3,1), fig.add_subplot(1,3,2), fig.add_subplot(1,3,3)\nax1.imshow(conved_im, cmap=\"Greys\", vmin=-5, vmax=29), ax1.set_title(\"Convolved Image\")\nax2.imshow(biased_im, cmap=\"Greys\", vmin=-5, vmax=29), ax2.set_title(\"Biased Image\")\nrelu_im = np.maximum(0,biased_im)\nax3.imshow(relu_im, cmap=\"Greys\", vmin=0, vmax=29), ax3.set_title(\"Relu-ed Image\")\n\n(&lt;matplotlib.image.AxesImage&gt;,\n Text(0.5, 1.0, 'Relu-ed Image'))\n\n\n\n\n\n\n\n\n\nWhen we added the bias to our image, some of our pixels became negative. That means that, after we applied the activation function, these pixels actually became zero-valued, meaning our final image now has some areas of whitespace that weren‚Äôt there before.\nIt may seem as though this activation function merely removes information: pixels that previously had value are now becoming zeroed-out, now lending us no information about the image. We won‚Äôt go into a detailed explanation as to why activation functions are so important (as well as an explanation of the advantages and disadvantages of different choices for the activation function), but there are many resources online that do a deep-dive into this topic. For now, I‚Äôll just give the main reasons why we include the activation step:\n\nDying gradients\nprevent weights from blowing up\n\n 2. Interpreting Filters\nThe point, really, of a UNet, is to learn the weights of the filters, and the biases, that transform an image and allow us to augment that image into another image. So, we may want to attempt to look at the filters and determine the ways that it might be transforming our image and helping to learn patterns.\n  2.1 The Horizontal Edge Detector\nSome filters, such as the horizontal edge detector are fairly easily intepretable in the ways that they tranform an image. We‚Äôll take a look at the horizontal edge detector below.\n\nhoriz_edge_filter = np.array([[ 1,  2,  1],\n                              [ 0,  0,  0],\n                              [-1, -2, -1]])\n\nplt.imshow(horiz_edge_filter, cmap = 'RdBu')\n\n\n\n\n\n\n\n\nThis filter is comprised of: a row of positive values, a row of zero values, and a row of negative values. It may not be immediately obvious how this can pick out horizontal edges, but consider the case of an image with a very simple horizontal edge:\n\nhoriz_edge_im = np.array([[ 1, 1, 1],\n                          [ 0, 0, 0],\n                          [ -1, -1, -1]])\n\nplt.imshow(horiz_edge_im, cmap = 'Greys')\n\n\n\n\n\n\n\n\nIf we convolve this image with this filter (that is, take the sum of the element-wise products of these two 3x3 matricies) we will get a single number:\n\nconved_val = np.sum(horiz_edge_filter*horiz_edge_im)\nprint(\"The new pixel value would be:\", conved_val)\n\nThe new pixel value would be: 8\n\n\nIf instead, this image had been of a vertical edge. Then, when we check what value the convolution gives us, we intead get:\n\nvert_edge_im = np.array([[ 1, 0, -1],\n                         [ 1, 0, -1],\n                         [ 1, 0, -1]])\n\nplt.imshow(vert_edge_im, cmap = 'Greys')\n\nconved_val = np.sum(horiz_edge_filter*vert_edge_im)\nprint(\"The new pixel value would be:\", conved_val)\n\nThe new pixel value would be: 0\n\n\n\n\n\n\n\n\n\nWe can see that the structure of the filter is that it‚Äôs symmetric along its horizontal axis. That is, the row of positive values is mirrored by a row of negative values at the bottom of the filter. This means that, any portion of an image which it is applied to, which is also symmetric along its horizontal axis, will give us a value of 0, because the positive and negatives will cancel out.\n\\[\\begin{equation*}\n\\begin{bmatrix}1&2&1\\\\0&0&0\\\\-1&-2&-1\\end{bmatrix} \\times \\begin{bmatrix}a&b&c\\\\a&b&c\\\\a&b&c\\end{bmatrix} = a + 2b + c + 0 -a -2b -c = 0\n\\end{equation*}\\]\nWhereas a portion of an image that changes values along its horizontal axis will give us a nonzero value.\nNote that, the values in the center row of the image never matter, because the center row of the filter is all zeros.\n  2.2 The Horizontal Edge Detector in Action\nYou may notice that an image like the one below, which you would identify as a horizontal line, will not get identified by this filter, because\n\nIt‚Äôs horizontally symmetric, and\nEvery element-wise multiplication includes a zero.\n\n\nhoriz_edge_im = np.array([[ 1, 1, 1],\n                          [ 0, 0, 0],\n                          [ 1, 1, 1]])\n\nplt.imshow(horiz_edge_im, cmap = 'Greys')\n\n\n\n\n\n\n\n\nBut, in practice, we apply these filters over a larger image, not over an image of matching size, so we‚Äôll see that single-pixel edges are still detected by this filter, just not when the edge is on the center pixel.\nLet‚Äôs start by loading in an image with some edges, which we‚Äôll pass our horizontal edge detector over.\n\nfrom PIL import Image\nim = np.array(Image.open('filepath/images/horiz_im.png'))[:,:,2]\nim = im/255\nim = np.round(im)\n\nim[35][0:20] = 1 #add a single-pixel-width edge, to see if we can detect that too\npadded_im = np.pad(im, pad_width = (1,1), mode=\"constant\", constant_values=0)\n\n\nplt.imshow(padded_im, cmap=\"Greys\")\n\n\n\n\n\n\n\n\nNow, if we convolve with our filter like we did in Part 1:\n**note, this code may take a little time to run*\n\nfig = plt.figure()\n\n# first, get the filter sizes which will help us later\nfilter_sz = horiz_edge_filter.shape[0]\nfilter_width = int(np.floor(filter_sz/2))\n\nconv_ims = []\nconved_im = padded_im.copy() # set it to a copy, that way we can watch the image transform\nfor i in range(1, im.shape[0]+1): # from 1-65 instead of 0-64, because we added the padding\n  for j in range(1,im.shape[0]+1):\n    # first, replace the pixel in the image with the convolved one\n    conved_region = padded_im[i-filter_width:i+filter_width+1,j-filter_width:j+filter_width+1]*horiz_edge_filter\n    conved_im[i,j] = np.sum(conved_region) # replace pixels of the copy with the convolution\n    # make an image where the filter is overlayed, too\n    filter_im = conved_im.copy()\n    filter_im[i-filter_width:i+filter_width+1,j-filter_width:j+filter_width+1] = conved_region\n    if (i&gt;12 and i&lt;18 and j&gt;10) or (i&gt;50 and i&lt;55 and j&gt;10) or (i&gt;33 and i&lt;37 and j&lt;20): # it would take too long to plot the whole movie, so just do interesting parts\n      conv_ims.append([plt.imshow(filter_im, animated=True, cmap = 'RdBu_r',vmin=-2,vmax=2)]) # with filter overlayed\n      conv_ims.append([plt.imshow(conved_im, animated=True, cmap = 'RdBu_r',vmin=-2,vmax=2)]) # convolved im result\n\nani = animation.ArtistAnimation(fig, conv_ims, interval=100, blit=True, repeat_delay=1000)\nplt.close()\n\nHTML(ani.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThere are a few important things to note about this output image:\n\nThat the output image contains both blue and red pixels - this filter is able to pick out not only the edge, but the direction the edge is going in - that is, higher valued to lower valued pixels or vice versa.\nThat the single-pixel width edge is detected by this filter - and is replaced with these two representations of the edge - where the edge created a border of low to high valued pixels, and where it creates a border of high to low valued pixels.\nThe perfectly vertical portions of the thick lines are ignored by the filter - but diagonal regions are still detected.\n\nIn A U-Net, we want to learn the filters that can transform our image. To do this, we usually must convolve an image with many different filters, with deeper layers applying filters to versions of the image which have already been convolved. So as you can probably guess, the filters of a real U-Net are usually not doing something as simple and interpretable as the horizontal edge detection.\nIn Section 4, we‚Äôll talk more about how to view and interpret the filters of a UNet trained on a real-world example, but for now, we‚Äôll take a break to establish exactly what the UNet is doing.\n 3. The Architecture of a UNet\nNow that we have some foundations about the convolutions that a UNet uses to learn how to transform an image, we‚Äôll take a closer look at architecture of a UNet which makes this possible.\n  3.1 The UNet Structure\nThe basic structure of a UNet looks like this:\n\ndisplay(Im('%s/images/UNet_Structure.png' %filepath, width=650, height=400))\n\n\n\n\n\n\n\n\nU-Nets are named as such because they have this U-like shape, where the input image is first reduced in dimensionality in the downsizing portion, then increased in dimensionality back to its original size in the upsizing portion. As you can see, there are 4 main types of operations, which we‚Äôll briefly describe here but give a detailed description of in the upcoming subsections:\n\nconvolutions: We have already discussed the convolution operation and components of a convolutional layer. In these convolutional portions, the image is convolved repeatedly, with differing (and often differently sized) filters.\npooling: The image is decreased in dimensionality, by representing regions of a few pixels of the image with only one pixel.\nup-convolutions: Sort of the opposite of pooling, one pixel is copied several times to become multiple pixels of the image, in order to increase the image dimensionality.\nconcatenations: An image from a previous part of the network is stacked with the image from the current part of the network\n\n  3.2 Convolutions and Convolution Blocks\nWe already talked about convolutions in Section 1. Here, we‚Äôll take a look at exactly what makes a convolutional layer, and how those layers stack to extract the information we want from our image.\nYou can use a filter to convolve an image. But usually in a convolutional network, we want to use many filters to convolve an image, because each filter is learning something different about the image. Additionally, we usually perform multiple convolutions in a row of the same number of filters/shape of filter, in what‚Äôs often called a convolutional block. Here, we might convolve our image with 5 3x3 filters, then take the result of that convolution, and convolve it with another 5 3x3 filters.\nSo what exactly is the output of a convolution, and what do we do when we have 5 of them?\nIf we start with an image, say 8x8, and we convolve it with a 3x3 filter, then provided that we valid padded it first, we get out a 8x8 image which is some version of the original:\n\nim = np.array([[0,1,1,2,4,4,3,2,2],  # the region of pixels\n               [4,3,2,4,5,5,4,3,1],\n               [0,1,1,4,5,4,3,2,2],\n               [2,2,1,3,5,3,0,1,2],\n               [3,2,1,1,2,3,4,2,2],\n               [4,3,2,1,0,1,3,3,2],\n               [3,2,1,2,1,1,4,4,5],\n               [2,1,1,2,1,3,5,6,7],\n               [1,0,0,2,1,4,6,8,8]])\n\nfilt1 = np.array([[-1,-1,0], # a filter I made up\n                  [-1,0,1],\n                  [0,1,1]])\n\nfrom scipy.ndimage import convolve    # a handy function that can do convolutions for us\nconv_im1 = convolve(im, filt1, mode = 'constant')  # set mode=constant for valid padding\n\nfig = plt.figure(figsize=(15,5))\nax1, ax2, ax3 = fig.subplots(1,3)\nax1.imshow(im, cmap='Greys'), ax1.set_title('Original Image')\nax2.imshow(filt1, cmap='Greys'), ax2.set_title('Filter')\nax3.imshow(conv_im1, cmap='Greys'), ax3.set_title('Convolved Image')\n\n(&lt;matplotlib.image.AxesImage&gt;,\n Text(0.5, 1.0, 'Convolved Image'))\n\n\n\n\n\n\n\n\n\nIf we have a second filter, then we have another version of the image which was convolved with that filter:\n\nfilt2 = np.array([[1,2,1],  # another filter I made up\n                  [2,3,2],\n                  [1,2,1]])\n\nconv_im2 = convolve(im, filt2, mode = 'constant')  # set mode=constant for valid padding\n\nfig = plt.figure(figsize=(15,5))\nax1, ax2, ax3 = fig.subplots(1,3)\nax1.imshow(im, cmap='Greys'), ax1.set_title('Original Image')\nax2.imshow(filt2, cmap='Greys'), ax2.set_title('Filter #2')\nax3.imshow(conv_im2, cmap='Greys'), ax3.set_title('Convolved Image #2')\n\n(&lt;matplotlib.image.AxesImage&gt;,\n Text(0.5, 1.0, 'Convolved Image #2'))\n\n\n\n\n\n\n\n\n\nIf we have 5 such filters, then we have 5 unique ‚Äúversions‚Äù of the original image:\n\nfilt3 = np.array([[1,2,1], [0,0,0], [-1,-2,-1]])    # more filter that I made up, not necessarily\nfilt4 = np.array([[0,1,0], [2,3,2], [-1,-2,-1]])    # ones that should do anything interesting\nfilt5 = np.array([[-1,-2,-1], [0,0,0], [1,2,1]])\n\nconv_im3 = convolve(im, filt3, mode = 'constant')\nconv_im4 = convolve(im, filt4, mode = 'constant')\nconv_im5 = convolve(im, filt5, mode = 'constant')\n\nfig = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4, ax5 = fig.subplots(1,5)\nax1.imshow(conv_im1, cmap='Greys'), ax1.set_title('Convolved Image #1')\nax2.imshow(conv_im2, cmap='Greys'), ax2.set_title('Convolved Image #2')\nax3.imshow(conv_im3, cmap='Greys'), ax3.set_title('Convolved Image #3')\nax4.imshow(conv_im4, cmap='Greys'), ax4.set_title('Convolved Image #4')\nax5.imshow(conv_im5, cmap='Greys'), ax5.set_title('Convolved Image #5')\n\n(&lt;matplotlib.image.AxesImage&gt;,\n Text(0.5, 1.0, 'Convolved Image #5'))\n\n\n\n\n\n\n\n\n\nNow, we have 5 representations of our original image, each with some unique features that were emphasized or de-emphasized because of the filter that created them. So, we want a way to keep all of this information that our filters gave us. But, we also want a way to be able to associate these versions of the image with one another. The dark pixels in the bottom right of all of these convolved images above, for example, are all some representation of the dark region in the lower right of our starting image. That is, the bottom right regions of our convolved images still correspond to and give us information about the bottom right region of our original image.\nSo, what we do is stack the images so that each one becomes a channel of one complete image. These channels are just like the RGB channels you might be used to in normal color images: each one contains some information about the image, and each matching pixel across different channels is telling you something about the same region of the image. That‚Äôs exactly what our convolved images are doing - they‚Äôre each telling us different pieces of information about the same regions of the original image.\nWhen we stack these convolved images into channels, we increase the depth of the image: our 8x8x(1 channel) original image is now an 8x8x(5 channel) image.\n\ndisplay(Im('%s/images/operation_examples/conv_example_im1.png' %filepath, height=370, width=370))\n\n\n\n\n\n\n\n\nNext, it‚Äôs typical to convolve our image a second time. Let‚Äôs say that our convolution block involves a second set of convolutions, where this time we want to use 3, 3x3 filters.\nYou might be wondering: how are we going to convolve an 8x8x5 image with a 3x3 filter? The answer is that our filters will now also need to have 5 channels, so really, we‚Äôll be using 3, 3x3x5(channel) filters.\nNote: I keep making this distinction that these third dimensions are channels. That‚Äôs because it‚Äôs an important distinction: convolutions can happen in 3D, too, so a 3x3x5 filter (not a 3x3x(5 channel)) filter actually would, in general, be a somewhat different operation, which I‚Äôll point out and talk about more in a little bit. This is why we would still refer to the second set of filters in this convolution block as 3x3 filters, instead of specifying that they have 5 channels. The number of channels is implied by the network architecture; if we were to call them 3x3x5 filters it would sound like we are doing 3D convolutions.\nWhen we convolve a multi-channel image with a multi-channel filter (always with the matching number of channels as the image), what we are effectively doing is convolving each channel of our image with its own filter, and then adding the results togeher. So, in the second part of our convolution block, where we have 3, 3x3x(5 channel) filters, it‚Äôs really like each filter gives us 5 versions of our image.\nLet‚Äôs look at what one of the filters is doing:\n\nfig = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4, ax5 = fig.subplots(1,5)\nax1.imshow(conv_im1, cmap='Greys'), ax1.set_title('Image Channel #1'), ax1.axis('off')\nax2.imshow(conv_im2, cmap='Greys'), ax2.set_title('Image Channel #2'), ax2.axis('off')\nax3.imshow(conv_im3, cmap='Greys'), ax3.set_title('Image Channel #3'), ax3.axis('off')\nax4.imshow(conv_im4, cmap='Greys'), ax4.set_title('Image Channel #4'), ax4.axis('off')\nax5.imshow(conv_im5, cmap='Greys'), ax5.set_title('Image Channel #5'), ax5.axis('off')\n\nfig2 = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4, ax5 = fig2.subplots(1,5)\n# Filter #1, 5 channels, each 3x3\nfilt1_ch1 = np.array([[0,-1,1],[-1,-1,1],[1,1,0]])\nax1.imshow(filt1_ch1, cmap='Greys'), ax1.axis('off'), ax1.set_title('Filter #1, Channel #1')\nfilt1_ch2 = np.array([[2,-1,2],[2,1,2],[2,0,2]])\nax2.imshow(filt1_ch2, cmap='Greys'), ax2.axis('off'), ax2.set_title('Filter #1, Channel #2')\nfilt1_ch3 = np.array([[-1,1,-1],[1,-1,1],[-2,-1,2]])\nax3.imshow(filt1_ch3, cmap='Greys'), ax3.axis('off'), ax3.set_title('Filter #1, Channel #3')\nfilt1_ch4 = np.array([[-1,-2,-1],[-1,-1,0],[2,-1,-2]])\nax4.imshow(filt1_ch4, cmap='Greys'), ax4.axis('off'), ax4.set_title('Filter #1, Channel #4')\nfilt1_ch5 = np.array([[-1,-1,1],[-2,-1,0],[2,-1,-2]])\nax5.imshow(filt1_ch5, cmap='Greys'), ax5.axis('off'), ax5.set_title('Filter #1, Channel #5')\n\nfig3 = plt.figure(figsize=(18,5))\nax1, ax2, ax3, ax4, ax5 = fig3.subplots(1,5)\n# Convolve each channel of the 8x8x5 image with the corresponding filter channel\nfilt1_ch1_convIm = convolve(conv_im1, filt1_ch1, mode = 'constant')\nax1.imshow(filt1_ch1_convIm, cmap='Greys'), ax1.axis('off'), ax1.set_title('Convolved Image Channel #1')\nfilt1_ch2_convIm = convolve(conv_im2, filt1_ch2, mode = 'constant')\nax2.imshow(filt1_ch2_convIm, cmap='Greys'), ax2.axis('off'), ax2.set_title('Convolved Image Channel #2')\nfilt1_ch3_convIm = convolve(conv_im3, filt1_ch3, mode = 'constant')\nax3.imshow(filt1_ch3_convIm, cmap='Greys'), ax3.axis('off'), ax3.set_title('Convolved Image Channel #3')\nfilt1_ch4_convIm = convolve(conv_im4, filt1_ch4, mode = 'constant')\nax4.imshow(filt1_ch4_convIm, cmap='Greys'), ax4.axis('off'), ax4.set_title('Convolved Image Channel #4')\nfilt1_ch5_convIm = convolve(conv_im5, filt1_ch5, mode = 'constant')\nax5.imshow(filt1_ch5_convIm, cmap='Greys'), ax5.axis('off'), ax5.set_title('Convolved Image Channel #5')\n\n(&lt;matplotlib.image.AxesImage&gt;,\n (-0.5, 8.5, 8.5, -0.5),\n Text(0.5, 1.0, 'Convolved Image Channel #5'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, we start with a 5-channel image, we convolve each channel with the corresponding filter of a 5-channel filter, and we get out 5 images.\nIf we have 3 of such filters, that would give us 3 (filters) x 5(channels) = 15 versions of our image. You might expect that we would stack all of these again, and end up with a 8x8x15 output from this convolution, but that‚Äôs not the case.\nAlthough we stack the outputs of our filters into channels, we actually add the channels of an image after it‚Äôs convolved. So really, the output of our first convolution, using Filter #1, a 3x3x(5 channel) filter on our 8x8x(5 channel) image is:\n\nfilt1_convOutput = filt1_ch1_convIm + filt1_ch2_convIm + filt1_ch3_convIm + filt1_ch4_convIm + filt1_ch5_convIm\n\nplt.imshow(filt1_convOutput, cmap=\"Greys\")\n\n\n\n\n\n\n\n\nSo, actually, if the second set of convolutions in our convolutional block had 3, 3x3 filters, the output of the layer would be an 8x8x(3 channel) image. That is, the number of channels in the output of a convolutional layer is equal to the number of filters used. Regardless of the number of channels the input to the convolutional layer had, because we add the channels together after applying our filters, we always end up with one image per filter.\nOkay, now you might be wondering: why do we add these multiple channels together, but we stacked the outputs from different filters instead of adding those together. Why don‚Äôt we do the same thing in both cases? The logic is roughly this: think of the purpose of each filter to be to learn something different about our image. Adding together the outputs from different filters would muddle their information together, so we want to make sure to keep the information preserved by stacking. But multi-channel filters, while they sort of act like multiple filters over multiple images, are truly one filter over one image. So, if we want those filters to focus on learning one thing about the image, then we want to add the channels together: because the multiple channels should be working together to tell us something about the image.\nSo, in summary:\n\nConvolutional blocks are typically made up of a few convolutional layers.\nA covolutional layer typically involves convolving the image input to the layer with many filters, all of the same size.\nThe output of a convolutional layer is an image with multiple channels - one per filter.\nIf the input to a convolutional layer has multiple channels, the filters used on the image in that layer must all have the same number of channels as the image.\nEach channel of the input image is convolved with a corresponding channel of the filter, to create a corresponding channel of the output.\nThe channels of an output image convolved with one filter are added together to make one image per filter, but the images generated by different filters are stacked to create the multiple channels of the output.\n\n  3.2.1 Some follow up on that note about 3D convolutions\n3D convolutions are convolutions over volumes. A 3x3x3 filter over a volume performs a similar operation as a 3x3 filter over a 2D image, in that the weights of the filter are multipled by a region of the image, then summed together to get one pixel value. The only real difference, is that a 3D filter on a 3D volume also strides over the volume dimension, instead of just across the image in the 2D dimensions.\n\ndisplay(Im('%s/images/operation_examples/3D_vs_2D_convs.png' %filepath, height=400, width=850))\n\n\n\n\n\n\n\n\nThus, a 3D convolution over a volume will usually also produce a volume: the original image will usually be padded in all dimensions, so that as the filter slides over all dimensions of the image, multiplying the weights and adding them together, the output volume has the same dimensions as an input volume.\nIn principle, our 3x3x(5 channel) filters are acting the same on an 8x8x(5 channel) image as a 3x3x5 3D filter would act on an 8x8x5 3D volume, in that we are mutiplying the weights by the pixel values and adding the results together to get one pixel value.\n\ndisplay(Im('%s/images/operation_examples/3D_vs_2DMultiChannel_convs.png' %filepath, height=400, width=850))\n\n\n\n\n\n\n\n\nBut, that doesn‚Äôt mean that multi-channel convolutions and 3D convolutions are generally the same thing. This only happens because, if the dimension of a filter matches the dimension of an image, the filter can‚Äôt slide over in that dimension. In the example above, the 3x3x5 3D filter can‚Äôt slide in the z dimension, so it can only move along x and y just like our multi-channel filter would. But 3D filters will typically be symmetric the way that 2D filters are typically symmetric: a 3D filter would likely be size 3x3x3, instead of 3x3x5, just like a 2D filter is almost always something like 3x3 instead of 3x5. Thus, a 3D convolution will usually be able to slide back along the z dimension of an image, and output a volume.\nThis is an important distinction because 3D volumes can also have multiple channels - in which case there would be multiple 3D filters making up one multi-channel 3D filter, and then thinking about multi-channel convolutions as just 3D convolutions doesn‚Äôt work anymore.\n  3.3 Pooling\nThe basic idea behind pooling is to reduce the dimensionality of an image, by representing some region of pixels with just one pixel instead.\nMax pooling is the most common type of pooling, and the type that we will use in our examples in this tutorial. In max pooling, a region of pixels is represented by the maximum-valued pixel within that region. So, we would represent a region of pixels like this one:\n\nim = np.array([[5,2],  # the region of pixels\n               [8,3]])\nim_max = np.max(im)    # what pooling would give us\n\nfig = plt.figure(figsize=(10,5))\nax1, ax2 = fig.subplots(1,2)\nax1.axis('off'), ax2.axis('off')\n\n# plot the image region with values \nax1.imshow(im, cmap=\"Greys\", vmin=0, vmax=12)\nfor j in range(im.shape[0]):\n  for i in range(im.shape[1]):\n    ax1.annotate(im[i][j], (j,i))\n\n# plot the resulting region\nax2.imshow(np.pad(np.array([im_max]*4).reshape(2,2),pad_width = (1,1), mode=\"constant\", constant_values=0), # just makes it look nice\n           cmap=\"Greys\", vmin=0, vmax=12)\nax2.annotate(im_max, (1.5,1.5))\nax2.arrow(-1.25,1.5,1,0,width=.05,head_width=.2,color='k') #draw an arrow\n\n\n\n\n\n\n\n\nThis would be 2x2 max pooling, because the region of pixels that we replace with a single pixel is size 2x2. We can arbitrarily choose the region size that we use for pooling, but this region is almost always square, and 2x2 is a very typical choice.\n2x2 max pooling an entire image involves taking every 2x2 region in the image and replacing it like so:\n\nim = np.array([[0,3,6,2,1,2],\n               [2,5,6,3,1,1],\n               [1,2,0,0,3,1],\n               [2,5,6,4,4,4],\n               [2,3,3,4,3,0],\n               [0,2,4,5,1,0]])\n\nfig = plt.figure(figsize=(10,5))\nax1, ax2 = fig.subplots(1,2)\nax1.axis('off'),ax2.axis('off')\n\ndisplay_ims = []\npooled_im = np.zeros((3,3))        # output image will have output shape = original shape / 2 for 2x2 pooling\nfor ind1 in range(pooled_im.shape[0]):\n  i = ind1*2         # so that we have an index that moves over by 2 pixels each time, instead of 1\n  for ind2 in range(pooled_im.shape[1]):\n    j = ind2*2\n    im1 = ax1.imshow(im, cmap=\"Greys\", vmin=-1, vmax=10, animated=True)\n    for k in range(im.shape[0]):\n      for l in range(im.shape[1]):\n        im1 = ax1.annotate(im[k][l], (l,k))    # plot the pixel values\n    ax1.set_title(\"Full Image\")\n    im1 = ax1.add_patch(matplotlib.patches.Rectangle((-.48+j,-.48+i),2,2,fill=False,color='red',lw=2)) #show region of pooling\n\n    pooled_im[ind1][ind2] = np.max(im[i:i+2,j:j+2])\n    im2 = ax2.imshow(pooled_im,cmap=\"Greys\",vmin=-1,vmax=10, animated=True)\n    ax2.set_title(\"Pooled Image\")                                       \n\n    display_ims.append([im1, im2, ax2.annotate(int(pooled_im[ind1][ind2]), (ind2,ind1))]) #also show pixel values\n\nani = animation.ArtistAnimation(fig, display_ims, interval=1000, blit=True, repeat_delay=1000)\nplt.close()\n\nHTML(ani.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nWe won‚Äôt discuss any of them in detail here, but there are other types of pooling. Average pooling, for instance, takes the average pixel value of a region as the new pixel value.\nYou might be wondering: what‚Äôs the advantage of throwing away information? 1.  Computationally, it‚Äôs advantageous to remove some information, especially if we can still retain the ‚Äúmost important‚Äù information when we do so. In convolutional neural networks in particular, the number of operations we need to perform scales with the size of the image as we convolve it, so reducing the image size can greatly reduce the number of computations we need to do. 2.  Pooling may help to ‚Äúsharpen‚Äù certain features in the image. Because filters are sort of trying to pick out specific features in an image, choosing the pixel that gave the highest ‚Äúsignal‚Äù in a region of an image may help to single out the most important parts of that image.\n  3.4 Upsampling\nUpsampling is unique to UNets - the step is performed because we need to increase the size of our image after a series of convolutions and pooling has decreased it. In this way, it‚Äôs like the opposite of pooling - instead of shrinking an image by representing a region of pixels with one pixel, we create a region of pixels by copying one pixel into multiple pixels.\n\nim = np.array([[1,3,6],\n               [2,5,6],\n               [2,4,3]])\n\nfig = plt.figure(figsize=(10,5))\nax1, ax2 = fig.subplots(1,2)\n\ndisplay_ims = []\nupsampled_im = np.zeros((6,6))        # output image will have output shape = original shape * 2 for 2x2 upsampling\nfor i in range(pooled_im.shape[0]):\n  ind1 = i*2\n  for j in range(pooled_im.shape[1]):\n    ind2 = j*2\n    im1 = ax1.imshow(im, cmap=\"Greys\", vmin=-1, vmax=10, animated=True)\n    for k in range(im.shape[0]):\n      for l in range(im.shape[1]):\n        im1 = ax1.annotate(im[k][l], (l,k))    # plot the pixel values\n    ax1.set_title(\"Full Image\")\n    im1 = ax1.add_patch(matplotlib.patches.Rectangle((-.5+j,-.5+i),1,1,fill=False,color='red',lw=2)) #show we're upsampling\n\n    for k in range(ind1,ind1+2):\n      for l in range(ind2,ind2+2):\n        upsampled_im[k][l] = im[i,j]\n        im2 = ax2.imshow(upsampled_im,cmap=\"Greys\",vmin=-1,vmax=10, animated=True)\n        display_ims.append([im1,im2,ax2.text(l,k,int(upsampled_im[k][l]))]) # plot the pixel values too\n    ax2.set_title(\"Upsampled Image\")                                       \n\nani = animation.ArtistAnimation(fig, display_ims, interval=600, blit=True, repeat_delay=1000)\nplt.close()\n\nHTML(ani.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nSo, you can see that even though the upsampled image looks identical to the original, it actually has dimensions 6x6 instead of 3x3, and 4 times the number of pixels.\n  3.5 Concatenations\nConcatenations are also unique to UNets. As we convolve our image and pool it, we lose the spatial information of our features. If we‚Äôve reduced the dimensionality of our starting image to 2x2, for example, then each pixel in that 2x2 image represents about a quarter of our initial image, meaning that we‚Äôve lost all information about finer resolution features within each quarter.\nIf we were to simple upsample and convolve our image back up to its original size, there would be no way to get that information back, because upsampling just copies the same pixels over again - it doesn‚Äôt increase the resolution of the details. For this reason, we need to do concatenations.\nIn the concatenation step, we take the output of a previous layer in the downsizing portion of the UNet, and stack it with the output from the upsizing portion of the UNet that has the same dimensions. In this way, we get to use the finer resolution information that the downsizing steps still had, but we also get our larger scale information from our upsampling step.\nFor instance, let‚Äôs say in the second convolutonal block of a UNet, we convolve our image with 3 filters, so we have an output that looks like this:\n\ndisplay(Im('%s/images/operation_examples/concat_example_im1.png' %filepath, height=370, width=370))\n\n\n\n\n\n\n\n\nThen, let‚Äôs say that in the next steps in the UNet, this image is pooled down to size 4x4x3, and more convolutions are done on the image, keeping it at size 4x4x3 but further transforming it.\nIf after these convolutions, we begin the upsizing portion of the UNet, we would begin with an operation which upsamples the 4x4x3 image back into an 8x8x3 image, which looks like:\n\ndisplay(Im('%s/images/operation_examples/concat_example_im2.png' %filepath, height=370, width=370))\n\n\n\n\n\n\n\n\nIn the concatenation step of the UNet, these two 8x8x3 images are stacked, so the image becomes 8x8x6. Then, this stacked image would go on to be convolved further, with the 6 stacked images all acting as different channels of the same image.\n\ndisplay(Im('%s/images/operation_examples/concat_example_im3.png' %filepath, height=400, width=400))\n\n\n\n\n\n\n\n\n 4. A Very Simple UNet Example\nTo get a better handle on how exactly these operations work, how they transform our image, and how they change the dimensionality of the image at each step, let‚Äôs closely investigate an extremely simple example of a UNet.\n  4.1 The Data/Problem\nSay we have a simple 8x8 image, made of black and white pixels randomly scattered. And we want to create a UNet to invert the image for us. Our data might look like this:\n\nim_in = np.array([[0,0,0,0,1,0,0,0],   # example image in, which we'll also use for testing later\n              [1,0,1,1,0,0,1,0],       # obviously, not a real random scattering, but just an example\n              [0,0,1,1,0,0,0,0],\n              [0,1,1,0,0,1,0,0],\n              [0,0,0,1,1,0,0,1],\n              [0,0,0,0,1,1,0,0],\n              [0,0,0,0,1,1,0,0],\n              [0,0,0,1,1,0,0,1]])\n\nim_out = np.abs(1-im_in)              # example image output, just the inversion of the input image\n\n# show our example images\nfig=plt.figure(figsize=(10,5))\nax1,ax2 = fig.subplots(1,2)\nax1.imshow(im_in, cmap=\"Greys_r\")\nax1.set_title(\"in\")\nax2.imshow(im_out, cmap=\"Greys_r\")\nax2.set_title(\"out\")\n\nText(0.5, 1.0, 'out')\n\n\n\n\n\n\n\n\n\nAnd we can easily generate a dataset of 100 examples:\n\nX_example = []\ny_example = []\nfor i in range(100):\n  X_example.append(np.round(np.random.rand(8,8)).reshape(8,8,1))\n  y_example.append(np.abs(1-X_example[-1]).reshape(8,8,1))\n\nX_example = np.array(X_example)\ny_example = np.array(y_example)\n\nThis inversion operation is obviously very simple: It takes one line of code and 2 operations (a subtraction and an absolute value) to perfectly invert our image. But, because this is a transformation of an image, a very simple UNet should also be able to perform this inversion for us, so that‚Äôs what we‚Äôll try to make here.\n  4.2 The Architecture\nWe‚Äôll use a simple UNet, with a few 3x3 filters, to do this inversion. The architecture will look like this:\n\ndisplay(Im('%s/images/simple_example_UFormat.png' %filepath, width=950, height=480))\n\n\n\n\n\n\n\n\nThis might look a little overwhelming right now, but we‚Äôre going to go through each of the operations that this network will perform in more detail in the upcoming sections.\nWe will also add layers to the model in keras as we go through them. To start building a model in keras, we just need to start defining our layers. This begins with the input:\n\ninput_size = X_example[0].shape    # get the size of the input images, in our case this is 8x8x1\nprint(input_size) \n\ninputs = Input(input_size)         # then, we just define an input layer and tell keras to expect images of size 8x8x1\n\n(8, 8, 1)\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n\n\n  4.3 Conv Block #1\nThe first convolution block has 3 steps:\n\n Conv1: 2 3x3 filter convolutions\n Conv2: 2 3x3 filter convolutions\n Pool1: 2x2 pooling\n\nIn the first convolution step, the input image is convolved twice: Once with one 3x3 filter, and another time with another 3x3 filter. We first pad the input image with zeros, so that the convolved image is 9x9, and the result is 2, 8x8 images, that we then add our bias to and then pass through the ReLu activation function. Each of these images is a ‚Äúrepresentation‚Äù of the original image. These images are stacked to become two channels of the same image, and the layer output is 1, 8x8x2(channel) image.\nBecause we have 2 filters, each with 3x3 weights, and an associated bias for each filter, this means our first layer has a total of: \\[\\begin{equation*}\n2\\times(3\\times3) + 2 = 20\n\\end{equation*}\\] learnable parameters.\n\ndisplay(Im('%s/images/layers/conv1.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nWe can add this to the model:\n\nconv1 = Conv2D(filters = 2,   # here, we tell the layer we want to use 2 filters\n               kernel_size = (3,3),  # the filters are of size 3x3\n               activation = 'relu',    # we want to use the ReLU activation function\n               padding = 'same',     # same padding means the output size will equal the input size(before padding)\n               kernel_initializer = 'he_normal')(inputs)  # we'll initialize the weights with the He normal distribution. We also\n                                                          # need to tell this layer what the input to it will be, which is the input\n                                                          # layer (inputs)\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n\n\n\nThe next convolution step, takes the output from the first convolution step, and again convolves it twice: Once with one 3x3x2 filter, and another time with another 3x3x2 filter. Note that these filters now need to have 2 channels, because the output from the first layer had 2 channels. As we discussed in the previous section, when a 2-channel filter convolves a 2-channel image, the outputs are added together to generate the output. That is, the darker blue filter convolves the lighter green channel of the image, and the lighter blue filter convolves the darker green image. Then, the two channel outputs are added together to create the darker purple image generated from the convolution. The same, of course, happens with the grey filter and the image, generating the lighter purple, the second of our two output images.\nAs with the first convolution layer, we pad the input image with zeros, add a bias after the convolution, and pass the images through the ReLu activation function. These output images are again stacked to become two channels of the same image, making the layer output 1, 8x8x2(channel) image.\nBecause we have 2 filters, each with 3x3x2 weights, and an associated bias for each filter, this means this second layer has a total of: \\[\\begin{equation*}\n2\\times(3\\times3\\times2) + 2 = 38\n\\end{equation*}\\] learnable parameters.\n\ndisplay(Im('%s/images/layers/conv2.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nWe add this to our model, the same way we added the first convolution:\n\nconv2 = Conv2D(filters = 2,   # 2 filters again\n               kernel_size = (3,3),  # size 3x3 filters. Keras is smart, so we don't need to tell it that these filters need to have 2\n                                     # channels; it will know that because it will know that the input to the layer has 2 channels\n               activation = 'relu', padding = 'same',    # we'll be keeping the activation, padding, and initializer the same for all\n               kernel_initializer = 'he_normal')(conv1)  # of our layers. But note, the input to this layer was now the output from\n                                                         # conv1\n\nThe last step in this convolution block is pooling, where we downsize our image by applying 2x2 pooling to it.\nThere are no learnable parameters in a pooling step, but it‚Äôs important to note that we do not pool across channels - our 8x8x2 output becomes 4x4x2, because the two channels are each pooled seperately and remain stacked.\n\ndisplay(Im('%s/images/layers/pool1.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nAdding a pooling layer to our model is also straightforward with Keras:\n\npool1 = MaxPooling2D(pool_size=(2, 2))(conv2)   # we just need to tell it the size of the region to pool,\n                                                # and that we're pooling the output from conv2\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\n\n\n  4.4 Conv Block #2\nThe second convolution block mimics the first, but we‚Äôll increase the number of filters:\n\n Conv3 has 3 3x3 filter convolutions\n Conv4 has 3 3x3 filter convolutions\n\nWe also won‚Äôt pool here, as the image is already small enough, and the next step will be to re-increase the image size.\nFor Conv3, the input image is convolved three times: with 3 filters that each have size 3x3, and 2 channels because our output from the pooling layer had 2 channels. As always, we first pad the input image with zeros, add our bias after the convolution, and pass through the ReLu activation function. These images are stacked to become three channels of the same image, and the layer output is 1, 4x4x3(channel) image.\nBecause we have 3 filters, each with 3x3x2 weights, and an associated bias for each filter, this means this layer has a total of: \\[\\begin{equation*}\n3\\times(3\\times3\\times2) + 3 = 57\n\\end{equation*}\\] learnable parameters.\n\ndisplay(Im('%s/images/layers/conv3.png' %filepath, height=370, width=1000))\n\n\n\n\n\n\n\n\nAdding this to our model:\n\nconv3 = Conv2D(filters = 3, kernel_size = (3,3),  #again, we don't need to tell it that we'll need 2-channel filters\n               activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n\nFor Conv4, the input image is convolved three times: with 3 filters that each have size 3x3, and 3 channels because our output from the Conv3 layer had 3 channels (because it was convolved with 3 filters). We pad the input image with zeros, add our bias after the convolution, and pass through the ReLu activation function. These images are stacked to become three channels of the same image, and the layer output is 1, 4x4x3(channel) image.\nBecause we have 3 filters, each with 3x3x3 weights, and an associated bias for each filter, this means this layer has a total of: \\[\\begin{equation*}\n3\\times(3\\times3\\times3) + 3 = 84\n\\end{equation*}\\] learnable parameters.\n\ndisplay(Im('%s/images/layers/conv4.png' %filepath, height=370, width=1000))\n\n\n\n\n\n\n\n\nAgain, this is easy to add to the model:\n\nconv4 = Conv2D(filters = 3, kernel_size = 3,  # if you just give kernel_size a single number, it assumes a square filter of that dimension\n               activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n\n  4.5 UpConv Block #1\nNext, we begin the upsizing portion of the UNet. This first Up-Convolution block will have 3 steps:\n\n UpSamp1 will do 2x2 upsampling\n Conv5 has 2 3x3 filter convolutions\n Concat1 will stack Conv5 output with Conv2 output\n\nIn the upsampling step, we‚Äôll upsize our image taking each pixel and copying it into a 2x2 square.\nThere are no learnable parameters in an upsampling step, but it‚Äôs important to note that, as with pooling, channels aren‚Äôt upsampled - our 4x4x3 image becomes 8x8x3, because the two channels are each upsampled seperately and remain stacked.\n\ndisplay(Im('%s/images/layers/upsamp1.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nAdding the upsampling layer to our model is also as easy as adding the pooling layer was:\n\nup1 = UpSampling2D(size = (2,2))(conv4)\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n\n\n\nIn the Conv5 step, the image we just created by upsampling is convolved two times: with 2 filters that each have size 3x3, and 3 channels. As always, we pad the input image with zeros before the convolution, and add one bias for each filter before passing the image through the ReLu activation function. These images are stacked to become two channels of the same image, and the layer output is 1, 8x8x2(channel) image.\nBecause we have 2 filters, each with 3x3x3 weights, and an associated bias for each filter, this means this layer has a total of: \\[\\begin{equation*}\n2\\times(3\\times3\\times3) + 2 = 56\n\\end{equation*}\\] learnable parameters.\n\ndisplay(Im('%s/images/layers/conv5.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nWe can add this convolutional layer to the mode making sure that we are applying it to the output from the upsampling layer:\n\nconv5 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up1)\n\nNext is the concatenation step. We now have an 8x8x(2 channel) image as the output from Conv5. We also had, from our downsizing steps, an 8x8x(2 channel) image as the output from Conv2. In the concatenation step, we stack these together so that we have an 8x8x(4 channel) image.\nConcatenations, because they just involve stacking images, will have no learnable parameters.\n\ndisplay(Im('%s/images/layers/concat1.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nTo do this concatenation with keras, we just need to specify what layer outputs (conv5 and conv2) we‚Äôre looking to concatenate:\n\nconcat1 = concatenate([conv2,conv5], axis = 3)     # axis = 3 tells the model that we need to stack these images as extra channels. Both\n                                                   # conv5 and conv3 will have shape (None, 8, 8, 2), so axis = 3 means to stack along the axis\n                                                   # which has shape 2, the channel axis\n\n  4.6 UpConv Block #2\nThis is the final block in our model. It will contain\n\n Conv6: 2, 3x3 convolutions\n Conv7: 1 3x3 convolution\n\nConv6 will convolve our concatenated image, with 2 filters that each have size 3x3, and 4 channels. We will pad, add bias, and ReLU as usual. The layer output is 1, 8x8x2(channel) image.\nBecause we have 2 filters, each with 3x3x4 weights, and an associated bias for each filter, this means this layer has a total of: \\[\\begin{equation*}\n2\\times(3\\times3\\times4) + 2 = 74\n\\end{equation*}\\] learnable parameters.\n\ndisplay(Im('%s/images/layers/conv6.png' %filepath, height=370, width=1000))\n\n\n\n\n\n\n\n\nWe add this to the model the same as any other convolutional layer, making sure we apply it to the output from our concatenation:\n\nconv6 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(concat1)\n\nFinally, Conv7 will convolve our image, with 1 filter of size 3x3, and 2 channels. Because our input only had 1 channel, our final convolution must use only 1 filter, to ensure that the output has only 1 channel. We will pad and add bias as usual.\nThe one difference from all of our other convolutional layers that we‚Äôll make is using the sigmoid activation function rather than ReLU. Because of the shape of the sigmoid function, values are more easily forced to be either 0 or 1. Because this is our output layer, and we know that our data was comprised of exclusively 0 or 1-valued pixels, the signmoid function will hopefully help to squash our pixel values to the correct one of these two values.\nThe layer output is 1, 8x8 image. Because we have 1 filter with 3x3x2 weights, and an associated bias, this layer has a total of: \\[\\begin{equation*}\n1\\times(3\\times3\\times2) + 1 = 19\n\\end{equation*}\\] learnable parameters.\n\ndisplay(Im('%s/images/layers/conv7.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nAdding this to our model:\n\nconv7 = Conv2D(1, 3, activation = 'sigmoid', padding = 'same', kernel_initializer = 'he_normal')(conv6) # note the change in activation function\n\n  4.7 The Final Model\nNow, let‚Äôs finish putting the model together, and have a look at the model summary that keras gives us, and try it out.\nTo finish up our model, we just need to define it by telling keras what layer is the input and what is the output. We‚Äôll also need to compile the model before we can use it, where we‚Äôll get to choose a few hyperparameters. To keep it simple, we‚Äôll choose a common optimizer, the Adam optimizer, and only specify the learning rate. We also need to choose what loss function to use, and we‚Äôll use the mean-squared error, which keras already has built in for us.\nThis tutorial isn‚Äôt meant to cover the huge body of options for all of these hyperparameters, loss functions, and other functionalities that we can add when compiling our model, but the keras website: https://keras.io/models/model/ does a good job of listing all of the options it has for the compile method.\n\nsimple_model = Model(input = inputs, output = conv7) # we tell it that the first layer is the input layer, and that conv7 is going to be\n                                                     # the layer that gives us the output. All of the layers in between were connected as we defined\n                                                     # them, so we don't need to give the model any of those here.\n\nsimple_model.compile(optimizer = Adam(lr = .0005), # Adam is an extremely common optimizer, and the lr is the learning rate\n                   loss = 'mse')\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n  \"\"\"Entry point for launching an IPython kernel.\n\n\nKeras will also display for us a summary of our model, showing the different layers, their shapes, and the number of learnable parameters per layer, and is a handy way to make sure that the model is consistent and doing everything we expect it to.\n\nsimple_model.summary()\n\nModel: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 8, 8, 1)      0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 8, 8, 2)      20          input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 8, 8, 2)      38          conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 4, 4, 2)      0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 4, 4, 3)      57          max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 4, 4, 3)      84          conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nup_sampling2d_1 (UpSampling2D)  (None, 8, 8, 3)      0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 8, 8, 2)      56          up_sampling2d_1[0][0]            \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 8, 8, 4)      0           conv2d_2[0][0]                   \n                                                                 conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 8, 8, 2)      74          concatenate_1[0][0]              \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 8, 8, 1)      19          conv2d_6[0][0]                   \n==================================================================================================\nTotal params: 348\nTrainable params: 348\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\nWe can see, if we go back and check the number of learnable parameters, and the output shapes for each of these layers, that they match exactly what we expected. The fact that the model compiles properly is good news, too - we‚Äôll get an error if we tried to build a model that doesn‚Äôt connect properly or where the shapes don‚Äôt make sense.\nTraining a model in keras is also super simple. Let‚Äôs try training this model, on our example data, for 500 epochs - 500 iterations of the model seeing all of the example images and adjusting the weights accordingly.\n\nsimple_model_history = simple_model.fit(X_example, y_example,   # the fake data we made, X is input, y is output\n                                        epochs = 500,           # we'll try out 100 epochs\n                                        verbose = 1)            # verbose = 1 tells keras that we want to see how well the model is doing at every epoch\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nEpoch 1/500\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\n100/100 [==============================] - 15s 154ms/step - loss: 0.4256\nEpoch 2/500\n100/100 [==============================] - 0s 247us/step - loss: 0.4156\nEpoch 3/500\n100/100 [==============================] - 0s 235us/step - loss: 0.4049\nEpoch 4/500\n100/100 [==============================] - 0s 235us/step - loss: 0.3936\nEpoch 5/500\n100/100 [==============================] - 0s 256us/step - loss: 0.3821\nEpoch 6/500\n100/100 [==============================] - 0s 258us/step - loss: 0.3702\nEpoch 7/500\n100/100 [==============================] - 0s 224us/step - loss: 0.3589\nEpoch 8/500\n100/100 [==============================] - 0s 230us/step - loss: 0.3477\nEpoch 9/500\n100/100 [==============================] - 0s 242us/step - loss: 0.3374\nEpoch 10/500\n100/100 [==============================] - 0s 239us/step - loss: 0.3278\nEpoch 11/500\n100/100 [==============================] - 0s 247us/step - loss: 0.3189\nEpoch 12/500\n100/100 [==============================] - 0s 243us/step - loss: 0.3113\nEpoch 13/500\n100/100 [==============================] - 0s 286us/step - loss: 0.3042\nEpoch 14/500\n100/100 [==============================] - 0s 244us/step - loss: 0.2981\nEpoch 15/500\n100/100 [==============================] - 0s 243us/step - loss: 0.2929\nEpoch 16/500\n100/100 [==============================] - 0s 239us/step - loss: 0.2884\nEpoch 17/500\n100/100 [==============================] - 0s 293us/step - loss: 0.2844\nEpoch 18/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2810\nEpoch 19/500\n100/100 [==============================] - 0s 240us/step - loss: 0.2780\nEpoch 20/500\n100/100 [==============================] - 0s 231us/step - loss: 0.2755\nEpoch 21/500\n100/100 [==============================] - 0s 250us/step - loss: 0.2731\nEpoch 22/500\n100/100 [==============================] - 0s 228us/step - loss: 0.2711\nEpoch 23/500\n100/100 [==============================] - 0s 236us/step - loss: 0.2692\nEpoch 24/500\n100/100 [==============================] - 0s 224us/step - loss: 0.2676\nEpoch 25/500\n100/100 [==============================] - 0s 258us/step - loss: 0.2661\nEpoch 26/500\n100/100 [==============================] - 0s 240us/step - loss: 0.2647\nEpoch 27/500\n100/100 [==============================] - 0s 241us/step - loss: 0.2635\nEpoch 28/500\n100/100 [==============================] - 0s 264us/step - loss: 0.2624\nEpoch 29/500\n100/100 [==============================] - 0s 322us/step - loss: 0.2613\nEpoch 30/500\n100/100 [==============================] - 0s 230us/step - loss: 0.2604\nEpoch 31/500\n100/100 [==============================] - 0s 225us/step - loss: 0.2595\nEpoch 32/500\n100/100 [==============================] - 0s 238us/step - loss: 0.2587\nEpoch 33/500\n100/100 [==============================] - 0s 233us/step - loss: 0.2580\nEpoch 34/500\n100/100 [==============================] - 0s 253us/step - loss: 0.2573\nEpoch 35/500\n100/100 [==============================] - 0s 229us/step - loss: 0.2567\nEpoch 36/500\n100/100 [==============================] - 0s 250us/step - loss: 0.2561\nEpoch 37/500\n100/100 [==============================] - 0s 318us/step - loss: 0.2556\nEpoch 38/500\n100/100 [==============================] - 0s 303us/step - loss: 0.2550\nEpoch 39/500\n100/100 [==============================] - 0s 228us/step - loss: 0.2545\nEpoch 40/500\n100/100 [==============================] - 0s 224us/step - loss: 0.2540\nEpoch 41/500\n100/100 [==============================] - 0s 248us/step - loss: 0.2536\nEpoch 42/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2531\nEpoch 43/500\n100/100 [==============================] - 0s 234us/step - loss: 0.2527\nEpoch 44/500\n100/100 [==============================] - 0s 230us/step - loss: 0.2524\nEpoch 45/500\n100/100 [==============================] - 0s 247us/step - loss: 0.2520\nEpoch 46/500\n100/100 [==============================] - 0s 244us/step - loss: 0.2517\nEpoch 47/500\n100/100 [==============================] - 0s 248us/step - loss: 0.2513\nEpoch 48/500\n100/100 [==============================] - 0s 249us/step - loss: 0.2510\nEpoch 49/500\n100/100 [==============================] - 0s 220us/step - loss: 0.2507\nEpoch 50/500\n100/100 [==============================] - 0s 250us/step - loss: 0.2504\nEpoch 51/500\n100/100 [==============================] - 0s 242us/step - loss: 0.2501\nEpoch 52/500\n100/100 [==============================] - 0s 255us/step - loss: 0.2498\nEpoch 53/500\n100/100 [==============================] - 0s 290us/step - loss: 0.2495\nEpoch 54/500\n100/100 [==============================] - 0s 242us/step - loss: 0.2493\nEpoch 55/500\n100/100 [==============================] - 0s 244us/step - loss: 0.2491\nEpoch 56/500\n100/100 [==============================] - 0s 263us/step - loss: 0.2488\nEpoch 57/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2486\nEpoch 58/500\n100/100 [==============================] - 0s 224us/step - loss: 0.2484\nEpoch 59/500\n100/100 [==============================] - 0s 215us/step - loss: 0.2482\nEpoch 60/500\n100/100 [==============================] - 0s 257us/step - loss: 0.2480\nEpoch 61/500\n100/100 [==============================] - 0s 230us/step - loss: 0.2478\nEpoch 62/500\n100/100 [==============================] - 0s 260us/step - loss: 0.2475\nEpoch 63/500\n100/100 [==============================] - 0s 264us/step - loss: 0.2473\nEpoch 64/500\n100/100 [==============================] - 0s 251us/step - loss: 0.2471\nEpoch 65/500\n100/100 [==============================] - 0s 234us/step - loss: 0.2469\nEpoch 66/500\n100/100 [==============================] - 0s 286us/step - loss: 0.2467\nEpoch 67/500\n100/100 [==============================] - 0s 283us/step - loss: 0.2465\nEpoch 68/500\n100/100 [==============================] - 0s 250us/step - loss: 0.2463\nEpoch 69/500\n100/100 [==============================] - 0s 231us/step - loss: 0.2461\nEpoch 70/500\n100/100 [==============================] - 0s 229us/step - loss: 0.2459\nEpoch 71/500\n100/100 [==============================] - 0s 259us/step - loss: 0.2457\nEpoch 72/500\n100/100 [==============================] - 0s 271us/step - loss: 0.2455\nEpoch 73/500\n100/100 [==============================] - 0s 223us/step - loss: 0.2453\nEpoch 74/500\n100/100 [==============================] - 0s 223us/step - loss: 0.2451\nEpoch 75/500\n100/100 [==============================] - 0s 213us/step - loss: 0.2449\nEpoch 76/500\n100/100 [==============================] - 0s 243us/step - loss: 0.2446\nEpoch 77/500\n100/100 [==============================] - 0s 271us/step - loss: 0.2444\nEpoch 78/500\n100/100 [==============================] - 0s 250us/step - loss: 0.2441\nEpoch 79/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2439\nEpoch 80/500\n100/100 [==============================] - 0s 266us/step - loss: 0.2436\nEpoch 81/500\n100/100 [==============================] - 0s 262us/step - loss: 0.2433\nEpoch 82/500\n100/100 [==============================] - 0s 262us/step - loss: 0.2431\nEpoch 83/500\n100/100 [==============================] - 0s 239us/step - loss: 0.2428\nEpoch 84/500\n100/100 [==============================] - 0s 228us/step - loss: 0.2425\nEpoch 85/500\n100/100 [==============================] - 0s 249us/step - loss: 0.2422\nEpoch 86/500\n100/100 [==============================] - 0s 246us/step - loss: 0.2418\nEpoch 87/500\n100/100 [==============================] - 0s 238us/step - loss: 0.2415\nEpoch 88/500\n100/100 [==============================] - 0s 219us/step - loss: 0.2412\nEpoch 89/500\n100/100 [==============================] - 0s 305us/step - loss: 0.2408\nEpoch 90/500\n100/100 [==============================] - 0s 249us/step - loss: 0.2404\nEpoch 91/500\n100/100 [==============================] - 0s 245us/step - loss: 0.2400\nEpoch 92/500\n100/100 [==============================] - 0s 224us/step - loss: 0.2396\nEpoch 93/500\n100/100 [==============================] - 0s 295us/step - loss: 0.2393\nEpoch 94/500\n100/100 [==============================] - 0s 243us/step - loss: 0.2389\nEpoch 95/500\n100/100 [==============================] - 0s 227us/step - loss: 0.2385\nEpoch 96/500\n100/100 [==============================] - 0s 218us/step - loss: 0.2380\nEpoch 97/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2376\nEpoch 98/500\n100/100 [==============================] - 0s 242us/step - loss: 0.2372\nEpoch 99/500\n100/100 [==============================] - 0s 243us/step - loss: 0.2368\nEpoch 100/500\n100/100 [==============================] - 0s 240us/step - loss: 0.2363\nEpoch 101/500\n100/100 [==============================] - 0s 237us/step - loss: 0.2359\nEpoch 102/500\n100/100 [==============================] - 0s 255us/step - loss: 0.2354\nEpoch 103/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2349\nEpoch 104/500\n100/100 [==============================] - 0s 235us/step - loss: 0.2344\nEpoch 105/500\n100/100 [==============================] - 0s 245us/step - loss: 0.2339\nEpoch 106/500\n100/100 [==============================] - 0s 247us/step - loss: 0.2334\nEpoch 107/500\n100/100 [==============================] - 0s 238us/step - loss: 0.2329\nEpoch 108/500\n100/100 [==============================] - 0s 288us/step - loss: 0.2323\nEpoch 109/500\n100/100 [==============================] - 0s 234us/step - loss: 0.2318\nEpoch 110/500\n100/100 [==============================] - 0s 256us/step - loss: 0.2313\nEpoch 111/500\n100/100 [==============================] - 0s 287us/step - loss: 0.2308\nEpoch 112/500\n100/100 [==============================] - 0s 277us/step - loss: 0.2302\nEpoch 113/500\n100/100 [==============================] - 0s 248us/step - loss: 0.2297\nEpoch 114/500\n100/100 [==============================] - 0s 228us/step - loss: 0.2292\nEpoch 115/500\n100/100 [==============================] - 0s 220us/step - loss: 0.2286\nEpoch 116/500\n100/100 [==============================] - 0s 288us/step - loss: 0.2281\nEpoch 117/500\n100/100 [==============================] - 0s 213us/step - loss: 0.2276\nEpoch 118/500\n100/100 [==============================] - 0s 239us/step - loss: 0.2270\nEpoch 119/500\n100/100 [==============================] - 0s 286us/step - loss: 0.2265\nEpoch 120/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2260\nEpoch 121/500\n100/100 [==============================] - 0s 239us/step - loss: 0.2255\nEpoch 122/500\n100/100 [==============================] - 0s 254us/step - loss: 0.2249\nEpoch 123/500\n100/100 [==============================] - 0s 236us/step - loss: 0.2244\nEpoch 124/500\n100/100 [==============================] - 0s 298us/step - loss: 0.2238\nEpoch 125/500\n100/100 [==============================] - 0s 233us/step - loss: 0.2233\nEpoch 126/500\n100/100 [==============================] - 0s 244us/step - loss: 0.2227\nEpoch 127/500\n100/100 [==============================] - 0s 301us/step - loss: 0.2221\nEpoch 128/500\n100/100 [==============================] - 0s 277us/step - loss: 0.2215\nEpoch 129/500\n100/100 [==============================] - 0s 250us/step - loss: 0.2210\nEpoch 130/500\n100/100 [==============================] - 0s 233us/step - loss: 0.2204\nEpoch 131/500\n100/100 [==============================] - 0s 227us/step - loss: 0.2198\nEpoch 132/500\n100/100 [==============================] - 0s 261us/step - loss: 0.2192\nEpoch 133/500\n100/100 [==============================] - 0s 277us/step - loss: 0.2187\nEpoch 134/500\n100/100 [==============================] - 0s 245us/step - loss: 0.2181\nEpoch 135/500\n100/100 [==============================] - 0s 240us/step - loss: 0.2175\nEpoch 136/500\n100/100 [==============================] - 0s 237us/step - loss: 0.2169\nEpoch 137/500\n100/100 [==============================] - 0s 252us/step - loss: 0.2164\nEpoch 138/500\n100/100 [==============================] - 0s 252us/step - loss: 0.2158\nEpoch 139/500\n100/100 [==============================] - 0s 276us/step - loss: 0.2152\nEpoch 140/500\n100/100 [==============================] - 0s 281us/step - loss: 0.2146\nEpoch 141/500\n100/100 [==============================] - 0s 246us/step - loss: 0.2140\nEpoch 142/500\n100/100 [==============================] - 0s 245us/step - loss: 0.2134\nEpoch 143/500\n100/100 [==============================] - 0s 238us/step - loss: 0.2129\nEpoch 144/500\n100/100 [==============================] - 0s 284us/step - loss: 0.2123\nEpoch 145/500\n100/100 [==============================] - 0s 280us/step - loss: 0.2117\nEpoch 146/500\n100/100 [==============================] - 0s 260us/step - loss: 0.2111\nEpoch 147/500\n100/100 [==============================] - 0s 294us/step - loss: 0.2105\nEpoch 148/500\n100/100 [==============================] - 0s 224us/step - loss: 0.2100\nEpoch 149/500\n100/100 [==============================] - 0s 241us/step - loss: 0.2094\nEpoch 150/500\n100/100 [==============================] - 0s 251us/step - loss: 0.2088\nEpoch 151/500\n100/100 [==============================] - 0s 230us/step - loss: 0.2083\nEpoch 152/500\n100/100 [==============================] - 0s 225us/step - loss: 0.2077\nEpoch 153/500\n100/100 [==============================] - 0s 218us/step - loss: 0.2071\nEpoch 154/500\n100/100 [==============================] - 0s 268us/step - loss: 0.2066\nEpoch 155/500\n100/100 [==============================] - 0s 259us/step - loss: 0.2060\nEpoch 156/500\n100/100 [==============================] - 0s 215us/step - loss: 0.2054\nEpoch 157/500\n100/100 [==============================] - 0s 230us/step - loss: 0.2048\nEpoch 158/500\n100/100 [==============================] - 0s 243us/step - loss: 0.2043\nEpoch 159/500\n100/100 [==============================] - 0s 268us/step - loss: 0.2037\nEpoch 160/500\n100/100 [==============================] - 0s 261us/step - loss: 0.2031\nEpoch 161/500\n100/100 [==============================] - 0s 258us/step - loss: 0.2026\nEpoch 162/500\n100/100 [==============================] - 0s 268us/step - loss: 0.2020\nEpoch 163/500\n100/100 [==============================] - 0s 234us/step - loss: 0.2014\nEpoch 164/500\n100/100 [==============================] - 0s 268us/step - loss: 0.2009\nEpoch 165/500\n100/100 [==============================] - 0s 225us/step - loss: 0.2003\nEpoch 166/500\n100/100 [==============================] - 0s 218us/step - loss: 0.1997\nEpoch 167/500\n100/100 [==============================] - 0s 215us/step - loss: 0.1992\nEpoch 168/500\n100/100 [==============================] - 0s 225us/step - loss: 0.1986\nEpoch 169/500\n100/100 [==============================] - 0s 261us/step - loss: 0.1980\nEpoch 170/500\n100/100 [==============================] - 0s 238us/step - loss: 0.1975\nEpoch 171/500\n100/100 [==============================] - 0s 246us/step - loss: 0.1969\nEpoch 172/500\n100/100 [==============================] - 0s 316us/step - loss: 0.1964\nEpoch 173/500\n100/100 [==============================] - 0s 237us/step - loss: 0.1958\nEpoch 174/500\n100/100 [==============================] - 0s 232us/step - loss: 0.1953\nEpoch 175/500\n100/100 [==============================] - 0s 258us/step - loss: 0.1947\nEpoch 176/500\n100/100 [==============================] - 0s 229us/step - loss: 0.1941\nEpoch 177/500\n100/100 [==============================] - 0s 264us/step - loss: 0.1936\nEpoch 178/500\n100/100 [==============================] - 0s 279us/step - loss: 0.1930\nEpoch 179/500\n100/100 [==============================] - 0s 244us/step - loss: 0.1925\nEpoch 180/500\n100/100 [==============================] - 0s 237us/step - loss: 0.1919\nEpoch 181/500\n100/100 [==============================] - 0s 226us/step - loss: 0.1914\nEpoch 182/500\n100/100 [==============================] - 0s 225us/step - loss: 0.1908\nEpoch 183/500\n100/100 [==============================] - 0s 252us/step - loss: 0.1902\nEpoch 184/500\n100/100 [==============================] - 0s 248us/step - loss: 0.1897\nEpoch 185/500\n100/100 [==============================] - 0s 270us/step - loss: 0.1892\nEpoch 186/500\n100/100 [==============================] - 0s 242us/step - loss: 0.1886\nEpoch 187/500\n100/100 [==============================] - 0s 249us/step - loss: 0.1881\nEpoch 188/500\n100/100 [==============================] - 0s 274us/step - loss: 0.1875\nEpoch 189/500\n100/100 [==============================] - 0s 270us/step - loss: 0.1870\nEpoch 190/500\n100/100 [==============================] - 0s 261us/step - loss: 0.1864\nEpoch 191/500\n100/100 [==============================] - 0s 230us/step - loss: 0.1858\nEpoch 192/500\n100/100 [==============================] - 0s 249us/step - loss: 0.1852\nEpoch 193/500\n100/100 [==============================] - 0s 281us/step - loss: 0.1846\nEpoch 194/500\n100/100 [==============================] - 0s 259us/step - loss: 0.1840\nEpoch 195/500\n100/100 [==============================] - 0s 273us/step - loss: 0.1835\nEpoch 196/500\n100/100 [==============================] - 0s 237us/step - loss: 0.1829\nEpoch 197/500\n100/100 [==============================] - 0s 237us/step - loss: 0.1823\nEpoch 198/500\n100/100 [==============================] - 0s 236us/step - loss: 0.1817\nEpoch 199/500\n100/100 [==============================] - 0s 276us/step - loss: 0.1811\nEpoch 200/500\n100/100 [==============================] - 0s 267us/step - loss: 0.1804\nEpoch 201/500\n100/100 [==============================] - 0s 228us/step - loss: 0.1798\nEpoch 202/500\n100/100 [==============================] - 0s 230us/step - loss: 0.1791\nEpoch 203/500\n100/100 [==============================] - 0s 239us/step - loss: 0.1784\nEpoch 204/500\n100/100 [==============================] - 0s 232us/step - loss: 0.1777\nEpoch 205/500\n100/100 [==============================] - 0s 250us/step - loss: 0.1770\nEpoch 206/500\n100/100 [==============================] - 0s 312us/step - loss: 0.1763\nEpoch 207/500\n100/100 [==============================] - 0s 272us/step - loss: 0.1756\nEpoch 208/500\n100/100 [==============================] - 0s 238us/step - loss: 0.1748\nEpoch 209/500\n100/100 [==============================] - 0s 300us/step - loss: 0.1741\nEpoch 210/500\n100/100 [==============================] - 0s 231us/step - loss: 0.1733\nEpoch 211/500\n100/100 [==============================] - 0s 276us/step - loss: 0.1725\nEpoch 212/500\n100/100 [==============================] - 0s 216us/step - loss: 0.1716\nEpoch 213/500\n100/100 [==============================] - 0s 248us/step - loss: 0.1708\nEpoch 214/500\n100/100 [==============================] - 0s 223us/step - loss: 0.1700\nEpoch 215/500\n100/100 [==============================] - 0s 215us/step - loss: 0.1691\nEpoch 216/500\n100/100 [==============================] - 0s 230us/step - loss: 0.1682\nEpoch 217/500\n100/100 [==============================] - 0s 295us/step - loss: 0.1673\nEpoch 218/500\n100/100 [==============================] - 0s 234us/step - loss: 0.1663\nEpoch 219/500\n100/100 [==============================] - 0s 259us/step - loss: 0.1654\nEpoch 220/500\n100/100 [==============================] - 0s 269us/step - loss: 0.1644\nEpoch 221/500\n100/100 [==============================] - 0s 251us/step - loss: 0.1634\nEpoch 222/500\n100/100 [==============================] - 0s 227us/step - loss: 0.1623\nEpoch 223/500\n100/100 [==============================] - 0s 232us/step - loss: 0.1613\nEpoch 224/500\n100/100 [==============================] - 0s 245us/step - loss: 0.1602\nEpoch 225/500\n100/100 [==============================] - 0s 292us/step - loss: 0.1591\nEpoch 226/500\n100/100 [==============================] - 0s 255us/step - loss: 0.1579\nEpoch 227/500\n100/100 [==============================] - 0s 239us/step - loss: 0.1567\nEpoch 228/500\n100/100 [==============================] - 0s 300us/step - loss: 0.1555\nEpoch 229/500\n100/100 [==============================] - 0s 241us/step - loss: 0.1542\nEpoch 230/500\n100/100 [==============================] - 0s 246us/step - loss: 0.1530\nEpoch 231/500\n100/100 [==============================] - 0s 232us/step - loss: 0.1517\nEpoch 232/500\n100/100 [==============================] - 0s 261us/step - loss: 0.1503\nEpoch 233/500\n100/100 [==============================] - 0s 272us/step - loss: 0.1489\nEpoch 234/500\n100/100 [==============================] - 0s 238us/step - loss: 0.1475\nEpoch 235/500\n100/100 [==============================] - 0s 241us/step - loss: 0.1461\nEpoch 236/500\n100/100 [==============================] - 0s 265us/step - loss: 0.1447\nEpoch 237/500\n100/100 [==============================] - 0s 265us/step - loss: 0.1433\nEpoch 238/500\n100/100 [==============================] - 0s 241us/step - loss: 0.1419\nEpoch 239/500\n100/100 [==============================] - 0s 255us/step - loss: 0.1404\nEpoch 240/500\n100/100 [==============================] - 0s 232us/step - loss: 0.1389\nEpoch 241/500\n100/100 [==============================] - 0s 255us/step - loss: 0.1374\nEpoch 242/500\n100/100 [==============================] - 0s 281us/step - loss: 0.1360\nEpoch 243/500\n100/100 [==============================] - 0s 237us/step - loss: 0.1345\nEpoch 244/500\n100/100 [==============================] - 0s 262us/step - loss: 0.1330\nEpoch 245/500\n100/100 [==============================] - 0s 258us/step - loss: 0.1315\nEpoch 246/500\n100/100 [==============================] - 0s 247us/step - loss: 0.1300\nEpoch 247/500\n100/100 [==============================] - 0s 255us/step - loss: 0.1284\nEpoch 248/500\n100/100 [==============================] - 0s 234us/step - loss: 0.1269\nEpoch 249/500\n100/100 [==============================] - 0s 215us/step - loss: 0.1254\nEpoch 250/500\n100/100 [==============================] - 0s 232us/step - loss: 0.1239\nEpoch 251/500\n100/100 [==============================] - 0s 343us/step - loss: 0.1225\nEpoch 252/500\n100/100 [==============================] - 0s 272us/step - loss: 0.1210\nEpoch 253/500\n100/100 [==============================] - 0s 228us/step - loss: 0.1195\nEpoch 254/500\n100/100 [==============================] - 0s 236us/step - loss: 0.1181\nEpoch 255/500\n100/100 [==============================] - 0s 219us/step - loss: 0.1167\nEpoch 256/500\n100/100 [==============================] - 0s 220us/step - loss: 0.1152\nEpoch 257/500\n100/100 [==============================] - 0s 228us/step - loss: 0.1139\nEpoch 258/500\n100/100 [==============================] - 0s 289us/step - loss: 0.1125\nEpoch 259/500\n100/100 [==============================] - 0s 248us/step - loss: 0.1112\nEpoch 260/500\n100/100 [==============================] - 0s 242us/step - loss: 0.1098\nEpoch 261/500\n100/100 [==============================] - 0s 303us/step - loss: 0.1085\nEpoch 262/500\n100/100 [==============================] - 0s 238us/step - loss: 0.1072\nEpoch 263/500\n100/100 [==============================] - 0s 213us/step - loss: 0.1059\nEpoch 264/500\n100/100 [==============================] - 0s 221us/step - loss: 0.1046\nEpoch 265/500\n100/100 [==============================] - 0s 236us/step - loss: 0.1034\nEpoch 266/500\n100/100 [==============================] - 0s 244us/step - loss: 0.1021\nEpoch 267/500\n100/100 [==============================] - 0s 261us/step - loss: 0.1008\nEpoch 268/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0995\nEpoch 269/500\n100/100 [==============================] - 0s 249us/step - loss: 0.0982\nEpoch 270/500\n100/100 [==============================] - 0s 266us/step - loss: 0.0970\nEpoch 271/500\n100/100 [==============================] - 0s 225us/step - loss: 0.0958\nEpoch 272/500\n100/100 [==============================] - 0s 272us/step - loss: 0.0945\nEpoch 273/500\n100/100 [==============================] - 0s 265us/step - loss: 0.0933\nEpoch 274/500\n100/100 [==============================] - 0s 251us/step - loss: 0.0921\nEpoch 275/500\n100/100 [==============================] - 0s 229us/step - loss: 0.0909\nEpoch 276/500\n100/100 [==============================] - 0s 205us/step - loss: 0.0897\nEpoch 277/500\n100/100 [==============================] - 0s 232us/step - loss: 0.0885\nEpoch 278/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0874\nEpoch 279/500\n100/100 [==============================] - 0s 298us/step - loss: 0.0862\nEpoch 280/500\n100/100 [==============================] - 0s 214us/step - loss: 0.0851\nEpoch 281/500\n100/100 [==============================] - 0s 287us/step - loss: 0.0841\nEpoch 282/500\n100/100 [==============================] - 0s 242us/step - loss: 0.0830\nEpoch 283/500\n100/100 [==============================] - 0s 270us/step - loss: 0.0819\nEpoch 284/500\n100/100 [==============================] - 0s 243us/step - loss: 0.0808\nEpoch 285/500\n100/100 [==============================] - 0s 232us/step - loss: 0.0798\nEpoch 286/500\n100/100 [==============================] - 0s 241us/step - loss: 0.0787\nEpoch 287/500\n100/100 [==============================] - 0s 258us/step - loss: 0.0777\nEpoch 288/500\n100/100 [==============================] - 0s 271us/step - loss: 0.0767\nEpoch 289/500\n100/100 [==============================] - 0s 255us/step - loss: 0.0756\nEpoch 290/500\n100/100 [==============================] - 0s 256us/step - loss: 0.0747\nEpoch 291/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0738\nEpoch 292/500\n100/100 [==============================] - 0s 263us/step - loss: 0.0728\nEpoch 293/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0718\nEpoch 294/500\n100/100 [==============================] - 0s 235us/step - loss: 0.0709\nEpoch 295/500\n100/100 [==============================] - 0s 237us/step - loss: 0.0700\nEpoch 296/500\n100/100 [==============================] - 0s 269us/step - loss: 0.0691\nEpoch 297/500\n100/100 [==============================] - 0s 245us/step - loss: 0.0682\nEpoch 298/500\n100/100 [==============================] - 0s 234us/step - loss: 0.0673\nEpoch 299/500\n100/100 [==============================] - 0s 218us/step - loss: 0.0664\nEpoch 300/500\n100/100 [==============================] - 0s 222us/step - loss: 0.0656\nEpoch 301/500\n100/100 [==============================] - 0s 247us/step - loss: 0.0647\nEpoch 302/500\n100/100 [==============================] - 0s 215us/step - loss: 0.0639\nEpoch 303/500\n100/100 [==============================] - 0s 234us/step - loss: 0.0631\nEpoch 304/500\n100/100 [==============================] - 0s 224us/step - loss: 0.0624\nEpoch 305/500\n100/100 [==============================] - 0s 247us/step - loss: 0.0616\nEpoch 306/500\n100/100 [==============================] - 0s 255us/step - loss: 0.0610\nEpoch 307/500\n100/100 [==============================] - 0s 247us/step - loss: 0.0603\nEpoch 308/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0595\nEpoch 309/500\n100/100 [==============================] - 0s 265us/step - loss: 0.0586\nEpoch 310/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0580\nEpoch 311/500\n100/100 [==============================] - 0s 236us/step - loss: 0.0573\nEpoch 312/500\n100/100 [==============================] - 0s 293us/step - loss: 0.0566\nEpoch 313/500\n100/100 [==============================] - 0s 218us/step - loss: 0.0560\nEpoch 314/500\n100/100 [==============================] - 0s 223us/step - loss: 0.0554\nEpoch 315/500\n100/100 [==============================] - 0s 250us/step - loss: 0.0548\nEpoch 316/500\n100/100 [==============================] - 0s 306us/step - loss: 0.0542\nEpoch 317/500\n100/100 [==============================] - 0s 261us/step - loss: 0.0536\nEpoch 318/500\n100/100 [==============================] - 0s 269us/step - loss: 0.0530\nEpoch 319/500\n100/100 [==============================] - 0s 254us/step - loss: 0.0523\nEpoch 320/500\n100/100 [==============================] - 0s 268us/step - loss: 0.0517\nEpoch 321/500\n100/100 [==============================] - 0s 266us/step - loss: 0.0512\nEpoch 322/500\n100/100 [==============================] - 0s 324us/step - loss: 0.0506\nEpoch 323/500\n100/100 [==============================] - 0s 300us/step - loss: 0.0500\nEpoch 324/500\n100/100 [==============================] - 0s 334us/step - loss: 0.0496\nEpoch 325/500\n100/100 [==============================] - 0s 300us/step - loss: 0.0489\nEpoch 326/500\n100/100 [==============================] - 0s 267us/step - loss: 0.0484\nEpoch 327/500\n100/100 [==============================] - 0s 322us/step - loss: 0.0479\nEpoch 328/500\n100/100 [==============================] - 0s 345us/step - loss: 0.0474\nEpoch 329/500\n100/100 [==============================] - 0s 342us/step - loss: 0.0468\nEpoch 330/500\n100/100 [==============================] - 0s 409us/step - loss: 0.0463\nEpoch 331/500\n100/100 [==============================] - 0s 291us/step - loss: 0.0458\nEpoch 332/500\n100/100 [==============================] - 0s 309us/step - loss: 0.0453\nEpoch 333/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0448\nEpoch 334/500\n100/100 [==============================] - 0s 277us/step - loss: 0.0443\nEpoch 335/500\n100/100 [==============================] - 0s 344us/step - loss: 0.0438\nEpoch 336/500\n100/100 [==============================] - 0s 298us/step - loss: 0.0433\nEpoch 337/500\n100/100 [==============================] - 0s 296us/step - loss: 0.0429\nEpoch 338/500\n100/100 [==============================] - 0s 279us/step - loss: 0.0424\nEpoch 339/500\n100/100 [==============================] - 0s 216us/step - loss: 0.0420\nEpoch 340/500\n100/100 [==============================] - 0s 254us/step - loss: 0.0415\nEpoch 341/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0411\nEpoch 342/500\n100/100 [==============================] - 0s 280us/step - loss: 0.0406\nEpoch 343/500\n100/100 [==============================] - 0s 262us/step - loss: 0.0402\nEpoch 344/500\n100/100 [==============================] - 0s 227us/step - loss: 0.0398\nEpoch 345/500\n100/100 [==============================] - 0s 238us/step - loss: 0.0394\nEpoch 346/500\n100/100 [==============================] - 0s 240us/step - loss: 0.0390\nEpoch 347/500\n100/100 [==============================] - 0s 212us/step - loss: 0.0386\nEpoch 348/500\n100/100 [==============================] - 0s 232us/step - loss: 0.0382\nEpoch 349/500\n100/100 [==============================] - 0s 285us/step - loss: 0.0379\nEpoch 350/500\n100/100 [==============================] - 0s 271us/step - loss: 0.0376\nEpoch 351/500\n100/100 [==============================] - 0s 274us/step - loss: 0.0372\nEpoch 352/500\n100/100 [==============================] - 0s 270us/step - loss: 0.0368\nEpoch 353/500\n100/100 [==============================] - 0s 233us/step - loss: 0.0364\nEpoch 354/500\n100/100 [==============================] - 0s 222us/step - loss: 0.0361\nEpoch 355/500\n100/100 [==============================] - 0s 218us/step - loss: 0.0358\nEpoch 356/500\n100/100 [==============================] - 0s 265us/step - loss: 0.0355\nEpoch 357/500\n100/100 [==============================] - 0s 249us/step - loss: 0.0351\nEpoch 358/500\n100/100 [==============================] - 0s 279us/step - loss: 0.0348\nEpoch 359/500\n100/100 [==============================] - 0s 236us/step - loss: 0.0345\nEpoch 360/500\n100/100 [==============================] - 0s 292us/step - loss: 0.0342\nEpoch 361/500\n100/100 [==============================] - 0s 327us/step - loss: 0.0339\nEpoch 362/500\n100/100 [==============================] - 0s 246us/step - loss: 0.0336\nEpoch 363/500\n100/100 [==============================] - 0s 242us/step - loss: 0.0333\nEpoch 364/500\n100/100 [==============================] - 0s 238us/step - loss: 0.0331\nEpoch 365/500\n100/100 [==============================] - 0s 324us/step - loss: 0.0328\nEpoch 366/500\n100/100 [==============================] - 0s 247us/step - loss: 0.0326\nEpoch 367/500\n100/100 [==============================] - 0s 248us/step - loss: 0.0324\nEpoch 368/500\n100/100 [==============================] - 0s 238us/step - loss: 0.0320\nEpoch 369/500\n100/100 [==============================] - 0s 234us/step - loss: 0.0318\nEpoch 370/500\n100/100 [==============================] - 0s 224us/step - loss: 0.0315\nEpoch 371/500\n100/100 [==============================] - 0s 252us/step - loss: 0.0313\nEpoch 372/500\n100/100 [==============================] - 0s 276us/step - loss: 0.0310\nEpoch 373/500\n100/100 [==============================] - 0s 222us/step - loss: 0.0308\nEpoch 374/500\n100/100 [==============================] - 0s 257us/step - loss: 0.0306\nEpoch 375/500\n100/100 [==============================] - 0s 274us/step - loss: 0.0304\nEpoch 376/500\n100/100 [==============================] - 0s 272us/step - loss: 0.0301\nEpoch 377/500\n100/100 [==============================] - 0s 243us/step - loss: 0.0299\nEpoch 378/500\n100/100 [==============================] - 0s 221us/step - loss: 0.0297\nEpoch 379/500\n100/100 [==============================] - 0s 213us/step - loss: 0.0294\nEpoch 380/500\n100/100 [==============================] - 0s 223us/step - loss: 0.0292\nEpoch 381/500\n100/100 [==============================] - 0s 271us/step - loss: 0.0290\nEpoch 382/500\n100/100 [==============================] - 0s 255us/step - loss: 0.0288\nEpoch 383/500\n100/100 [==============================] - 0s 231us/step - loss: 0.0286\nEpoch 384/500\n100/100 [==============================] - 0s 259us/step - loss: 0.0284\nEpoch 385/500\n100/100 [==============================] - 0s 261us/step - loss: 0.0282\nEpoch 386/500\n100/100 [==============================] - 0s 279us/step - loss: 0.0280\nEpoch 387/500\n100/100 [==============================] - 0s 248us/step - loss: 0.0279\nEpoch 388/500\n100/100 [==============================] - 0s 240us/step - loss: 0.0277\nEpoch 389/500\n100/100 [==============================] - 0s 309us/step - loss: 0.0275\nEpoch 390/500\n100/100 [==============================] - 0s 216us/step - loss: 0.0273\nEpoch 391/500\n100/100 [==============================] - 0s 221us/step - loss: 0.0271\nEpoch 392/500\n100/100 [==============================] - 0s 267us/step - loss: 0.0269\nEpoch 393/500\n100/100 [==============================] - 0s 262us/step - loss: 0.0267\nEpoch 394/500\n100/100 [==============================] - 0s 234us/step - loss: 0.0266\nEpoch 395/500\n100/100 [==============================] - 0s 216us/step - loss: 0.0265\nEpoch 396/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0262\nEpoch 397/500\n100/100 [==============================] - 0s 226us/step - loss: 0.0260\nEpoch 398/500\n100/100 [==============================] - 0s 230us/step - loss: 0.0258\nEpoch 399/500\n100/100 [==============================] - 0s 240us/step - loss: 0.0256\nEpoch 400/500\n100/100 [==============================] - 0s 300us/step - loss: 0.0255\nEpoch 401/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0253\nEpoch 402/500\n100/100 [==============================] - 0s 262us/step - loss: 0.0251\nEpoch 403/500\n100/100 [==============================] - 0s 248us/step - loss: 0.0250\nEpoch 404/500\n100/100 [==============================] - 0s 311us/step - loss: 0.0249\nEpoch 405/500\n100/100 [==============================] - 0s 327us/step - loss: 0.0248\nEpoch 406/500\n100/100 [==============================] - 0s 246us/step - loss: 0.0245\nEpoch 407/500\n100/100 [==============================] - 0s 283us/step - loss: 0.0244\nEpoch 408/500\n100/100 [==============================] - 0s 262us/step - loss: 0.0242\nEpoch 409/500\n100/100 [==============================] - 0s 232us/step - loss: 0.0240\nEpoch 410/500\n100/100 [==============================] - 0s 250us/step - loss: 0.0239\nEpoch 411/500\n100/100 [==============================] - 0s 238us/step - loss: 0.0237\nEpoch 412/500\n100/100 [==============================] - 0s 266us/step - loss: 0.0236\nEpoch 413/500\n100/100 [==============================] - 0s 232us/step - loss: 0.0234\nEpoch 414/500\n100/100 [==============================] - 0s 230us/step - loss: 0.0233\nEpoch 415/500\n100/100 [==============================] - 0s 260us/step - loss: 0.0231\nEpoch 416/500\n100/100 [==============================] - 0s 236us/step - loss: 0.0230\nEpoch 417/500\n100/100 [==============================] - 0s 278us/step - loss: 0.0228\nEpoch 418/500\n100/100 [==============================] - 0s 256us/step - loss: 0.0227\nEpoch 419/500\n100/100 [==============================] - 0s 247us/step - loss: 0.0227\nEpoch 420/500\n100/100 [==============================] - 0s 221us/step - loss: 0.0225\nEpoch 421/500\n100/100 [==============================] - 0s 222us/step - loss: 0.0223\nEpoch 422/500\n100/100 [==============================] - 0s 229us/step - loss: 0.0221\nEpoch 423/500\n100/100 [==============================] - 0s 244us/step - loss: 0.0220\nEpoch 424/500\n100/100 [==============================] - 0s 297us/step - loss: 0.0218\nEpoch 425/500\n100/100 [==============================] - 0s 330us/step - loss: 0.0217\nEpoch 426/500\n100/100 [==============================] - 0s 296us/step - loss: 0.0215\nEpoch 427/500\n100/100 [==============================] - 0s 316us/step - loss: 0.0214\nEpoch 428/500\n100/100 [==============================] - 0s 316us/step - loss: 0.0213\nEpoch 429/500\n100/100 [==============================] - 0s 225us/step - loss: 0.0211\nEpoch 430/500\n100/100 [==============================] - 0s 278us/step - loss: 0.0210\nEpoch 431/500\n100/100 [==============================] - 0s 274us/step - loss: 0.0210\nEpoch 432/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0208\nEpoch 433/500\n100/100 [==============================] - 0s 256us/step - loss: 0.0206\nEpoch 434/500\n100/100 [==============================] - 0s 270us/step - loss: 0.0205\nEpoch 435/500\n100/100 [==============================] - 0s 235us/step - loss: 0.0203\nEpoch 436/500\n100/100 [==============================] - 0s 243us/step - loss: 0.0202\nEpoch 437/500\n100/100 [==============================] - 0s 232us/step - loss: 0.0201\nEpoch 438/500\n100/100 [==============================] - 0s 221us/step - loss: 0.0200\nEpoch 439/500\n100/100 [==============================] - 0s 231us/step - loss: 0.0198\nEpoch 440/500\n100/100 [==============================] - 0s 269us/step - loss: 0.0197\nEpoch 441/500\n100/100 [==============================] - 0s 254us/step - loss: 0.0196\nEpoch 442/500\n100/100 [==============================] - 0s 276us/step - loss: 0.0195\nEpoch 443/500\n100/100 [==============================] - 0s 248us/step - loss: 0.0194\nEpoch 444/500\n100/100 [==============================] - 0s 240us/step - loss: 0.0192\nEpoch 445/500\n100/100 [==============================] - 0s 238us/step - loss: 0.0191\nEpoch 446/500\n100/100 [==============================] - 0s 218us/step - loss: 0.0190\nEpoch 447/500\n100/100 [==============================] - 0s 231us/step - loss: 0.0189\nEpoch 448/500\n100/100 [==============================] - 0s 267us/step - loss: 0.0187\nEpoch 449/500\n100/100 [==============================] - 0s 252us/step - loss: 0.0186\nEpoch 450/500\n100/100 [==============================] - 0s 237us/step - loss: 0.0185\nEpoch 451/500\n100/100 [==============================] - 0s 266us/step - loss: 0.0183\nEpoch 452/500\n100/100 [==============================] - 0s 249us/step - loss: 0.0182\nEpoch 453/500\n100/100 [==============================] - 0s 261us/step - loss: 0.0181\nEpoch 454/500\n100/100 [==============================] - 0s 212us/step - loss: 0.0180\nEpoch 455/500\n100/100 [==============================] - 0s 278us/step - loss: 0.0179\nEpoch 456/500\n100/100 [==============================] - 0s 246us/step - loss: 0.0177\nEpoch 457/500\n100/100 [==============================] - 0s 251us/step - loss: 0.0175\nEpoch 458/500\n100/100 [==============================] - 0s 232us/step - loss: 0.0174\nEpoch 459/500\n100/100 [==============================] - 0s 284us/step - loss: 0.0173\nEpoch 460/500\n100/100 [==============================] - 0s 241us/step - loss: 0.0172\nEpoch 461/500\n100/100 [==============================] - 0s 222us/step - loss: 0.0170\nEpoch 462/500\n100/100 [==============================] - 0s 254us/step - loss: 0.0169\nEpoch 463/500\n100/100 [==============================] - 0s 224us/step - loss: 0.0168\nEpoch 464/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0167\nEpoch 465/500\n100/100 [==============================] - 0s 237us/step - loss: 0.0165\nEpoch 466/500\n100/100 [==============================] - 0s 267us/step - loss: 0.0164\nEpoch 467/500\n100/100 [==============================] - 0s 256us/step - loss: 0.0163\nEpoch 468/500\n100/100 [==============================] - 0s 224us/step - loss: 0.0162\nEpoch 469/500\n100/100 [==============================] - 0s 272us/step - loss: 0.0161\nEpoch 470/500\n100/100 [==============================] - 0s 240us/step - loss: 0.0160\nEpoch 471/500\n100/100 [==============================] - 0s 217us/step - loss: 0.0158\nEpoch 472/500\n100/100 [==============================] - 0s 240us/step - loss: 0.0157\nEpoch 473/500\n100/100 [==============================] - 0s 278us/step - loss: 0.0156\nEpoch 474/500\n100/100 [==============================] - 0s 227us/step - loss: 0.0155\nEpoch 475/500\n100/100 [==============================] - 0s 270us/step - loss: 0.0154\nEpoch 476/500\n100/100 [==============================] - 0s 221us/step - loss: 0.0153\nEpoch 477/500\n100/100 [==============================] - 0s 226us/step - loss: 0.0151\nEpoch 478/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0150\nEpoch 479/500\n100/100 [==============================] - 0s 236us/step - loss: 0.0149\nEpoch 480/500\n100/100 [==============================] - 0s 235us/step - loss: 0.0148\nEpoch 481/500\n100/100 [==============================] - 0s 289us/step - loss: 0.0147\nEpoch 482/500\n100/100 [==============================] - 0s 268us/step - loss: 0.0145\nEpoch 483/500\n100/100 [==============================] - 0s 324us/step - loss: 0.0145\nEpoch 484/500\n100/100 [==============================] - 0s 239us/step - loss: 0.0144\nEpoch 485/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0142\nEpoch 486/500\n100/100 [==============================] - 0s 250us/step - loss: 0.0141\nEpoch 487/500\n100/100 [==============================] - 0s 273us/step - loss: 0.0140\nEpoch 488/500\n100/100 [==============================] - 0s 288us/step - loss: 0.0139\nEpoch 489/500\n100/100 [==============================] - 0s 249us/step - loss: 0.0138\nEpoch 490/500\n100/100 [==============================] - 0s 238us/step - loss: 0.0137\nEpoch 491/500\n100/100 [==============================] - 0s 212us/step - loss: 0.0136\nEpoch 492/500\n100/100 [==============================] - 0s 235us/step - loss: 0.0135\nEpoch 493/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0134\nEpoch 494/500\n100/100 [==============================] - 0s 218us/step - loss: 0.0133\nEpoch 495/500\n100/100 [==============================] - 0s 248us/step - loss: 0.0131\nEpoch 496/500\n100/100 [==============================] - 0s 261us/step - loss: 0.0131\nEpoch 497/500\n100/100 [==============================] - 0s 253us/step - loss: 0.0130\nEpoch 498/500\n100/100 [==============================] - 0s 251us/step - loss: 0.0128\nEpoch 499/500\n100/100 [==============================] - 0s 264us/step - loss: 0.0127\nEpoch 500/500\n100/100 [==============================] - 0s 261us/step - loss: 0.0126\n\n\nSo, how did the model do? Let‚Äôs check by using the im_in and im_out example images that we made at the start of this section, which the model hasn‚Äôt seen before, to see if it can do the inversion for us.\n\nfig = plt.figure(figsize=(15,5))\nax1,ax2,ax3 = fig.subplots(1,3)\n\npredicted_im_out = simple_model.predict(im_in.reshape(1,8,8,1)).reshape(8,8)   # even though our image was 8x8 to begin with, keras needs the \n                                                                               # shape to match the input shape it expected, so we have to reshape twice\n\nax1.imshow(im_in, cmap=\"Greys\"), ax1.set_title(\"Input\")\nax2.imshow(im_out, cmap=\"Greys\"), ax2.set_title(\"True Output\")\nax3.imshow(predicted_im_out, cmap=\"Greys\"), ax3.set_title(\"Predicted Output\")\n\n(&lt;matplotlib.image.AxesImage&gt;,\n Text(0.5, 1.0, 'Predicted Output'))\n\n\n\n\n\n\n\n\n\nIt doesn‚Äôt do too bad! You can rerun the last 2 cells, training the model for another 500 epochs (it will keep training what it currently has instead of training all over again, unless you re-run model.compile()), and check if it does even better (spoiler: it does). But either way, let‚Äôs move on and look at exactly what‚Äôs happening under the hood of this network a little more closely.\n 5. Interpreting Filters\nSo far, we‚Äôve been talking in detail about what a UNet does in a more conceptual way. But now that we‚Äôve built a real example, let‚Äôs take a closer look at how real data is transformed by a real model. Then, we‚Äôll build a model for a real-world problem, and try to see how much we can visualize and understand from a model doing a much more complicated transformation.\n  5.1 Our Simple Example\nThe simple inversion example that we did didn‚Äôt have many layers or filters. So, we can actually go through and easily look at every single weight our model learned.\nKeras saves the weights that all of our filters had, so it‚Äôs easy to go through and display all of our model‚Äôs filters:\n\n# iterate over all of our layers\nfor layer in simple_model.layers:\n  if \"conv\" in layer.name:       # there are other (non-conv) layers, with no learnable params\n    filters, biases = layer.get_weights()            # filters will have shape (filter_size, filter_size, number channels, number filter)\n    f_min, f_max = filters.min(), filters.max()\n    filters = (filters - f_min) / (f_max - f_min)    # normalize all filters for one layer\n\n    # set up one figure per layer\n    fig = plt.figure(figsize=(12,2))\n    plt.title(\"Layer %s\" %(layer.name)), plt.xticks([]), plt.yticks([]) # say what layer we're on, and box in layers (but without axis ticks)\n    fig_count = 1    # to keep track of subplots\n\n    # Iterate over all the filters in the layer\n    for i in range(filters.shape[-1]):    # 'i' will iterate over the number of filters in that layer\n      ax = fig.add_subplot(1,13,fig_count), plt.axis('off')    # this adds a dummy subplot, just to leave some whitespace between new filters\n      fig_count += 1\n      for j in range(filters.shape[-2]):      # 'j' will iterate over the channels of the filter\n        ax = fig.add_subplot(1,13,fig_count)  # make a new subplot per channel per filter\n        if j == 0:\n          ax.set_title(\"Filter %d\" %((i+1)))  # only title the first channel of the filter, to keep it neat\n        plt.imshow(filters[:,:,j,i],cmap='RdBu'), plt.axis('off')\n        fig_count += 1\n    plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at all of our filters can be useful, but it also can show us a bunch of seemingly random weights, as it does above. What‚Äôs often more useful is to look at the layer activations - or the ‚Äúversion‚Äù of the image that convoling with a filter, adding its bias, and passing through an activation function gives us. For each of the filters above, we can look at the activation that would come from convolving an image with it.\nSo, let‚Äôs use our example image again, and see what the activations for that image are at each layer.\n\nlayer_outputs = [layer.output for layer in simple_model.layers[1:]]     # the first layer is input, but we want all layers after that\nactivation_model = Model(inputs=simple_model.input, outputs=layer_outputs)    # Make a model that returns each layer output given the input\nactivations = activation_model.predict(im_in.reshape(1,8,8,1))    # then, get all the activations for our test image\n\n\nfor layer_num in range(len(activations)):\n  layer_activation = activations[layer_num]\n  fig = plt.figure(figsize = (15,2*np.ceil(layer_activation.shape[-1]/9)))\n  plt.title(\"%s Activations\" %simple_model.layers[layer_num+1].name)\n  plt.axis('off')\n  for filter_num in range(layer_activation.shape[-1]):\n    ax = fig.add_subplot(np.ceil(layer_activation.shape[-1]/9), 9, filter_num+1)\n    ax.matshow(layer_activation[0, :, :, filter_num], cmap='Greys_r')\n    if \"conv\" in simple_model.layers[layer_num+1].name:\n      ax.set_title(\"Filter %d\" %(filter_num+1))    # only have filters in the conv layers, else just call them Im 1/2/etc..\n    else:\n      ax.set_title(\"Im %d\" %(filter_num+1))\n    plt.axis('off')\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, those aren‚Äôt exactly interesting either. There are a few things to point out here:\n\nThe pooling, upsampling, and concatenate ‚Äúactivations‚Äù (there‚Äôs no activation function in these layers so the name doesn‚Äôt make as much sense for them) are plotted, so you can confirm these operations are acting as we expect.\nBecause the activations are after the different channels of the filter have summed, we are showing one image per filter, not one image per filter channel.\n\nBut, even though we can look at all of our weights and activations, we aren‚Äôt getting too much insight here on how our image is being inverted by these operations. A big part of this could be because the task we‚Äôre asking the U-Net to perform - inverting an image - doesn‚Äôt actually care too much about the features in the image. It isn‚Äôt like identifying an eye to figure out that you‚Äôre looking at a face - each pixel in this case could be inverted completely independently from those around it.\n  5.2 A Real-World Example\nSo instead, let‚Äôs take a look at a real-world example, where the features of the image are much more important, and try to see what we can learn about what the U-Net is doing.\nThe example data and U-Net that we‚Äôre going to be using are originally from:\nhttps://github.com/zhixuhao/unet\nBut we‚Äôre going to augment the data differently, and I‚Äôve made the U-Net smaller (in both number of layers and number of filters) to make it more manageable to visualize.\nThe task for this U-Net is image segmentation. Given a grayscale image, we want to create a black/white mask, where areas of interest are in black and all other areas are in white. Let‚Äôs start by loading in the data so we can take a look at an example. We are also going to augment the data as we load it in. That means, we‚Äôll rotate and/or transpose images as we read them in, so that they can become multiple new images to use for training. We‚Äôll also chop each of our images (which begin as 512x512) into multiple smaller images, again, because smaller images will be easier to visualize later.\n\ninput_path = '%s/data/ims' %filepath\noutput_path = '%s/data/labels' %filepath\n\nrotation_angles = [90,180,270]     # set degrees to rotate by, leave [] if no augmentation\nmirror_angles = [Image.TRANSPOSE,Image.FLIP_LEFT_RIGHT,Image.FLIP_TOP_BOTTOM]    # set types of mirroring/transposing the image\n\nsz = 512   # image sizes (one dimension specified, images must be square)\nn_crops = 8   #  number of images (along one dimension) to crop original to. Will end up with n_crops*n_crops images for every original image\n\ncropped_sz = int(sz/n_crops)  # size of image we want to crop to. sz/n_crops must be an integer to divide images evenly\nsz = cropped_sz   # set the new size to the size of the cropped images\n\nX = []\nfor file in np.sort(os.listdir(input_path)):\n  full_im = Image.open('%s/%s' %(input_path, file))\n  for i in range(n_crops):\n    for j in range(n_crops):\n      box = (i*cropped_sz,j*cropped_sz,(i+1)*cropped_sz,(j+1)*cropped_sz)    # get the region of the full image that will become the cropped image\n      im = full_im.crop(box)\n      X.append(np.array(im).reshape(sz,sz,1))\n      for angle in rotation_angles:     # rotate images on all angles\n        rotated_im = im.rotate(angle)\n        X.append(np.array(rotated_im).reshape(sz,sz,1))  # add rotated im as new one\n      for angle in mirror_angles:    # transpose images on all angles\n        mirror_im = im.transpose(angle)\n        X.append(np.array(mirror_im).reshape(sz,sz,1))  # add transposed im as new one\n\n# build y in the same way as X, with images and rotations in the same order to they match each other\ny = []\nfor file in np.sort(os.listdir(output_path)):\n  full_im = Image.open('%s/%s' %(output_path, file))\n  for i in range(n_crops):\n    for j in range(n_crops):\n      box = (i*cropped_sz,j*cropped_sz,(i+1)*cropped_sz,(j+1)*cropped_sz)\n      im = full_im.crop(box)\n      y.append(np.array(im).reshape(sz,sz,1))\n      for angle in rotation_angles:\n        rotated_im = im.rotate(angle)\n        y.append(np.array(rotated_im).reshape(sz,sz,1))\n      for angle in mirror_angles:\n        mirror_im = im.transpose(angle)\n        y.append(np.array(mirror_im).reshape(sz,sz,1))\n\ninput_size = X[0].shape\n\nX = np.array(X)\ny = np.array(y)\n\n# scale data to a 0-1 range\nX = (X-np.min(X))/(np.max(X)-np.min(X))\ny = (y-np.min(y))/(np.max(y)-np.min(y))\n\nNow let‚Äôs look at some examples of our images, and confirm that our augmentations worked properly.\n\nfig = plt.figure(figsize=(15,9))\nax1 = fig.add_subplot(2,4,1)\nax1.imshow(X[0].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Input Image\")\nax2 = fig.add_subplot(2,4,2)\nax2.imshow(y[0].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Output Mask\")\nax3 = fig.add_subplot(2,4,5)\nax3.imshow(X[3].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Input Image, Rotated 270\")\nax4 = fig.add_subplot(2,4,6)\nax4.imshow(y[3].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Output Mask, Rotated 270\")\nax5 = fig.add_subplot(2,4,3)\nax5.imshow(X[0].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Input Image\")\nax6 = fig.add_subplot(2,4,4)\nax6.imshow(y[0].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Output Mask\")\nax7 = fig.add_subplot(2,4,7)\nax7.imshow(X[5].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Input Image, Flip Left/Right\")\nax8 = fig.add_subplot(2,4,8)\nax8.imshow(y[5].reshape(sz,sz),cmap='Greys_r'), plt.title(\"Output Mask, Flip Left/Right\")\nplt.tight_layout()\n\nprint(\"We now have %d pieces of data\" %len(X))\n\nWe now have 13440 pieces of data\n\n\n\n\n\n\n\n\n\nNow, let‚Äôs build the model that we‚Äôll train on this problem. We‚Äôre going to build this model exactly the same way that we built the very simple model, but it will have more layers and more filters.\n\ninputs = Input(input_size)\n# conv block 1\nconv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\nconv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n# conv block 2\nconv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\nconv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n# conv block 3\nconv3 = Conv2D(64, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\nconv3 = Conv2D(64, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\npool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n# conv block 4\nconv4 = Conv2D(128, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\nconv4 = Conv2D(128, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\npool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n# upconv block 1\nup7 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv4))\nconcat7 = concatenate([conv3,up7], axis = 3)\nconv7 = Conv2D(64, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(concat7)\nconv7 = Conv2D(64, 5, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n# upconv block 2\nup8 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\nconcat8 = concatenate([conv2,up8], axis = 3)\nconv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(concat8)\nconv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n# upconv block 3\nup9 = Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\nconcat9 = concatenate([conv1,up9], axis = 3)\nconv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(concat9)\nconv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\nconv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n# output\nconv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\nmodel = Model(input = inputs, output = conv10)\nmodel.compile(optimizer = Adam(lr = 1e-4), loss = 'mse', metrics = ['accuracy'])\nmodel.summary()\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nModel: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            (None, 64, 64, 1)    0                                            \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 64, 64, 16)   160         input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 16)   0           conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 32)   0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 16, 16, 64)   51264       max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 16, 16, 64)   102464      conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 8, 8, 128)    204928      max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 8, 8, 128)    409728      conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_2 (UpSampling2D)  (None, 16, 16, 128)  0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 16, 16, 64)   32832       up_sampling2d_2[0][0]            \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 16, 16, 128)  0           conv2d_13[0][0]                  \n                                                                 conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 16, 16, 64)   204864      concatenate_2[0][0]              \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 16, 16, 64)   102464      conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_3 (UpSampling2D)  (None, 32, 32, 64)   0           conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 32, 32, 32)   8224        up_sampling2d_3[0][0]            \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 32, 32, 64)   0           conv2d_11[0][0]                  \n                                                                 conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 32, 32, 32)   18464       concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 32, 32, 32)   9248        conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_4 (UpSampling2D)  (None, 64, 64, 32)   0           conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 64, 64, 16)   2064        up_sampling2d_4[0][0]            \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 64, 64, 32)   0           conv2d_9[0][0]                   \n                                                                 conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 64, 64, 16)   4624        concatenate_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 64, 64, 16)   2320        conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 64, 64, 2)    290         conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 64, 64, 1)    3           conv2d_25[0][0]                  \n==================================================================================================\nTotal params: 1,170,149\nTrainable params: 1,170,149\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n\n\nLet‚Äôs train this model:\n Note: this model is a bit more complicated than our very simple one, so this cell could take some time to run.\n\nmodel_history = model.fit(X, y, epochs = 100, verbose = 1,batch_size=100)\n\nEpoch 1/100\n13440/13440 [==============================] - 13s 951us/step - loss: 0.1271 - acc: 0.8048\nEpoch 2/100\n13440/13440 [==============================] - 10s 725us/step - loss: 0.1084 - acc: 0.8543\nEpoch 3/100\n13440/13440 [==============================] - 10s 727us/step - loss: 0.1045 - acc: 0.8678\nEpoch 4/100\n13440/13440 [==============================] - 10s 730us/step - loss: 0.1014 - acc: 0.8769\nEpoch 5/100\n13440/13440 [==============================] - 10s 737us/step - loss: 0.0985 - acc: 0.8839\nEpoch 6/100\n13440/13440 [==============================] - 10s 737us/step - loss: 0.0959 - acc: 0.8895\nEpoch 7/100\n13440/13440 [==============================] - 10s 740us/step - loss: 0.0938 - acc: 0.8932\nEpoch 8/100\n13440/13440 [==============================] - 10s 745us/step - loss: 0.0915 - acc: 0.8972\nEpoch 9/100\n13440/13440 [==============================] - 10s 745us/step - loss: 0.0897 - acc: 0.9000\nEpoch 10/100\n13440/13440 [==============================] - 10s 749us/step - loss: 0.0879 - acc: 0.9029\nEpoch 11/100\n13440/13440 [==============================] - 10s 751us/step - loss: 0.0864 - acc: 0.9050\nEpoch 12/100\n13440/13440 [==============================] - 10s 764us/step - loss: 0.0853 - acc: 0.9066\nEpoch 13/100\n13440/13440 [==============================] - 10s 761us/step - loss: 0.0838 - acc: 0.9088\nEpoch 14/100\n13440/13440 [==============================] - 10s 762us/step - loss: 0.0827 - acc: 0.9103\nEpoch 15/100\n13440/13440 [==============================] - 10s 761us/step - loss: 0.0811 - acc: 0.9129\nEpoch 16/100\n13440/13440 [==============================] - 10s 766us/step - loss: 0.0800 - acc: 0.9141\nEpoch 17/100\n13440/13440 [==============================] - 10s 766us/step - loss: 0.0790 - acc: 0.9155\nEpoch 18/100\n13440/13440 [==============================] - 10s 770us/step - loss: 0.0777 - acc: 0.9174\nEpoch 19/100\n13440/13440 [==============================] - 10s 775us/step - loss: 0.0762 - acc: 0.9196\nEpoch 20/100\n13440/13440 [==============================] - 10s 774us/step - loss: 0.0754 - acc: 0.9206\nEpoch 21/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0744 - acc: 0.9220\nEpoch 22/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0737 - acc: 0.9227\nEpoch 23/100\n13440/13440 [==============================] - 10s 775us/step - loss: 0.0731 - acc: 0.9231\nEpoch 24/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0717 - acc: 0.9254\nEpoch 25/100\n13440/13440 [==============================] - 10s 775us/step - loss: 0.0708 - acc: 0.9264\nEpoch 26/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0701 - acc: 0.9272\nEpoch 27/100\n13440/13440 [==============================] - 10s 780us/step - loss: 0.0693 - acc: 0.9280\nEpoch 28/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0687 - acc: 0.9285\nEpoch 29/100\n13440/13440 [==============================] - 10s 774us/step - loss: 0.0679 - acc: 0.9295\nEpoch 30/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0672 - acc: 0.9304\nEpoch 31/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0664 - acc: 0.9311\nEpoch 32/100\n13440/13440 [==============================] - 10s 780us/step - loss: 0.0658 - acc: 0.9318\nEpoch 33/100\n13440/13440 [==============================] - 10s 781us/step - loss: 0.0652 - acc: 0.9322\nEpoch 34/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0645 - acc: 0.9330\nEpoch 35/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0646 - acc: 0.9321\nEpoch 36/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0635 - acc: 0.9338\nEpoch 37/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0627 - acc: 0.9348\nEpoch 38/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0623 - acc: 0.9349\nEpoch 39/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0615 - acc: 0.9359\nEpoch 40/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0610 - acc: 0.9363\nEpoch 41/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0605 - acc: 0.9367\nEpoch 42/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0600 - acc: 0.9371\nEpoch 43/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0594 - acc: 0.9376\nEpoch 44/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0589 - acc: 0.9380\nEpoch 45/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0584 - acc: 0.9386\nEpoch 46/100\n13440/13440 [==============================] - 10s 780us/step - loss: 0.0578 - acc: 0.9391\nEpoch 47/100\n13440/13440 [==============================] - 10s 774us/step - loss: 0.0575 - acc: 0.9393\nEpoch 48/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0568 - acc: 0.9400\nEpoch 49/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0562 - acc: 0.9406\nEpoch 50/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0559 - acc: 0.9407\nEpoch 51/100\n13440/13440 [==============================] - 10s 781us/step - loss: 0.0554 - acc: 0.9412\nEpoch 52/100\n13440/13440 [==============================] - 10s 781us/step - loss: 0.0549 - acc: 0.9417\nEpoch 53/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0542 - acc: 0.9424\nEpoch 54/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0538 - acc: 0.9428\nEpoch 55/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0534 - acc: 0.9431\nEpoch 56/100\n13440/13440 [==============================] - 10s 780us/step - loss: 0.0531 - acc: 0.9432\nEpoch 57/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0525 - acc: 0.9438\nEpoch 58/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0520 - acc: 0.9442\nEpoch 59/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0515 - acc: 0.9448\nEpoch 60/100\n13440/13440 [==============================] - 10s 775us/step - loss: 0.0509 - acc: 0.9455\nEpoch 61/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0507 - acc: 0.9454\nEpoch 62/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0501 - acc: 0.9462\nEpoch 63/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0496 - acc: 0.9466\nEpoch 64/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0494 - acc: 0.9466\nEpoch 65/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0490 - acc: 0.9470\nEpoch 66/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0482 - acc: 0.9479\nEpoch 67/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0479 - acc: 0.9481\nEpoch 68/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0475 - acc: 0.9484\nEpoch 69/100\n13440/13440 [==============================] - 10s 780us/step - loss: 0.0469 - acc: 0.9492\nEpoch 70/100\n13440/13440 [==============================] - 11s 783us/step - loss: 0.0465 - acc: 0.9496\nEpoch 71/100\n13440/13440 [==============================] - 10s 780us/step - loss: 0.0460 - acc: 0.9500\nEpoch 72/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0460 - acc: 0.9498\nEpoch 73/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0454 - acc: 0.9504\nEpoch 74/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0450 - acc: 0.9508\nEpoch 75/100\n13440/13440 [==============================] - 10s 778us/step - loss: 0.0445 - acc: 0.9514\nEpoch 76/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0441 - acc: 0.9518\nEpoch 77/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0441 - acc: 0.9515\nEpoch 78/100\n13440/13440 [==============================] - 10s 774us/step - loss: 0.0432 - acc: 0.9527\nEpoch 79/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0426 - acc: 0.9535\nEpoch 80/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0423 - acc: 0.9536\nEpoch 81/100\n13440/13440 [==============================] - 11s 782us/step - loss: 0.0419 - acc: 0.9541\nEpoch 82/100\n13440/13440 [==============================] - 10s 781us/step - loss: 0.0415 - acc: 0.9544\nEpoch 83/100\n13440/13440 [==============================] - 11s 785us/step - loss: 0.0411 - acc: 0.9548\nEpoch 84/100\n13440/13440 [==============================] - 11s 786us/step - loss: 0.0408 - acc: 0.9550\nEpoch 85/100\n13440/13440 [==============================] - 11s 786us/step - loss: 0.0408 - acc: 0.9547\nEpoch 86/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0400 - acc: 0.9558\nEpoch 87/100\n13440/13440 [==============================] - 11s 782us/step - loss: 0.0398 - acc: 0.9560\nEpoch 88/100\n13440/13440 [==============================] - 10s 775us/step - loss: 0.0395 - acc: 0.9562\nEpoch 89/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0388 - acc: 0.9571\nEpoch 90/100\n13440/13440 [==============================] - 11s 782us/step - loss: 0.0384 - acc: 0.9575\nEpoch 91/100\n13440/13440 [==============================] - 10s 775us/step - loss: 0.0383 - acc: 0.9574\nEpoch 92/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0379 - acc: 0.9579\nEpoch 93/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0374 - acc: 0.9584\nEpoch 94/100\n13440/13440 [==============================] - 10s 779us/step - loss: 0.0370 - acc: 0.9588\nEpoch 95/100\n13440/13440 [==============================] - 10s 776us/step - loss: 0.0367 - acc: 0.9592\nEpoch 96/100\n13440/13440 [==============================] - 10s 775us/step - loss: 0.0363 - acc: 0.9596\nEpoch 97/100\n13440/13440 [==============================] - 10s 777us/step - loss: 0.0362 - acc: 0.9595\nEpoch 98/100\n13440/13440 [==============================] - 10s 780us/step - loss: 0.0358 - acc: 0.9599\nEpoch 99/100\n13440/13440 [==============================] - 11s 788us/step - loss: 0.0355 - acc: 0.9603\nEpoch 100/100\n13440/13440 [==============================] - 11s 786us/step - loss: 0.0353 - acc: 0.9604\n\n\nNow, before we take a closer look at what the model is doing, let‚Äôs look at how accurately it was able to produce our desired masks to see if we‚Äôve sucessfully completed our task.\n\ndata_num = 2500  # can change to look at a different image and its prediction\n\ninput_im = X[data_num].reshape(sz,sz)\npredicted_im = model.predict(X[data_num:data_num+1]).reshape(sz,sz)\noutput_im = y[data_num].reshape(sz,sz)\n\nfig = plt.figure(figsize=(15,5))\nax1, ax2, ax3 = fig.subplots(1,3)\nax1.imshow(input_im,cmap='Greys_r'), ax1.set_title(\"Input\")\nax2.imshow(predicted_im,cmap='Greys_r'), ax2.set_title(\"Predicted Output\")\nax3.imshow(output_im,cmap='Greys_r'), ax3.set_title(\"True Output\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAnd it looks pretty good! Okay, so let‚Äôs look at the filters again for this network, and see if any of them seem to stand out as picking out some features for us.\nWe‚Äôll do this in the exact same way that we did above, except we‚Äôll only look at the filters for the first convolutional layer of the first convolutional block. This is because we have way more filters in this example than we did in the last example, so even in the second convolution of the first convolution block involves filters with 16 channels, so visualizing all of them would be extremely hard.\n\nfor layer in model.layers[1:2]:\n  if \"conv\" in layer.name:\n    filters, biases = layer.get_weights()\n    f_min, f_max = filters.min(), filters.max()\n    filters = (filters - f_min) / (f_max - f_min)\n\n    fig = plt.figure(figsize=(15,5))\n    plt.title(\"Layer %s\" %layer.name)\n    plt.axis('off')\n    for i in range(filters.shape[-1]):\n      ax = fig.add_subplot(np.ceil(filters.shape[-1]/8),8,i+1)\n      plt.imshow(filters[:,:,:,i].reshape(filters.shape[0],filters.shape[1]),cmap='RdBu')\n      plt.axis('off')\n    plt.tight_layout()\n\n\n\n\n\n\n\n\nIt might not be any more evident in this real-world example than it was in our very simple example what these filters are doing. But, now that our images have significant structure, we might be able to figure out what these filters are telling us. Let‚Äôs look at the activations for this first convolutional layer to see if anything jumps out.\n\nlayer_outputs = [layer.output for layer in model.layers[1:]] \nactivation_model = Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\nactivations = activation_model.predict(input_im.reshape(1,64,64,1)) \n\nfor layer_num in range(1):\n  layer_activation = activations[layer_num]\n  fig = plt.figure(figsize = (15,5))\n  plt.title(\"%s Activations\" %model.layers[layer_num+1].name)\n  plt.axis('off')\n  for filter_num in range(layer_activation.shape[-1]):\n    ax = fig.add_subplot(np.ceil(layer_activation.shape[-1]/8), 8, filter_num+1)\n    ax.imshow(layer_activation[0, :, :, filter_num], cmap='Greys_r')\n    plt.axis('off')\n  plt.tight_layout()\n\n\n\n\n\n\n\n\nNow, if you‚Äôve just been reading the cells of this notebook without re-running them, you have the same filters and activations as I do. Otherwise, if you‚Äôve been re-running the cells, and you‚Äôve re-trained the model, your filters and activations will be different than mine. But, you should see that the activations all look like versions of the image that are meant to pick out something.\nIn the cells below, I‚Äôm going to manually load in some of the filters and activations that I have in my notebook, and talk about them. If you didn‚Äôt re-run the notebook, this will be filters and activations 2,6, and 7 (python-indexed) from above. Otherwise, your filters and activations will be different, but likely some will still be relatively similar, to mine.\nSo, here are the filters we‚Äôre going to look at:\n\nfilt_2 = np.array([[0.6744713 , 0.69218516, 0.6413911 ],\n                   [0.36131164, 0.426186  , 0.4645831 ],\n                   [0.534594  , 0.5280584 , 0.4137286 ]])\n\nfilt_6 = np.array([[0.7352305,  0.61722165, 0.70441914],\n                   [0.85438216, 0.61504716, 0.7480632 ],\n                   [0.16859733, 0.65413386, 0.84601194]])\n\nfilt_7 = np.array([[0.6218604,  0.25640523, 0.45756572],\n                   [0.5342595,  0.20999649, 0.53925157],\n                   [0.3942946,  0.810168,   0.5870034 ]])\n\nfig = plt.figure(figsize=(15,5))\nax1,ax2,ax3 = fig.subplots(1,3)\nax1.imshow(filt_2,cmap='RdBu'), ax1.set_title(\"Filter 1\")\nax2.imshow(filt_6,cmap='RdBu'), ax2.set_title(\"Filter 2\")\nax3.imshow(filt_7,cmap='RdBu'), ax3.set_title(\"Filter 3\")\n\n\n\n\n\n\n\n\nNow, let‚Äôs look at the activations these filters gave:\n\ndisplay(Im('%s/images/realWorld_example_activations.png' %filepath, height=270, width=1000))\n\n\n\n\n\n\n\n\nSo, let‚Äôs try to speculate what these filters might be doing.\n\nFilter #1 looks like it might be a horizontal/diagonal edge detector. It seems to pick out the strongest borders between black and and white pixels in the original image, provided that border isn‚Äôt vertical.\nFilter #2 doesn‚Äôt seem to change the image significantly, besides blurring it somewhat to smooth out the lighter sections.\nFilter #3 seems to only emphasize the darkest regions of the original image: the highest concentration of black pixels in the original image are highlighted, and the rest of the image is uniform.\n\nIf we go back and look at our filters, is there any evidence that this is what they‚Äôre doing? Well, maybe. If you look at the structure of them, you can pick out that Filter #1 might have differing values horizontally/diagonally which might be giving us the horizontal edges, or that Filter #2 might be keeping information from all the cells about equally which blurs the image out, but it‚Äôs easier to look at the activation and say what it‚Äôs doing based off of that than to make guesses based on the weights.\n  5.3 Deeper Layers\nAnd what about the deeper layers?\nWell, the activations are easy enough to visualize. Although in the layers with 64 or 128 filters, this would mean looking at 128 images, that‚Äôs much easier than trying to look at the filters, which will end up with 128 channels.\nHowever, it can prove more challenging to interpret later activations because they are now convolved versions of already convolved versions of our images. So while these activations can still provide valuable insight at every level, it‚Äôs difficult to straightforwardly interpret what exactly the UNet is doing.\nStill, it‚Äôs clearly an incredibly powerful tool for image transformation, which hopefully you now have a little better of an understanding/intuition about!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Jupyter Notebook Tutorials",
    "section": "",
    "text": "Fine Tuning LLaVA with and without LoRA\n\n\n\n\n\n\ntraining\n\n\nPEFT\n\n\nvision\n\n\nLLMs\n\n\n\nIn this notebook, we dive deep into the architecture of LLaVA, with the goal of fine-tuning it (both with and without LoRA) to adapt it to determining the morphologies of GalaxyZoo2 images.\n\n\n\n\n\nAug 6, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\n\n\n\n\n\n\nPEFT Deep Dive: LoRA\n\n\n\n\n\n\ntraining\n\n\nPEFT\n\n\n\nMathematical foundations and practical implementation of LoRA, one of the most common paremeter efficient fine tuning methods.\n\n\n\n\n\nAug 5, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\n\n\n\n\n\n\nRL for LLMs\n\n\n\n\n\n\nreinforcement-learning\n\n\ntraining\n\n\nLLMs\n\n\n\nAn in-depth guide to reinforcement learning using proximal policy optimization (PPO), with a focus as it applies to modern large language models (and with human feedback!).\n\n\n\n\n\nFeb 25, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Caching\n\n\n\n\n\n\ncaching\n\n\nLLMs\n\n\ninferencing\n\n\n\nA guide to how prompt caching - a modular approach to KV caching. We‚Äôll talk through how and why this modular implementation works, and include a practical example of implementing it for LLaMa 3.2 1B.\n\n\n\n\n\nJan 10, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\n\n\n\n\n\n\nKV Caching\n\n\n\n\n\n\ncaching\n\n\nLLMs\n\n\ninferencing\n\n\n\nA guide to how KV caching is implemented for LLMs, including a practical example of implementing it for LLaMa 3.2 1B.\n\n\n\n\n\nJan 9, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\n\n\n\n\n\n\nThe U-Net Architecture\n\n\n\n\n\n\narchitectures\n\n\nCNNs\n\n\nvision\n\n\n\nA complete guide to the U-Net architecture. Covers all aspects of a layer, from the convolution operation, to pooling, to what makes U-Nets so special.\n\n\n\n\n\nJan 31, 2020\n\n\nAbbie Petulante\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Tutorials Blog!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2025\n\n\nAbbie Petulante\n\n\n\n\n\n\nNo matching items"
  }
]