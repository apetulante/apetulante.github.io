<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abbie Petulante">
<meta name="dcterms.date" content="2025-08-06">
<meta name="description" content="In this notebook, we dive deep into the architecture of LLaVA, with the goal of fine-tuning it (both with and without LoRA) to adapt it to determining the morphologies of GalaxyZoo2 images.">

<title>Fine Tuning LLaVA with and without LoRA – Abbie’s AI Tutorials</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Abbie’s AI Tutorials</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/apetulante"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/apetulante/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fine Tuning LLaVA with and without LoRA</h1>
                  <div>
        <div class="description">
          In this notebook, we dive deep into the architecture of LLaVA, with the goal of fine-tuning it (both with and without LoRA) to adapt it to determining the morphologies of GalaxyZoo2 images.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">training</div>
                <div class="quarto-category">PEFT</div>
                <div class="quarto-category">vision</div>
                <div class="quarto-category">LLMs</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abbie Petulante </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 6, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="fine-tuning-llava-on-astronomical-data" class="level1">
<h1>Fine-Tuning LLaVA on Astronomical Data</h1>
<p>In this notebook, we fine tune LLaVA (Large Language and Vision Assistant), a multimodal VLM (vision-language model) to be better adapted to describing astronomical images.</p>
<hr>
<section id="what-is-llava" class="level4">
<h4 class="anchored" data-anchor-id="what-is-llava">What is LLaVA?</h4>
<p>LLaVA is a vision-language model that combines a vision encoder (to “see” images) with a language model (to generate text), enabling it to answer questions about images and generate detailed descriptions. Originally trained on millions of general image-caption pairs, LLaVA can already describe everyday objects, scenes, and basic visual properties.</p>
<div id="cell-2" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image <span class="im">as</span> iImage, display</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>display(iImage(<span class="st">"images/llava_example.png"</span>, width<span class="op">=</span><span class="dv">800</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img" width="800"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="why-adapt-llava-for-astronomy" class="level4">
<h4 class="anchored" data-anchor-id="why-adapt-llava-for-astronomy">Why Adapt LLaVA for Astronomy?</h4>
<p>In this notebook, we’re going to try to fine tune LLaVA on the task of GalaxyZoo - labeled morphological properties of pictures of galaxies. But why choose LLaVA for this task, rather than, say a regular image model + a classification head? Vision-language models offer unique advantages:</p>
<ol type="1">
<li><strong>Flexibility</strong>: Instead of fixed categories, a vision-language model can describe novel or ambiguous features in natural language</li>
<li><strong>Interpretability</strong>: Models that describe why what they see points to a classification can be more interpretable, and we could ask follow-up questions to better understand why decisions were made</li>
<li><strong>Multi-task capability</strong>: A VLM like LLaVA is more flexible to the multiple morphological categories. It can classify spiral vs elliptical easily, and the absence/presence of more fine-grained features, and could be easily adapted later to include more if desired.</li>
</ol>
<p>By fine-tuning on Galaxy Zoo data, we’ll teach it to adapt to a new domain, recognizing and describing the astronomical features of galaxy morphologies. This can make use of all of its existing knowledge for how to look at images, but help it figure out what we want it to focus on for images of galaxies, and how to talk about them.</p>
<p>Let’s get started!</p>
</section>
<section id="basic-setup" class="level2">
<h2 class="anchored" data-anchor-id="basic-setup"><strong>0. Basic Setup</strong></h2>
<p>I’ll assume this notebook is running on <strong>Google colab</strong>.</p>
<p>If so, we need to install some packages before we proceed. After the below cell runs, <strong>restart the kernel</strong> to be sure the notebook has access to these packages.</p>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q bitsandbytes</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q peft</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q <span class="op">--</span>upgrade ipywidgets<span class="op">==</span><span class="fl">8.1.7</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 MB 34.5 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 2.9 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 120.2 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 96.1 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 56.0 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.1 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 6.0 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 41.0 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 18.9 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 3.9 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 105.3 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.8/139.8 kB 6.8 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 61.8 MB/s eta 0:00:00
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 81.7 MB/s eta 0:00:00</code></pre>
</div>
</div>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Any</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime, timedelta</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> hashlib</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoProcessor, LlavaForConditionalGeneration, TrainingArguments, Trainer</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="an-introduction-to-llava" class="level2">
<h2 class="anchored" data-anchor-id="an-introduction-to-llava"><strong>1. An Introduction to LLaVA</strong></h2>
<p><a href="https://llava-vl.github.io/">LLaVA</a> (Large Language and Vision Assistant) is a model that was introduced by Liu et al.&nbsp;in the <a href="https://arxiv.org/pdf/2304.08485">Vision Instruction Tuning</a> paper in late 2023.</p>
<p>It grafts a vision encoder onto a causal-LM so the model can “read” an image before predicting text. As such, it has multiple transformer components working in tandem:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 40%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What it is</th>
<th>Shape</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Vision encoder</strong></td>
<td>ViT-L/14 (CLIP) patch-embed layer, frozen</td>
<td>3 × 336 × 336 → 257 × 1024</td>
</tr>
<tr class="even">
<td><strong>Projector</strong></td>
<td>2-layer MLP (GELU)</td>
<td>1024 → 4096 (or 2048) (LLaMA hidden)</td>
</tr>
<tr class="odd">
<td><strong>Language model</strong></td>
<td>LLaMA decoder-only Transformer</td>
<td>4096-d (LLaMA), 2048-d (LLaMA-Tiny)</td>
</tr>
</tbody>
</table>
<p>Before we get to training this model, we’ll look thoroughly at its architecture and training procedure to understand what it’s capable of!</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> In this tutorial, we’ll be training on LLaVA-tiny, so we’ll focus on the more fine-grained details there where specified. There’s nothing special about LLaVA-tiny besides that these underlying components were chosen to be small - the same principles about how the models were stitched together and trained applies to the larger LLaVA versions as well.</p>
</blockquote>
<section id="the-vision-model" class="level3">
<h3 class="anchored" data-anchor-id="the-vision-model"><strong>The Vision Model</strong></h3>
<p>In order for LLaVA to be able to “look” at images, it needs an encoder portion that turns an image into meaningful embeddings.</p>
<hr>
<p>For this, LLaVA uses a <strong>Vision Transformer <a href="https://arxiv.org/pdf/2010.11929">(ViT)</a></strong> model — specifically, a large version known as <strong>ViT-L/14</strong>. This same vision encoder is used by both larger LLaVA models and LLaVA-tiny.</p>
<p><strong>ViT-L/14</strong>: - Has 24 transformer blocks (the “L” = the “Large” version). - Splits the image into patches of <strong>14 × 14 pixels</strong> (the “14”). - Processes images of <strong>336 × 336 pixels</strong>, which gives you <strong>24 × 24 = 576 patches</strong>, plus one extra special token (577 total tokens).</p>
<p>Each of these patches is treated a bit like a “visual word” — it gets turned into a <strong>1024-dimensional vector</strong> that summarizes the visual content in that small chunk of the image.</p>
<section id="how-a-vit-patches-images" class="level4">
<h4 class="anchored" data-anchor-id="how-a-vit-patches-images"><strong>How a ViT Patches Images</strong></h4>
<p>Let’s take a step back and look at how a ViT turns an image into a tokenized sequence.</p>
<p>Images are of course made of <em>pixels</em> - and ViT starts with images that are <strong>336x336 pixels</strong>. Transformer architectures fundamentally want to make use of <em>sequences</em>. While pixels are naturally numeric, flattening all pixels of an image, even of 336x336 (which is relatively small!) = 112896 values - too large to efficiently process, and probably not worth it in terms of the information actually contained in all individual pixel values.</p>
<p>The core innovation behind ViT is to sequence <strong>patches</strong> of the image. Small enough patches of an image can be considered to contain approximately one “thing” and thus can be treated as a single region of focus in the image. 24 x 24 patches taken over an image that’s 336x336 = 576 values - much better.</p>
<div id="cell-10" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>display(iImage(<span class="st">"images/pixels_vs_patches.png"</span>, width<span class="op">=</span><span class="dv">1000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img" width="1000"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="how-patches-are-transformed" class="level4">
<h4 class="anchored" data-anchor-id="how-patches-are-transformed"><strong>How Patches are Transformed</strong></h4>
<p>The next step is to use those patches to actually get meaningful information about the image.</p>
<p>For each patch {1, 2, …. 576}:</p>
<ol type="1">
<li>Flatten and Project</li>
</ol>
<ul>
<li>Flatten it into a single 1D vector of pixel values</li>
<li>Pass that through a <strong>learnable linear projection layer</strong>, which maps that pixel sequence into a <strong>1024-dimensional embedding</strong> that is a summary of the patch’s visual contents.</li>
</ul>
<ol start="2" type="1">
<li>Add Positional Information</li>
</ol>
<ul>
<li>Each patch has a corresponding <strong>learned positional embedding</strong> — a fixed vector (same shape as the patch embedding) that represents its location.</li>
<li>The model adds this positional vector to the patch embedding.</li>
</ul>
<p>So for each patch:</p>
<p>[embedding] = [patch vector] + [position vector]</p>
<p>This lets the model reason about <em>spatial layout</em>, not just content.</p>
<details>
<summary>
📌 Note: Why 196 → 1024 projection?
</summary>
<p>You might be wondering why we do the projection to 1024 values from 196. We talked about how 336x336 pixels -&gt; 112896 is a “waste”, but 24x24 patches of size 1024 = 589824 - <em>more</em> information than just our pixel values!</p>
<p>So <strong>why</strong> do we do this?</p>
<p>It might seem like just an inflation of the data, but doing this projection makes the information <strong>richer</strong> and more suitable for the Transformer to use. The 1024 vectors that come out of the linear projection layer are contextualized, rich embeddings of the pixel information, and provide <strong>more meaningful</strong> information than pixel values alone.</p>
<ul>
<li>Transformers expect high-dim input (e.g., LLaMA uses 2048-d text tokens)</li>
<li>Raw pixels are low-level; projection lets the model learn abstract features</li>
<li>The number of <em>tokens</em> works out to still just 576 (one per patch), so compute stays reasonable (we’ll see this in more detail soon)</li>
</ul>
<p>So think of this transformation like it’s converting an image patch into a dense “visual word.”</p>
</details>
<div id="cell-12" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>display(iImage(<span class="st">"images/processing_patches.png"</span>, width<span class="op">=</span><span class="dv">800</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img" width="800"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="how-each-transformer-layer-operates" class="level4">
<h4 class="anchored" data-anchor-id="how-each-transformer-layer-operates"><strong>How Each Transformer Layer Operates</strong></h4>
<p>Once we have this rich embedding for each of our patches, it’s time for the core transformer architecture to do its thing. Each transformer layer works to model <strong>relationships between patches</strong> using self-attention.</p>
<p>At each layer:</p>
<ol type="1">
<li><p><strong>Attention mechanism</strong><br>
Each patch embedding goes through fully connected layers to create Query, Key, and Value representations. Each patch “looks” at every other patch and decides how much attention to pay to each one.<br>
→ This creates a big 576 × 576 matrix of attention scores.<br>
→ Each patch gets updated as a <strong>weighted combination</strong> of all the others. → The result goes through another fully connected layer to produce the final attention output.</p></li>
<li><p><strong>Multi-Layer Perceptron (MLP)</strong><br>
After attention, each updated embedding goes through a seperate feedforward neural network. This typically expands the embedding to a larger size, applies a non-linearity, then contracts back to the original size.</p></li>
<li><p><strong>Output</strong><br>
We now have a new set of patch embeddings — same shape (576 × 1024), but now <strong>contextualized</strong>:<br>
Each patch now contains information not just about itself, but also about the rest of the image.</p></li>
</ol>
<p>In ViT-L which has 24 layers, this process is repeated 24 times. And so, the embedding of an image coming out of ViT is size <strong>576 x 1024</strong> representing 576, 1024-size embeddings that describe the image.</p>
<p>Below, we take a closer look at how the <strong>attention portion</strong> of one of these layers works.</p>
<details>
<summary>
📌 Revisiting that 1024 Dimension:
</summary>
<p>Ok, the point that we made above:</p>
<p>“The number of tokens works out to still just 576 (one per patch), so compute stays reasonable”</p>
<p>might not have immediately made sense. And as we just discussed, every layer of ViT will compute over this 576 x 1024 matrix, so we’ll constantly be passing around those 589824 values of information. It might have made sense why this projection to 1024 is <em>more meaningful</em>, but how is it also <em>more computationally efficient</em> than dealing with our raw pixel values (112896)?</p>
<p>This is because, thanks to the attention mechanism, transformers scale <em>quadratically</em> with sequence length (<span class="math inline">\(L\)</span>) but only <em>linearly</em> with embedding dimension (<span class="math inline">\(D\)</span>). This really comes down to the fact that the attention mechanism contains a <strong>dot product</strong> calculation of an <span class="math inline">\(L \times D\)</span> and <span class="math inline">\(D \times L\)</span> matrix, which requires that every row be multiplied by every column. Therefore, every increase in <span class="math inline">\(L\)</span> requires <span class="math inline">\(L^2\)</span> calculations.</p>
In ViT, the <strong>token count</strong> that assigns one token per patch is our sequence length. So, we save tons of compute using patches <em>to make the sequence length smaller for the transformer layers</em>. That’s why patch-based tokenization is critical — if we instead kept an 112896 sequence of pixels, using attention on it would be infeasible.
</details>
<div id="cell-14" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>display(iImage(<span class="st">"images/transformer_layer.png"</span>, width<span class="op">=</span><span class="dv">900</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img" width="900"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="the-language-model" class="level3">
<h3 class="anchored" data-anchor-id="the-language-model"><strong>The Language Model</strong></h3>
<p>The other half of LLaVA is of course the language understanding part. For LLaVA, the language model component is a decoder-only Transformer, whose job is to turn embeddings into words.</p>
<hr>
<p>LLaVA relies on a LLaMA model for its language component. In the case of LLaVA-tiny, it simply uses LLaMA-Tiny, a smaller version of LLaMA-7B:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Layers</th>
<th>Hidden size</th>
<th># Params</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LLaMA-7B</td>
<td>32</td>
<td>4096</td>
<td>6.7 B</td>
</tr>
<tr class="even">
<td>Tiny LLaMA</td>
<td>22</td>
<td>2048</td>
<td>~1.1 B</td>
</tr>
</tbody>
</table>
<p>So the architecture is the same — just fewer layers, making it small enough to fit and train on a modest GPU.</p>
<section id="how-decoder-inputs-are-created" class="level4">
<h4 class="anchored" data-anchor-id="how-decoder-inputs-are-created"><strong>How Decoder Inputs are Created</strong></h4>
<p>The job of a decoder-based language model is to take a sequence of tokens and turn it into language.</p>
<p>The input might be:</p>
<pre class="text"><code>&lt;image&gt; Describe this galaxy.</code></pre>
<p>This is tokenized, both the image (we’ll dicuss) and the words “Describe this galaxy” by a <strong>tokenizer</strong> into a numerical sequence that the decoder layers can handle.</p>
<p>The job of the tokenizer is to create meaningful embeddings for words. Each word token in the description is embedded into a vector that contains richer embeddings than just the word itself. Tokenizers are trained specifically to do this task.</p>
<p>The embedding dimension (<span class="math inline">\(D\)</span>), 2048, is fixed by the tokenizer and therefore the model. The sequence length (<span class="math inline">\(L\)</span>) depends on how many things there are to tokenize - it depends on the image size (although this is fixed to 336x336), and how many words we gave in our prompt to LLaVA. In practice, LLM’s have some maximum sequence length, <span class="math inline">\(L_{max}\)</span> that they can handle.</p>
</section>
<section id="how-a-decoder-layer-operates" class="level4">
<h4 class="anchored" data-anchor-id="how-a-decoder-layer-operates"><strong>How a Decoder Layer Operates</strong></h4>
<p>A decoder transformer generates words one by one - every new token generated is conditioned on what came before.</p>
<p>The primary computation for a decoder layer is the same as for any other transformer layer, except that the attention portion is different in that <em>masked attention</em> is used instead. This zeros out attention between the tokens and others tokens that are later in the sequence to them.</p>
<p>The function of this is two-fold. - During training, the model can’t “cheat” by allowing tokens to attend to ones that they shouldn’t know exist yet. - During inference, tokens in the past are <strong>fixed</strong> (already generated, or were input) - there’s no value in attending them to future tokens - you can’t do anything to change them now anyway.</p>
<div id="cell-18" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>display(iImage(<span class="st">"images/decoder_layer.png"</span>, width<span class="op">=</span><span class="dv">900</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img" width="900"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="how-the-next-token-is-generated" class="level4">
<h4 class="anchored" data-anchor-id="how-the-next-token-is-generated"><strong>How the Next Token is Generated</strong></h4>
<p>The text decoder of LLaVA-tiny has 22 layers, so the above process happens 22 times, and coming out of the decoder layers is an <span class="math inline">\(L \times 2048\)</span> matrix.</p>
<p>This is turned into a <em>prediction of the next token</em> by: - taking the embedding for the most recent (L-th) word (remember, this has now been contextualized many times by all the words before it) - it’s passed to a linear projecion layer to get a vector of length <code>vocab_size</code> (typically ~32k) - this is typically passed to a softmax function to produce, for all words in the vocabulary, the probability that a given word is the next token - with some temperature allowing for randomness, a token of high probability is chosen.</p>
<p>When this is appended to the sequence, L becomes L + 1, and the whole process of 22 decoder layers starts again with the new sequence. This gradually produces the output that you see when asking LLaVA about an image!</p>
</section>
</section>
<section id="the-fusion-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="the-fusion-mechanism"><strong>The Fusion Mechanism</strong></h3>
<p>It’s finally time to talk about the core “technology” behind a model like LLaVA - the fusion mechanism. Up until now, we have a “regular” ViT that processes an image and a “regular” large language model that makes text. The fusion mechanism is what connects them.</p>
<p>Since LLaVA’s task is to process and understand an image to be able to create text contextualized by it, the job of the fusion mechanism is really to turn the image emedding into something that the language model understands.</p>
<p>In reality, the fusion mechanism is extremely simple — it’s just a linear projection layer that maps the embeddings from the ViT into the same vector space as the language model’s token embeddings. This lets the image patches act like “visual tokens” that the language model can attend to just like words.</p>
<hr>
<p>LLaVA uses a <strong>simple fusion mechanism</strong> to do this: - The final output of the vision encoder is a sequence of 576 visual tokens (one per patch), each a 1024-dimensional vector. - These are passed through a small <strong>projection layer</strong> (usually an MLP) that <strong>maps 1024 → 2048</strong>, matching the LLaMA embedding size. - The projected visual tokens are then <strong>prepended</strong> to the input tokens of the language model, as if they were special “image tokens.”</p>
<p>The LLM then attends to these visual tokens just like text, using self-attention across the combined sequence. This way, the LLM gets the visual tokens in a space that’s already meaningful to it, but also get’s <em>told</em> that they are visual tokens, so that it can understand how to use them during the training phase.</p>
<p>This approach might seem super simple (and it is!) but it’s also very flexible, and surprisingly effective with the right training, as we’ll see.</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>display(iImage(<span class="st">"images/llava_pipeline.png"</span>, width<span class="op">=</span><span class="dv">1000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img" width="1000"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="training-llava" class="level3">
<h3 class="anchored" data-anchor-id="training-llava"><strong>Training LLaVA</strong></h3>
<p>The final thing to discuss is <em>how something like this gets trained</em>. A ViT and a LLaMA already understand their respective datatypes, so training has to make sure to work with, and not against that knowledge.</p>
<p>First, we’ll talk about an important element of how LLaVA was trained that happens, <em>even before the pipeline is trained all together</em> that sets LLaVA up for success.</p>
<hr>
<p><strong>ViT-L/14 Was Trained With Contrastive Learning</strong> LLaVA doesn’t train the vision encoder from scratch — it inherits a ViT model that was already trained using CLIP (Contrastive Language-Image Pretraining), created by OpenAI.</p>
<ul>
<li><p>CLIP was trained on 400 million (image, text) pairs scraped from the internet (web pages with associated images and alt text, captions, etc.).</p></li>
<li><p>The image goes through a ViT (like ViT-L/14).</p></li>
<li><p>The text goes through a Transformer-based text <strong>encoder</strong>. This turns text -&gt; embeddings, but importantly, <em>is not generative</em>.</p></li>
<li><p>Then it’s trained using a contrastive loss, which aims to push the embeddings of an image and the embeddings of its corresponding caption closer in embedding space, and embeddings that don’t correspond further away.</p></li>
</ul>
<p><strong>The result:</strong> ViT-L/14 learns to produce embeddings that are already aligned with natural language text embeddings, so the 576x1024 embeddings coming out of the ViT are already “language-aligned” to some extent — they live in the same conceptual space as captions.</p>
<section id="how-the-fusion-mechanism-is-trained" class="level4">
<h4 class="anchored" data-anchor-id="how-the-fusion-mechanism-is-trained"><strong>How the Fusion Mechanism is Trained</strong></h4>
<p>Because ViT was trained with CLIP to understand images, <em>and</em> to put them in a text-friendly space, it’s frozen during training.</p>
<p>The first phase of LLaVA training teaches the fusion mechanism to align, and the language model to accept, tokens.</p>
<ul>
<li>Dataset: image–caption pairs (COCO, CC3M, etc.)</li>
<li>Input: an image</li>
<li>Target Output: the corresponding caption (as tokens)</li>
<li>Loss: language modeling loss (cross-entropy on the caption tokens)</li>
</ul>
During this training phase, the weights of the fusion mechanism layer are trained (ViT output → LLM input space), and the language model is lightly tuned to learn what to do with image tokens. &gt;
<summary>
“lightly tuned” meaning using a low learning rate, PEFT, or unfreezing only certain layers, depending on the LLaVA version. The idea here is to make use of what the language model already knows as best as possible.
</summary>
<p>This is the training phase where LLaVA learns to “talk about” images at all.</p>
</section>
<section id="how-its-prompt-trained" class="level4">
<h4 class="anchored" data-anchor-id="how-its-prompt-trained"><strong>How It’s Prompt-Trained</strong></h4>
<p>The first phase of training teaches LLaVA to write a caption for a corresponding image. But LLaVA is also able to accept prompt instructions, i.e.&nbsp;you can include with your image “Describe this image” or “What’s interesting about this picture?”</p>
<p>For this to work, the language part of LLaVA needs to be <strong>instruction fine-tuned</strong>. We won’t get into the details here since it’s a bit outside of the scope of how LLaVA works specifically, but this phase of training is just meant to align the language part of the model - <em>both</em> ViT and the fusion mechanism are frozen while the language model learns to follow instructions.</p>
<hr>
<p>And that’s it!! Hopefully, you should now have a concrete understanding of how information flows through a model like LLaVA, and how we train a model to this sort of multi-modal alignment!</p>
</section>
</section>
</section>
<section id="dataset-creation" class="level2">
<h2 class="anchored" data-anchor-id="dataset-creation"><strong>2. Dataset Creation</strong></h2>
<p>In this notebook, we’ll be fine tuning our own LLaVA to understand astronomical images. Since LLaVA is trained on image and image caption pairs, we need a similar data structure to train it with.</p>
<p>High quality image captions that contain real astronomical descriptions are ideal. For the purposes of this tutorial, we’ll be gathering data from <a href="https://huggingface.co/datasets/mwalmsley/gz2/tree/main">Galaxy Zoo 2</a> - a set of galaxy images and their morphological classifications. We’ll build the morphological classifications into captions as best as we can. While not <em>totally</em> ideal for real, quality fine tuning, it should be sufficient to see some basic changes in the model.</p>
<hr>
<section id="the-data-source-galaxyzoo2" class="level4">
<h4 class="anchored" data-anchor-id="the-data-source-galaxyzoo2"><strong>The Data Source</strong>: GalaxyZoo2</h4>
<ul>
<li><strong>Source</strong> Hugging Face dataset <code>mwalmsley/gz2</code> (172 k SDSS galaxy JPGs + volunteer morphology votes).<br>
</li>
<li><strong>Images</strong> JPEG cut-outs around each galaxy.<br>
</li>
<li><strong>Labels converted to text</strong>
<ul>
<li><em>Elliptical / Spiral</em> (+ bar, arm count, bulge, merger/odd)<br>
</li>
<li>Axis-ratio for ellipticals (“round”, “elongated”)<br>
</li>
</ul></li>
<li><strong>Caption length</strong> ≈ 5-20 words typically.</li>
</ul>
</section>
<section id="create-dataset-collector" class="level3">
<h3 class="anchored" data-anchor-id="create-dataset-collector"><strong>Create Dataset Collector</strong></h3>
<p>The below code:</p>
<ol type="1">
<li>downloads <strong>N</strong> examples (set by <code>N</code>),<br>
</li>
<li>rescales to 336 × 336,<br>
</li>
<li>builds captions from the columns of galaxy morphological descriptions</li>
<li>categorizes galaxies by spiral vs elliptical to create a balanced dataset</li>
<li>builds captions using a balanced dataset</li>
<li>writes <code>gz2_llava.jsonl</code> for LLaVA training.</li>
</ol>
<section id="llavas-training-format" class="level4">
<h4 class="anchored" data-anchor-id="llavas-training-format"><strong>LLaVA’s Training Format</strong></h4>
<p>LLaVA expects training data in a <strong>conversation format</strong> that mimics how humans discuss images. Each training example contains:</p>
<ol type="1">
<li><strong>Human question/prompt</strong> about an image</li>
<li><strong>LLM model response</strong> with detailed explanation</li>
<li><strong>Image reference</strong> linking to the visual content</li>
</ol>
<p>In the below code, you can set <code>N</code> - the number of images to be downloaded <em>per categorization</em>.</p>
<div id="cell-27" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ paths</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>root <span class="op">=</span> Path(<span class="st">"gz2_llava_hf"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>imgs <span class="op">=</span> root <span class="op">/</span> <span class="st">"images"</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>root.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>imgs.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ load full dataset first</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading dataset..."</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> load_dataset(<span class="st">"mwalmsley/gz2"</span>, split<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">1000</span> <span class="co"># Max N per class</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ helper for captions</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> describe(r):</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ============ turn labels into words for captions =======================</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    smooth <span class="op">=</span> r[<span class="st">"smooth-or-featured-gz2_smooth_fraction"</span>] <span class="op">&gt;</span> <span class="fl">0.6</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> smooth:</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        out.append(<span class="st">"This is an image of an elliptical galaxy"</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># axis ratio</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> r[<span class="st">"how-rounded-gz2_round_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>            out.append(<span class="st">"nearly round in shape"</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> r[<span class="st">"how-rounded-gz2_in-between_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>            out.append(<span class="st">"moderately elongated in shape"</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> r[<span class="st">"how-rounded-gz2_cigar_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>            out.append(<span class="st">"highly elongated in shape"</span>)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        out.append(<span class="st">"This is an image of a spiral galaxy"</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> r[<span class="st">"bar-gz2_yes_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>            out.append(<span class="st">"with a central bar"</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> r[<span class="st">"spiral-arm-count-gz2_2_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>            out.append(<span class="st">"with two arms"</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> r[<span class="st">"spiral-arm-count-gz2_3_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>            out.append(<span class="st">"with three arms"</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> r[<span class="st">"spiral-arm-count-gz2_more-than-4_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>            out.append(<span class="st">"with many arms"</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># bulge prominence / shape</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> r[<span class="st">"bulge-size-gz2_obvious_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        out.append(<span class="st">"prominent bulge at its center"</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> r[<span class="st">"bulge-shape-gz2_boxy_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.5</span>:</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        out.append(<span class="st">"boxy bulge at its center"</span>)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mergers / oddities</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> r[<span class="st">"something-odd-gz2_yes_fraction"</span>] <span class="op">&gt;</span> <span class="fl">.4</span>:</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        out.append(<span class="st">"disturbed or merging with another galaxy"</span>)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">", "</span>.join(out) <span class="op">+</span> <span class="st">"."</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ categorize by type</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Categorizing galaxies..."</span>)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>spirals <span class="op">=</span> []</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>ellipticals <span class="op">=</span> []</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ex <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm.tqdm(ds, desc<span class="op">=</span><span class="st">"Categorizing"</span>)):</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>    smooth <span class="op">=</span> ex[<span class="st">"smooth-or-featured-gz2_smooth_fraction"</span>] <span class="op">&gt;</span> <span class="fl">0.6</span></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> smooth:</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>        ellipticals.append(i)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>        spirals.append(i)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(spirals)<span class="sc">}</span><span class="ss"> spirals and </span><span class="sc">{</span><span class="bu">len</span>(ellipticals)<span class="sc">}</span><span class="ss"> ellipticals"</span>)</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ balance dataset</span></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Take equal numbers of each</span></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>n_per_class <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(spirals), <span class="bu">len</span>(ellipticals), N)  <span class="co"># Max N per class</span></span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Selecting </span><span class="sc">{</span>n_per_class<span class="sc">}</span><span class="ss"> of each type (total: </span><span class="sc">{</span>n_per_class <span class="op">*</span> <span class="dv">2</span><span class="sc">}</span><span class="ss">)..."</span>)</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Random sample from each</span></span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>selected_spirals <span class="op">=</span> random.sample(spirals, n_per_class)</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>selected_ellipticals <span class="op">=</span> random.sample(ellipticals, n_per_class)</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine and shuffle</span></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>selected_indices <span class="op">=</span> selected_spirals <span class="op">+</span> selected_ellipticals</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>random.shuffle(selected_indices)</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ build records into captions</span></span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>records <span class="op">=</span> []</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>caption_lengths <span class="op">=</span> {<span class="st">"spiral"</span>: [], <span class="st">"elliptical"</span>: []}</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> tqdm.tqdm(selected_indices, desc<span class="op">=</span><span class="st">"Processing"</span>):</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>    ex <span class="op">=</span> ds[i]</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> ex[<span class="st">"image"</span>].convert(<span class="st">"RGB"</span>).resize((<span class="dv">336</span>, <span class="dv">336</span>))</span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>    fname <span class="op">=</span> imgs <span class="op">/</span> <span class="ss">f"</span><span class="sc">{</span>hashlib<span class="sc">.</span>md5(ex[<span class="st">'id_str'</span>].encode())<span class="sc">.</span>hexdigest()<span class="sc">}</span><span class="ss">.jpg"</span></span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>    img.save(fname, <span class="st">"JPEG"</span>, quality<span class="op">=</span><span class="dv">85</span>)</span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a>    caption <span class="op">=</span> describe(pd.Series(ex))</span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>    galaxy_type <span class="op">=</span> <span class="st">"elliptical"</span> <span class="cf">if</span> ex[<span class="st">"smooth-or-featured-gz2_smooth_fraction"</span>] <span class="op">&gt;</span> <span class="fl">0.6</span> <span class="cf">else</span> <span class="st">"spiral"</span></span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>    records.append({<span class="st">"image"</span>: <span class="bu">str</span>(fname), <span class="st">"text"</span>: caption})</span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>    caption_lengths[galaxy_type].append(<span class="bu">len</span>(caption.split(<span class="st">","</span>)))  <span class="co"># Count features</span></span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ statistics to make sure it's balanced enough</span></span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Dataset Statistics ==="</span>)</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total examples: </span><span class="sc">{</span><span class="bu">len</span>(records)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spirals: </span><span class="sc">{</span>n_per_class<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>n_per_class<span class="op">/</span><span class="bu">len</span>(records)<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%)"</span>)</span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ellipticals: </span><span class="sc">{</span>n_per_class<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>n_per_class<span class="op">/</span><span class="bu">len</span>(records)<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%)"</span>)</span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Average features per type:"</span>)</span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spirals: </span><span class="sc">{</span><span class="bu">sum</span>(caption_lengths[<span class="st">'spiral'</span>])<span class="op">/</span><span class="bu">len</span>(caption_lengths[<span class="st">'spiral'</span>])<span class="sc">:.1f}</span><span class="ss"> features"</span>)</span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ellipticals: </span><span class="sc">{</span><span class="bu">sum</span>(caption_lengths[<span class="st">'elliptical'</span>])<span class="op">/</span><span class="bu">len</span>(caption_lengths[<span class="st">'elliptical'</span>])<span class="sc">:.1f}</span><span class="ss"> features"</span>)</span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Show some examples of varying lengths</span></span>
<span id="cb11-101"><a href="#cb11-101" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Example captions by length:"</span>)</span>
<span id="cb11-102"><a href="#cb11-102" aria-hidden="true" tabindex="-1"></a>sorted_records <span class="op">=</span> <span class="bu">sorted</span>(records, key<span class="op">=</span><span class="kw">lambda</span> x: <span class="bu">len</span>(x[<span class="st">'text'</span>]))</span>
<span id="cb11-103"><a href="#cb11-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shortest: </span><span class="sc">{</span>sorted_records[<span class="dv">0</span>][<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-104"><a href="#cb11-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Median: </span><span class="sc">{</span>sorted_records[<span class="bu">len</span>(sorted_records)<span class="op">//</span><span class="dv">2</span>][<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-105"><a href="#cb11-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Longest: </span><span class="sc">{</span>sorted_records[<span class="op">-</span><span class="dv">1</span>][<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-106"><a href="#cb11-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-107"><a href="#cb11-107" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------------ dump jsonl</span></span>
<span id="cb11-108"><a href="#cb11-108" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(root <span class="op">/</span> <span class="st">"gz2_llava.jsonl"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb11-109"><a href="#cb11-109" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> records:</span>
<span id="cb11-110"><a href="#cb11-110" aria-hidden="true" tabindex="-1"></a>        f.write(json.dumps(r) <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb11-111"><a href="#cb11-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-112"><a href="#cb11-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">✓ Wrote </span><span class="sc">{</span><span class="bu">len</span>(records)<span class="sc">}</span><span class="ss"> balanced examples → </span><span class="sc">{</span>root<span class="op">/</span><span class="st">'gz2_llava.jsonl'</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading dataset...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: 
Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.
You are not authenticated with the Hugging Face Hub in this notebook.
If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"56db19fdfab340b3b9bd1de29cd82626","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5a53dad61e1a4b379ba553b47fdca6eb","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ce02a95340ba48ea84dc0892503807e9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"dc6e87fb06ae4acbafc4684dde7697dc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"576e26d5bf82480e820a6b759b192f49","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2742eb485a344ddf8c5403d347dba63e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6fa193e69f4f4435a3aa7419ef5e4666","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"39a022d57dbe4cf1b798dbe3006fc373","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5c5dbf270f024c8fbf0c4a8ae4185929","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Categorizing galaxies...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Categorizing: 100%|██████████| 172377/172377 [04:16&lt;00:00, 672.34it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 56141 spirals and 116236 ellipticals
Selecting 1000 of each type (total: 2000)...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Processing: 100%|██████████| 2000/2000 [00:11&lt;00:00, 166.74it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Dataset Statistics ===
Total examples: 2000
Spirals: 1000 (50.0%)
Ellipticals: 1000 (50.0%)

Average features per type:
Spirals: 2.1 features
Ellipticals: 2.5 features

Example captions by length:
Shortest: This is an image of a spiral galaxy.
Median: This is an image of an elliptical galaxy, moderately elongated in shape.
Longest: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, boxy bulge at its center, disturbed or merging with another galaxy.

✓ Wrote 2000 balanced examples → gz2_llava_hf/gz2_llava.jsonl</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<p>After running this data collection, you should have:</p>
<pre><code>gz2_llava_hf/
├── gz2_llava.jsonl.json          # Dataset with all metadata
├── images/                      # Downloaded astronomical images
│   ├── 00c1591a613183ff21a67f79a29b5940.jpg
│   ├── 00fd8380b58f4c5086f655e646e0d5a0.jpg
│   └── ...</code></pre>
<p>Each entry in <code>gz2_llava.jsonl</code> contains: - <code>image</code>: Path to the astronomical image - <code>text</code>: description of galaxy morphology</p>
</section>
</section>
<section id="inspect-downloaded-data" class="level3">
<h3 class="anchored" data-anchor-id="inspect-downloaded-data"><strong>Inspect Downloaded Data</strong></h3>
<p>Now, let’s look a at a random example in our newly-downloaded dataset.</p>
<p>This cell can be re-run to inspect a new random image.</p>
<div id="cell-30" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json, random</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># path to the JSON-Lines file you just wrote</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>JSONL <span class="op">=</span> <span class="st">"gz2_llava_hf/gz2_llava.jsonl"</span>      <span class="co"># adjust if you used a different folder</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># grab one random record (or change to lines[0] for the first)</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(JSONL) <span class="im">as</span> f:</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    rec <span class="op">=</span> json.loads(random.choice(f.readlines()))</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rec[<span class="st">"text"</span>])          <span class="co"># caption</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>display(Image.<span class="bu">open</span>(rec[<span class="st">"image"</span>]))  <span class="co"># shows the image in a notebook</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="create-the-dataset-class" class="level3">
<h3 class="anchored" data-anchor-id="create-the-dataset-class"><strong>Create the Dataset Class</strong></h3>
<p>The final step is to make sure we have a dataset class set up for use with the model.</p>
<p>We need to convert our conversation data into PyTorch’s training format. This cell creates a Dataset class that loads images, processes text, and tokenizes everything for LLaVA training.</p>
<p>One crucial step that happens here is that <strong>we mask out the image tokens so they are not trained on</strong>. If we don’t do this step, the full “output” that the model will check for correctness on will <em>include</em> the image tokens, which makes no sense!</p>
<div id="cell-32" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GZ2LLaVADataset(Dataset):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, jsonl_path, processor):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(jsonl_path) <span class="im">as</span> f:</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.data <span class="op">=</span> [json.loads(l) <span class="cf">for</span> l <span class="kw">in</span> f]</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.proc <span class="op">=</span> processor</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        item <span class="op">=</span> <span class="va">self</span>.data[idx]</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> Image.<span class="bu">open</span>(item[<span class="st">"image"</span>]).convert(<span class="st">"RGB"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build the full text</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> <span class="st">"USER: &lt;image&gt;</span><span class="ch">\n</span><span class="st">Describe this galaxy.</span><span class="ch">\n</span><span class="st">ASSISTANT: "</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        full_text <span class="op">=</span> prompt <span class="op">+</span> item[<span class="st">"text"</span>]</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process image and text together</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>        enc <span class="op">=</span> <span class="va">self</span>.proc(text<span class="op">=</span>full_text, images<span class="op">=</span>img, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        input_ids <span class="op">=</span> enc[<span class="st">"input_ids"</span>].squeeze()</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>        attention_mask <span class="op">=</span> enc[<span class="st">"attention_mask"</span>].squeeze()</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        pixel_values <span class="op">=</span> enc[<span class="st">"pixel_values"</span>].squeeze()</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># CRITICAL: Find where the actual caption starts</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We need to mask everything BEFORE the caption</span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> input_ids.clone()</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tokenize just the prompt to find its length AFTER image expansion</span></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>        prompt_enc <span class="op">=</span> <span class="va">self</span>.proc(text<span class="op">=</span>prompt, images<span class="op">=</span>img, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>        prompt_length <span class="op">=</span> prompt_enc[<span class="st">"input_ids"</span>].shape[<span class="dv">1</span>]</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mask everything up to the caption</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>        labels[:prompt_length] <span class="op">=</span> <span class="op">-</span><span class="dv">100</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debug to verify</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">&lt;</span> <span class="dv">3</span>:</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Count what we're actually learning</span></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>            unmasked <span class="op">=</span> (labels <span class="op">!=</span> <span class="op">-</span><span class="dv">100</span>).<span class="bu">sum</span>()</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Example </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Total tokens: </span><span class="sc">{</span><span class="bu">len</span>(input_ids)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Caption tokens to learn: </span><span class="sc">{</span>unmasked<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Caption text: </span><span class="sc">{</span>item[<span class="st">'text'</span>][:<span class="dv">50</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>            <span class="st">"input_ids"</span>: input_ids,</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>            <span class="st">"attention_mask"</span>: attention_mask,</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>            <span class="st">"pixel_values"</span>: pixel_values,</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>            <span class="st">"labels"</span>: labels,</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-3-perform-full-fine-tuning-on-llava-tiny" class="level3">
<h3 class="anchored" data-anchor-id="step-3-perform-full-fine-tuning-on-llava-tiny"><strong>Step 3: Perform Full Fine Tuning on LLaVA-Tiny</strong></h3>
<p>This cell loads the TinyLLaVA model and creates helper functions for testing. We’ll use these functions to compare the model’s responses before and after training.</p>
<p>We take tiny LLaVA from this <a href="https://huggingface.co/bczhou/tiny-llava-v1-hf">HuggingFace repository</a>.</p>
<section id="setup-and-prepare-tinyllava-for-tuning" class="level4">
<h4 class="anchored" data-anchor-id="setup-and-prepare-tinyllava-for-tuning"><strong>Setup and Prepare TinyLLaVA for Tuning</strong></h4>
</section>
<section id="llavaforconditionalgeneration.from_pretrained..." class="level4">
<h4 class="anchored" data-anchor-id="llavaforconditionalgeneration.from_pretrained..."><code>LlavaForConditionalGeneration.from_pretrained(...)</code></h4>
<ul>
<li><strong>Weights &amp; config</strong> are pulled from 🤗 Hub repo <strong><code>bczhou/tiny-llava-v1-hf</code></strong>.
<ul>
<li>It combines:
<ul>
<li><strong>Vision</strong>: CLIP ViT-L/14 (~300M parameters)</li>
<li><strong>Projection</strong>: 2-layer MLP (~8M parameters)<br>
</li>
<li><strong>Language</strong>: TinyLlama-1.1B-Chat (~1.1B parameters)</li>
<li><strong>Total</strong>: ~1.4B parameters → ≈ 2.8 GB in fp16, 5.6 GB in fp32</li>
</ul></li>
</ul></li>
<li><code>device_map="auto"</code> = 🤗 Accelerate loads each layer on the first GPU/CPU with room.</li>
<li><code>torch_dtype=torch.float32</code> tells HF to <strong>up-cast</strong> fp16 weights to fp32 when opening—slower, but avoids NaN/overflow on consumer GPUs.</li>
</ul>
<p>We also below will make a function that helps us test our model. We can set certain generation parameters, and do specify some:</p>
<ul>
<li><code>do_sample=False</code>: Deterministic (greedy) decoding - always picks the most likely token</li>
<li><code>num_beams=3</code>: Beam search explores multiple paths to find better sequences</li>
<li><code>repetition_penalty=1.2</code>: Discourages repeating phrases (common in small models)</li>
<li><code>no_repeat_ngram_size=2</code>: Prevents repeating 2-word phrases exactly</li>
<li><code>min_length=10</code>: Forces at least 10 tokens - prevents immediate EOS generation</li>
<li><code>max_new_tokens=30</code>: Limits response length for quick testing (and we don’t need much more anyway)</li>
</ul>
<div id="cell-35" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup_tiny_llava():</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One function to set everything up"""</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Setting up TinyLLaVA..."</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load model and processor</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    model_id <span class="op">=</span> <span class="st">"bczhou/tiny-llava-v1-hf"</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LlavaForConditionalGeneration.from_pretrained(</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        model_id,</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        torch_dtype<span class="op">=</span>torch.float32,  <span class="co"># Use FP32 instead of FP16</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        device_map<span class="op">=</span><span class="st">"auto"</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    processor <span class="op">=</span> AutoProcessor.from_pretrained(model_id)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fix patch_size issue - only a tinyllava thing</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> processor.patch_size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        processor.patch_size <span class="op">=</span> <span class="dv">14</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"✓ Fixed patch_size"</span>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✓ Model and processor ready"</span>)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model, processor</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_model(model, processor, test_image_path):</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simple test function"""</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(test_image_path).convert(<span class="st">'RGB'</span>).resize((<span class="dv">336</span>, <span class="dv">336</span>))</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> processor(</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="st">"USER: &lt;image&gt;</span><span class="ch">\n</span><span class="st">What is this? ASSISTANT:"</span>,</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        images<span class="op">=</span>image,</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    ).to(model.device)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model.generate(</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>inputs,</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>            max_new_tokens<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>            eos_token_id<span class="op">=</span>processor.tokenizer.eos_token_id,</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>            pad_token_id<span class="op">=</span>processor.tokenizer.pad_token_id,</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>            do_sample<span class="op">=</span><span class="va">False</span>,              <span class="co"># greedy</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>            repetition_penalty<span class="op">=</span><span class="fl">1.2</span>,       <span class="co"># avoid loops</span></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>            no_repeat_ngram_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>            num_beams<span class="op">=</span><span class="dv">3</span>,                   <span class="co"># optional beam search</span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>            min_length<span class="op">=</span><span class="dv">10</span>,  <span class="co"># Force at least 10 tokens</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> processor.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response.split(<span class="st">"ASSISTANT:"</span>)[<span class="op">-</span><span class="dv">1</span>].strip()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s quickly take a look at the architecture of TinyLLaVA and make sure that it matches what we expect.</p>
<div id="cell-37" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>model, processor <span class="op">=</span> setup_tiny_llava()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Setting up TinyLLaVA...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: 
Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.
You are not authenticated with the Hugging Face Hub in this notebook.
If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"372730317d394502a21cf84b1d05dd6c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f7bad3bdb10246cca4bc12795aa376fc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9b85e5b6279499a9244d54f97c95f99","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7bdc9f64e02449819525a52a1fa07061","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"426edeb6b04a4c629539df5a8d24a788","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bf1f756b1db14661a2538ea94bc593f8","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"011fa918f5fc4d2d945ea42c06061be4","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"47bb5067def542d5b7a981193a7de7f4","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c34195d2463f425e899c461ef6287e83","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c044cd8117e048a584ffbcaa019d851b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d3a6f08007984b05be1d70e9fdbbeb3e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>✓ Fixed patch_size
✓ Model and processor ready
LlavaForConditionalGeneration(
  (model): LlavaModel(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (multi_modal_projector): LlavaMultiModalProjector(
      (linear_1): Linear(in_features=1024, out_features=2048, bias=True)
      (act): GELUActivation()
      (linear_2): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (language_model): LlamaModel(
      (embed_tokens): Embedding(32064, 2048)
      (layers): ModuleList(
        (0-21): 22 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=256, bias=False)
            (v_proj): Linear(in_features=2048, out_features=256, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=32064, bias=False)
)</code></pre>
</div>
</div>
</section>
<section id="set-training-parameters" class="level4">
<h4 class="anchored" data-anchor-id="set-training-parameters"><strong>Set Training Parameters</strong></h4>
<p>Now, we are ready to set up the loop to perform full fine tuning!</p>
<p>Huggingface makes this super easy by just defining a <code>trainer</code>. But first, we’ll need to define the training arguments. This sets up exactly how the training will proceed, and it’s where we can set hyperparameters, determine what outputs we want to see, where the model will save, all of that!</p>
<div id="cell-39" class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training configuration - full FP32 to avoid all gradient issues</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./tiny-llava-trained"</span>,         <span class="co"># Where to save model checkpoints</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,                        <span class="co"># How many times to go through the dataset</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">1</span>,             <span class="co"># Process 1 example at a time (small for memory)</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1e-5</span>,                        <span class="co"># How fast the model learns (small = careful)</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">1000</span>,                        <span class="co"># Print progress every N training steps</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"no"</span>,                        <span class="co"># Don't save checkpoints (workshop demo only)</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"no"</span>,                        <span class="co"># Don't evaluate during training (keep simple)</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,               <span class="co"># Keep all data columns for LLaVA</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">False</span>,                                <span class="co"># Disable half-precision (avoids gradient issues)</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    bf16<span class="op">=</span><span class="va">False</span>,                                <span class="co"># Disable bfloat16 (avoids gradient issues)</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,                          <span class="co"># Don't send metrics to tracking services</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    dataloader_num_workers<span class="op">=</span><span class="dv">0</span>,                  <span class="co"># Use main thread only (avoids multiprocessing issues)</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    dataloader_pin_memory<span class="op">=</span><span class="va">False</span>                <span class="co"># Disable memory pinning (avoids GPU memory issues)</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="full-fine-tuning-training-loop" class="level4">
<h4 class="anchored" data-anchor-id="full-fine-tuning-training-loop"><strong>Full Fine Tuning Training Loop</strong></h4>
<p>Then, we’re finally ready to set it training.</p>
<p>Below, we’ll set the model and processor up, create the dataset from our formatted JSON, and start training.</p>
<p>This cell will also show us what the model’s response to a given image looked like both before and after training, so we can see if it got any better.</p>
<hr>
<p>One critical thing that we do below is <strong>freeze the vision model weights.</strong></p>
<p>The vision encoder is already excellent at extracting visual features, and training it on our small galaxy dataset would likely only make it worse at general vision tasks. We only need to teach the model how to describe galaxies, <em>not how to see them differently.</em></p>
<div id="cell-41" class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"🚀 Starting TinyLLaVA full fine tuning...."</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">30</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Setup using our setup function</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>model, processor <span class="op">=</span> setup_tiny_llava()</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. freeze the vision‐tower weights</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"vision_tower"</span> <span class="kw">in</span> name:</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. create the dataset from our LLaVA-formatted JSON</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> GZ2LLaVADataset(<span class="st">"gz2_llava_hf/gz2_llava.jsonl"</span>, processor)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span> <span class="op">*</span> <span class="bu">len</span>(dataset)) <span class="co"># set validation size - we'll just do 10%</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> val_size</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    dataset, [train_size, val_size],</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">42</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="co"># ------Test before training -----</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Testing BEFORE training:"</span>)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>sample_item <span class="op">=</span> dataset.data[<span class="dv">0</span>]</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>test_image <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>sample_item[<span class="st">'image'</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>before_response <span class="op">=</span> test_model(model, processor, test_image)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(test_image)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"BEFORE: </span><span class="sc">{</span>before_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------</span></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Set up the trainer with our args</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>    processing_class<span class="op">=</span>processor</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. set the training going</span></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Starting training..."</span>)</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>full_training_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✓ Training completed in </span><span class="sc">{</span>full_training_time<span class="sc">:.1f}</span><span class="ss"> seconds (</span><span class="sc">{</span>full_training_time<span class="op">/</span><span class="dv">60</span><span class="sc">:.1f}</span><span class="ss"> minutes)"</span>)</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a><span class="co">#  ----- Test after training ------</span></span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Testing AFTER training:"</span>)</span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>after_response <span class="op">=</span> test_model(model, processor, test_image)</span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"AFTER: </span><span class="sc">{</span>after_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"TRUE: </span><span class="sc">{</span>sample_item[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>🚀 Starting TinyLLaVA full fine tuning....
==============================
Setting up TinyLLaVA...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4ec8175fc47d4bca969d640f5a6a9535","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c6c17d3694404ad2aba82dab20e8a803","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3e269d329d9c44a19c11faa0b14612ee","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6e220be916634f68ba9531591a5f61ea","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"65240fd58d684692aa5489f0661a8e9c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a291a1a313204747b9c61859c611360a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b3ab9bbed271420eb523e2877dce5667","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2a90f2b2da5b4726b045f71b2de96a4f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"88e0977da52c44e39bd5a737134dfe3d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3bd6e2f0fdf24372a6a2826c51653068","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c03e6f2cda5a42c78665b7e6c3dfcf63","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>✓ Fixed patch_size
✓ Model and processor ready

Testing BEFORE training:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-16-output-15.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>BEFORE: The image is a close-up view of a star in the middle of the night sky. The star appears to be glowing brightly,

Starting training...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.</code></pre>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="5400" max="5400" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [5400/5400 36:55, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Step</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1000</td>
<td>0.195500</td>
</tr>
<tr class="even">
<td>2000</td>
<td>0.138800</td>
</tr>
<tr class="odd">
<td>3000</td>
<td>0.121600</td>
</tr>
<tr class="even">
<td>4000</td>
<td>0.103600</td>
</tr>
<tr class="odd">
<td>5000</td>
<td>0.091000</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Example 1:
  Total tokens: 609
  Caption tokens to learn: 14
  Caption text: This is an image of a spiral galaxy, with two arms...

Example 2:
  Total tokens: 613
  Caption tokens to learn: 18
  Caption text: This is an image of an elliptical galaxy, highly e...

Example 0:
  Total tokens: 618
  Caption tokens to learn: 23
  Caption text: This is an image of an elliptical galaxy, nearly r...

Example 2:
  Total tokens: 613
  Caption tokens to learn: 18
  Caption text: This is an image of an elliptical galaxy, highly e...

Example 1:
  Total tokens: 609
  Caption tokens to learn: 14
  Caption text: This is an image of a spiral galaxy, with two arms...

Example 0:
  Total tokens: 618
  Caption tokens to learn: 23
  Caption text: This is an image of an elliptical galaxy, nearly r...

Example 2:
  Total tokens: 613
  Caption tokens to learn: 18
  Caption text: This is an image of an elliptical galaxy, highly e...

Example 1:
  Total tokens: 609
  Caption tokens to learn: 14
  Caption text: This is an image of a spiral galaxy, with two arms...

Example 0:
  Total tokens: 618
  Caption tokens to learn: 23
  Caption text: This is an image of an elliptical galaxy, nearly r...
✓ Training completed in 2216.3 seconds (36.9 minutes)

Testing AFTER training:
AFTER: an image of an elliptical galaxy, nearly round in shape. prominent bulge at its center. disturbed or merging with another galax
TRUE: This is an image of an elliptical galaxy, nearly round in shape, prominent bulge at its center.</code></pre>
</div>
</div>
<p>Let’s see how it did on a random image (you can re-run this cell to generate a new image)</p>
<div id="cell-43" class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-------------FROM TRAINING-------------------'</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random index from the training subset</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>train_idx <span class="op">=</span> np.random.randint(<span class="bu">len</span>(train_dataset))</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>actual_idx <span class="op">=</span> train_dataset.indices[train_idx]  <span class="co"># Get the actual index in the original dataset</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> dataset.data[actual_idx]  <span class="co"># Use the original dataset</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> sample[<span class="st">"image"</span>]</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>fig1 <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TRUTH:"</span>, sample[<span class="st">"text"</span>])</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PREDICTED:"</span>, test_model(model, processor, img_path))</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">------------FROM VALIDATION------------------'</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random index from the validation subset</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>val_idx <span class="op">=</span> np.random.randint(<span class="bu">len</span>(val_dataset))</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>actual_idx <span class="op">=</span> val_dataset.indices[val_idx]  <span class="co"># Get the actual index in the original dataset</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> dataset.data[actual_idx]  <span class="co"># Use the original dataset</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> sample[<span class="st">"image"</span>]</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>fig2 <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TRUTH:"</span>, sample[<span class="st">"text"</span>])</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PREDICTED:"</span>, test_model(model, processor, img_path))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-------------FROM TRAINING-------------------
TRUTH: This is an image of a spiral galaxy, prominent bulge at its center.
PREDICTED: an image of a spiral galaxy, prominent bulge at its center. disturbed or merging with another Galy. prominentbulgeat

------------FROM VALIDATION------------------
TRUTH: This is an image of an elliptical galaxy, nearly round in shape.
PREDICTED: an image of an elliptical galaxy, nearly round in shape. boxy bulge at its center. disturbed or merging with another</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-17-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-17-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="step-4-perform-fine-tuning-with-lora" class="level3">
<h3 class="anchored" data-anchor-id="step-4-perform-fine-tuning-with-lora"><strong>Step 4: Perform Fine Tuning with LoRA</strong></h3>
<p>Now, we’ll use LoRA (Low-Rank Adaptation) to fine-tune TinyLLaVA more efficiently. LoRA lets us train only a small number of parameters while keeping the base model frozen.</p>
<p>But before we do that, let’s take another look at our model and talk about where exactly LoRA <em>could</em> be applied:</p>
<pre><code>LlavaForConditionalGeneration(
  (model): LlavaModel(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(...)  # ✅ COULD use LoRA (but typically don't)
          (position_embedding): Embedding(...)  
        )
        (pre_layrnorm): LayerNorm(...)  
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(...)  # ✅ COULD use LoRA (but we freeze vision)
                (v_proj): Linear(...)  # ✅ COULD use LoRA (but we freeze vision)
                (q_proj): Linear(...)  # ✅ COULD use LoRA (but we freeze vision)
                (out_proj): Linear(...) # ✅ COULD use LoRA (but we freeze vision)
              )
              (layer_norm1): LayerNorm(...)   
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()  
                (fc1): Linear(...)  # ✅ COULD use LoRA (but we freeze vision)
                (fc2): Linear(...)  # ✅ COULD use LoRA (but we freeze vision)
              )
              (layer_norm2): LayerNorm(...)  
            )
          )
        )
        (post_layernorm): LayerNorm(...)   
      )
    )
    (multi_modal_projector): LlavaMultiModalProjector(
      (linear_1): Linear(...)  # ⭐ WE USE LoRA HERE (mm_projector)
      (act): GELUActivation()  #
      (linear_2): Linear(...)  # ⭐ WE USE LoRA HERE (mm_projector)
    )
    (language_model): LlamaModel(
      (embed_tokens): Embedding(...)  #
      (layers): ModuleList(
        (0-21): 22 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(...)  # ⭐ WE USE LoRA HERE
            (k_proj): Linear(...)  # ⭐ WE USE LoRA HERE
            (v_proj): Linear(...)  # ⭐ WE USE LoRA HERE
            (o_proj): Linear(...)  # ⭐ WE USE LoRA HERE
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(...)  # ⭐ WE USE LoRA HERE
            (up_proj): Linear(...)    # ⭐ WE USE LoRA HERE
            (down_proj): Linear(...)  # ⭐ WE USE LoRA HERE
            (act_fn): SiLU()  #
          )
          (input_layernorm): LlamaRMSNorm(...)  #
          (post_attention_layernorm): LlamaRMSNorm(...)  #
        )
      )
      (norm): LlamaRMSNorm(...)  #
      (rotary_emb): LlamaRotaryEmbedding()  #
    )
  )
  (lm_head): Linear(...)  # ✅ COULD use LoRA (but typically don't)
)</code></pre>
<section id="setting-the-lora-configuration" class="level4">
<h4 class="anchored" data-anchor-id="setting-the-lora-configuration"><strong>Setting the LoRA Configuration</strong></h4>
<p>Here, we set the LoRA config, which will tell the peft library exactly how we want lora applied to our model.</p>
<p>Some key configuration choices made below: - <strong>r=8, lora_alpha=16</strong>: We keep rank fairly small, because LLaVA tiny is quite a small model. - <strong>lora_dropout=0.05</strong>: small dropout prevents overfitting on our limited galaxy descriptions - <strong>target_modules</strong>: We apply LoRA adaptations to 3 different key parts of the model: - Attention layers <strong>(<code>q_proj, k_proj, v_proj, o_proj</code>)</strong>: Help the model attend to relevant image regions - FFN layers <strong>(<code>gate_proj, up_proj, down_proj</code>)</strong>: Transform features for galaxy-specific outputs - Vision-language projector <strong>(<code>mm_projector</code>)</strong>: The crucial bridge between image and text is actually the most important for our task!</p>
<p>Including all these modules ensures the model can both process visual features AND generate appropriate text. As you’ll see below, with low rank, this is still a tiny fraction of our model size.</p>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>lora_cfg <span class="op">=</span> LoraConfig(</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>,<span class="st">"k_proj"</span>,<span class="st">"v_proj"</span>,<span class="st">"o_proj"</span>, <span class="co"># Attention layers</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"gate_proj"</span>, <span class="st">"up_proj"</span>, <span class="st">"down_proj"</span>,  <span class="co"># FFN layers</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"mm_projector"</span>] <span class="co"># fusion layer</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="setting-training-args" class="level4">
<h4 class="anchored" data-anchor-id="setting-training-args"><strong>Setting Training Args</strong></h4>
<p>We use slightly different training arguments for LoRA. Notably, we’re able to increase our batch size to 8, and our learning rate is a bit higher. Because our batch size is larger, we’ll also change <code>logging_steps</code>, as the number of steps is a function of how much data is processed at once.</p>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training configuration - full FP32 to avoid all gradient issues</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./tiny-llava-lora-trained"</span>,    <span class="co"># Where to save model checkpoints</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,                        <span class="co"># How many times to go through the dataset</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">8</span>,             <span class="co"># Can use larger batch size now, less memory needs</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">5e-5</span>,                        <span class="co"># A bit higher than full fine tuning</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">200</span>,                         <span class="co"># Print progress every N training steps</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"no"</span>,                        <span class="co"># Don't save checkpoints (workshop demo only)</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"no"</span>,                        <span class="co"># Don't evaluate during training (keep simple)</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,               <span class="co"># Keep all data columns for LLaVA</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">False</span>,                                <span class="co"># Disable half-precision (avoids gradient issues)</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    bf16<span class="op">=</span><span class="va">False</span>,                                <span class="co"># Disable bfloat16 (avoids gradient issues)</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,                          <span class="co"># Don't send metrics to tracking services</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    dataloader_num_workers<span class="op">=</span><span class="dv">0</span>,                  <span class="co"># Use main thread only (avoids multiprocessing issues)</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    dataloader_pin_memory<span class="op">=</span><span class="va">False</span>                <span class="co"># Disable memory pinning (avoids GPU memory issues)</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="handling-batched-data" class="level4">
<h4 class="anchored" data-anchor-id="handling-batched-data"><strong>Handling Batched Data</strong></h4>
<p>Thanks to LoRA, we’ll save enough memory that we can batch our data instead of giving one example at a time. But vision-language models like LLaVA require special handling when batching data because the default PyTorch collator doesn’t know how to handle our mixed inputs (images + text of varying lengths).</p>
<p>So, if we want to turn our batch size up to 8, we need to define a collator that says how to do that. Below, our custom <code>collate_fn</code> handles how to build batches from our mixed image/text, variable length data.</p>
<div id="cell-50" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> collate_fn(features):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># features is a list of dicts, each with keys:</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   'input_ids', 'attention_mask', 'pixel_values', 'labels'</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1) stack pixel_values (all same shape: 3×336×336)</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    pixel_values <span class="op">=</span> torch.stack([f[<span class="st">"pixel_values"</span>] <span class="cf">for</span> f <span class="kw">in</span> features])</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2) gather input_ids &amp; attention_mask for text, pad them to same length</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    batch_text <span class="op">=</span> {</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"input_ids"</span>:      [f[<span class="st">"input_ids"</span>] <span class="cf">for</span> f <span class="kw">in</span> features],</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"attention_mask"</span>: [f[<span class="st">"attention_mask"</span>] <span class="cf">for</span> f <span class="kw">in</span> features],</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    batch_text <span class="op">=</span> processor.tokenizer.pad(</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        batch_text,</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"longest"</span>,</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3) labels: pad/truncate to same as input_ids</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    if you already have f["labels"], you can pad those directly:</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    batch_labels <span class="op">=</span> processor.tokenizer.pad(</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"input_ids"</span>: [f[<span class="st">"labels"</span>] <span class="cf">for</span> f <span class="kw">in</span> features]},</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"longest"</span>,</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    )[<span class="st">"input_ids"</span>]</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4) assemble final batch</span></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> {</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">"pixel_values"</span>: pixel_values,</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">"input_ids"</span>:    batch_text[<span class="st">"input_ids"</span>],</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">"attention_mask"</span>: batch_text[<span class="st">"attention_mask"</span>],</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">"labels"</span>:       batch_labels,</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="setting-the-training-going" class="level4">
<h4 class="anchored" data-anchor-id="setting-the-training-going"><strong>Setting the Training Going</strong></h4>
<p>We re-use our model setup and a lot of the code above to do our training.</p>
<p>A few things to note - We print out <code>trainable_parameters</code>, this tells us what percentage of the model is actually going to be trained this time. - We still freeze the vision tower to ensure that none of it’s layers that matched our <code>target_modules</code> are actually trained, for the same reasons we discussed above. - As before, we’ll see one example before/after training.</p>
<div id="cell-52" class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 🚀 TinyLLaVA + LoRA fine-tuning on Galaxy Zoo 2</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"🚀 Starting TinyLLaVA LoRA fine-tuning…"</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">30</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> prepare_model_for_kbit_training</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. base weights + processor</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>model, processor <span class="op">=</span> setup_tiny_llava()   <span class="co"># our existing helper</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. attach LoRA adapters (only proj/FFN layers train)</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, lora_cfg)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. freeze **only** the vision‐tower LoRA weights</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"vision_tower"</span> <span class="kw">in</span> name <span class="kw">and</span> <span class="st">"lora_"</span> <span class="kw">in</span> name:</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>model.print_trainable_parameters()</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. make dataset</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> GZ2LLaVADataset(<span class="st">"gz2_llava_hf/gz2_llava.jsonl"</span>, processor)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span> <span class="op">*</span> <span class="bu">len</span>(dataset)) <span class="co"># set validation size - we'll just do 10%</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> val_size</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> torch.utils.data.random_split(</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>    dataset, [train_size, val_size],</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>    generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">42</span>)</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Split: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss"> train, </span><span class="sc">{</span><span class="bu">len</span>(val_dataset)<span class="sc">}</span><span class="ss"> validation"</span>)</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="co"># ------Test before training -----</span></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Testing BEFORE training:"</span>)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>sample_item <span class="op">=</span> dataset.data[<span class="dv">0</span>]</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>test_image <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>sample_item[<span class="st">'image'</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>before_response <span class="op">=</span> test_model(model, processor, test_image)</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(test_image)</span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"BEFORE: </span><span class="sc">{</span>before_response<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------</span></span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. trainer (reduce LR for LoRA)</span></span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>val_dataset,</span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> processor.tokenizer, <span class="co"># Trainer expects “tokenizer”</span></span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>collate_fn,</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Starting training..."</span>)</span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a>lora_training_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"✓ Training completed in </span><span class="sc">{</span>lora_training_time<span class="sc">:.1f}</span><span class="ss"> seconds (</span><span class="sc">{</span>lora_training_time<span class="op">/</span><span class="dv">60</span><span class="sc">:.1f}</span><span class="ss"> minutes)"</span>)</span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. test after</span></span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Testing AFTER training:"</span>)</span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AFTER:"</span>, test_model(model, processor, test_image))</span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. save lightweight adapters (~40 MB)</span></span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="st">"tinyllava_gz2_lora"</span>)</span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adapters saved to tinyllava_gz2_lora/"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>🚀 Starting TinyLLaVA LoRA fine-tuning…
==============================
Setting up TinyLLaVA...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"50d292437d0242a6b97613a2d63347e6","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>✓ Fixed patch_size
✓ Model and processor ready
trainable params: 12,615,680 || all params: 1,425,088,512 || trainable%: 0.8853
Split: 1800 train, 200 validation

Testing BEFORE training:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-21-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipython-input-1885621431.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>BEFORE: The image is a close-up view of a star in the middle of the night sky. The star appears to be glowing brightly,

Starting training...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.</code></pre>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="675" max="675" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [675/675 20:28, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Step</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>200</td>
<td>1.861200</td>
</tr>
<tr class="even">
<td>400</td>
<td>1.457400</td>
</tr>
<tr class="odd">
<td>600</td>
<td>1.410500</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Example 1:
  Total tokens: 609
  Caption tokens to learn: 14
  Caption text: This is an image of a spiral galaxy, with two arms...

Example 2:
  Total tokens: 613
  Caption tokens to learn: 18
  Caption text: This is an image of an elliptical galaxy, highly e...

Example 0:
  Total tokens: 618
  Caption tokens to learn: 23
  Caption text: This is an image of an elliptical galaxy, nearly r...

Example 2:
  Total tokens: 613
  Caption tokens to learn: 18
  Caption text: This is an image of an elliptical galaxy, highly e...

Example 1:
  Total tokens: 609
  Caption tokens to learn: 14
  Caption text: This is an image of a spiral galaxy, with two arms...

Example 0:
  Total tokens: 618
  Caption tokens to learn: 23
  Caption text: This is an image of an elliptical galaxy, nearly r...

Example 2:
  Total tokens: 613
  Caption tokens to learn: 18
  Caption text: This is an image of an elliptical galaxy, highly e...

Example 1:
  Total tokens: 609
  Caption tokens to learn: 14
  Caption text: This is an image of a spiral galaxy, with two arms...

Example 0:
  Total tokens: 618
  Caption tokens to learn: 23
  Caption text: This is an image of an elliptical galaxy, nearly r...
✓ Training completed in 1231.3 seconds (20.5 minutes)

Testing AFTER training:
AFTER: This is an image of an elliptical galaxy, nearly round in shape, prominent bulge at its center, disturbed or merging with
Adapters saved to tinyllava_gz2_lora/</code></pre>
</div>
</div>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (OPTIONAL), a trainer can always be trained further by running this again!</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="co">#trainer.train()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Again, re-run this cell to see performance on a random example.</p>
<div id="cell-55" class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-------------FROM TRAINING-------------------'</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random index from the training subset</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>train_idx <span class="op">=</span> np.random.randint(<span class="bu">len</span>(train_dataset))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>actual_idx <span class="op">=</span> train_dataset.indices[train_idx]  <span class="co"># Get the actual index in the original dataset</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> dataset.data[actual_idx]  <span class="co"># Use the original dataset</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> sample[<span class="st">"image"</span>]</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>fig1 <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TRUTH:"</span>, sample[<span class="st">"text"</span>])</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PREDICTED:"</span>, test_model(model, processor, img_path))</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">------------FROM VALIDATION------------------'</span>)</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random index from the validation subset</span></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>val_idx <span class="op">=</span> np.random.randint(<span class="bu">len</span>(val_dataset))</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>actual_idx <span class="op">=</span> val_dataset.indices[val_idx]  <span class="co"># Get the actual index in the original dataset</span></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> dataset.data[actual_idx]  <span class="co"># Use the original dataset</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> sample[<span class="st">"image"</span>]</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>fig2 <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TRUTH:"</span>, sample[<span class="st">"text"</span>])</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PREDICTED:"</span>, test_model(model, processor, img_path))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-------------FROM TRAINING-------------------
TRUTH: This is an image of a spiral galaxy, prominent bulge at its center.
PREDICTED: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, disturbed or

------------FROM VALIDATION------------------
TRUTH: This is an image of an elliptical galaxy, nearly round in shape.
PREDICTED: This is an image of an elliptical galaxy, moderately elongated in shape, prominent bulge at its center, boxy bul</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-23-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LLaVA_GalaxyZoo_files/figure-html/cell-23-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="what-did-lora-buy-us" class="level4">
<h4 class="anchored" data-anchor-id="what-did-lora-buy-us"><strong>What did LoRA Buy Us?</strong></h4>
<div id="cell-57" class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">⏱️  Training Time Comparison:"</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Full fine-tuning: </span><span class="sc">{</span>full_training_time<span class="sc">:.1f}</span><span class="ss">s"</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   LoRA fine-tuning: </span><span class="sc">{</span>lora_training_time<span class="sc">:.1f}</span><span class="ss">s"</span>)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Speedup: </span><span class="sc">{</span>full_training_time<span class="op">/</span>lora_training_time<span class="sc">:.1f}</span><span class="ss">x faster with LoRA"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
⏱️  Training Time Comparison:
   Full fine-tuning: 2216.3s
   LoRA fine-tuning: 1231.3s
   Speedup: 1.8x faster with LoRA</code></pre>
</div>
</div>
<p>Ok. You might be thinking - “2x speedup? Who cares!” because in this notebook, using LoRA only got us an about that. But remember that we’re working here with a tiny model, and a tiny amount of data. The real benefits of LoRA become apparent when:</p>
<ol type="1">
<li>Working with larger models (7B, 13B parameters) where it can be 10x+ faster</li>
<li>Deploying on limited hardware where memory is constrained (remeber, here, we saw a batch size of 1 on an A100 with full fine tuning, so our “beefy” GPU was already barely able to handle this task).</li>
<li>Iterating quickly on experiments - even 20 minutes vs 40 means is huge when running a model over and over again to make tweaks.</li>
</ol>
</section>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts"><strong>Final Thoughts</strong></h3>
<p>And that’s it! We’ve sucessfully trained a TinyLLaVA <em>both</em> with full fine tuning and using LoRA. And we definitely see learning, in both cases!</p>
<section id="takeaway-1-its-the-human-who-makes-the-decisions" class="level4">
<h4 class="anchored" data-anchor-id="takeaway-1-its-the-human-who-makes-the-decisions"><strong>Takeaway 1: It’s The Human who Makes the Decisions</strong></h4>
<p>Fine-tuning a specialized model is never simply “throw data at the model and hope”.</p>
<ol type="1">
<li><strong>Understanding Data Flow is Critical.</strong></li>
</ol>
<ul>
<li>We trace exactly how our images become tokens (336×336 → 576 patches → 576 tokens) to understand how information flows</li>
<li>Masking required understanding where image tokens end and text begins</li>
<li>Without this knowledge, we’d train on the wrong tokens and get garbage results</li>
</ul>
<ol start="2" type="1">
<li><strong>Architecture Knowledge Matters.</strong></li>
</ol>
<ul>
<li>We froze the vision encoder because it already understands images well</li>
<li>We trained the projection layer because that’s where image→language translation happens</li>
<li>Choosing LoRA target modules required knowing which layers affect generation quality</li>
</ul>
<ol start="3" type="1">
<li><strong>Data Structure Shapes Everything.</strong> We had to carefully structure our data:</li>
</ol>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">"USER: &lt;image&gt;</span><span class="ch">\n</span><span class="co">Describe this galaxy.</span><span class="ch">\n</span><span class="co">ASSISTANT: [actual description]"</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>          ↑                                           ↑</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    Image goes here                          Only train on this part</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model’s behavior completely depends on these formatting decisions.</p>
<ol start="4" type="1">
<li><strong>Debugging Requires Deep Understanding.</strong> It’s really common to get nonsense in your first stab at training a model. Debugging to get <em>good results</em> is often <strong>not</strong> “I have an error”, but rather “this doesn’t work as well as I thought” - and that sort of debugging benefits from a human overseeing the process.</li>
</ol>
</section>
<section id="takeaway-2-what-this-teaches-us-about-foundation-models" class="level4">
<h4 class="anchored" data-anchor-id="takeaway-2-what-this-teaches-us-about-foundation-models"><strong>Takeaway 2: What This Teaches Us About Foundation Models</strong></h4>
<p><strong>1. The Power of Shared Representation Spaces</strong></p>
<p>What we’ve done with LLaVA reveals a profound principle: <strong>different types of information can be projected into a shared “thinking space.”</strong></p>
<p>This simple projection is surprisingly deep:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>image_features (<span class="dv">1024</span><span class="er">d</span>) → projection layer → language space (<span class="dv">2048</span><span class="er">d</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>While it’s simple in structure, it works because both vision and language models learned rich, compositional representations, so the projection layer just needs to learn to translate between these representation spaces. Once in language space, images become “just another type of token”.</p>
<p><strong>2. Fusion is a General Principle</strong></p>
<p>The fusion mechanism we studied isn’t limited to images + text, and could be applied to:</p>
<ul>
<li>Audio: Whisper embeddings → projection → LLM space</li>
<li>Video: Frame embeddings + temporal encoding → projection → LLM space</li>
<li>Generalized scientific data: Spectra/time series → specialized encoder → projection → LLM space</li>
</ul>
<p>The pattern is universal, and there’s no reason multiple of these can’t happen at once, i.e - image → image projection → LLM space image tokens, - audio → audio projection → LLM space audio tokens, - LLM space image tokens + LLM space audio tokens + text tokens → LLM</p>
<p><strong>3. Toward Truly Universal Models</strong> These architectures get us closer to a future where:</p>
<ul>
<li><strong>Universal tokenization</strong>: All modalities become standardized tokens, and any decoding task can accept those tokens</li>
<li><strong>Shared architectures</strong>: One model type processes everything</li>
<li><strong>Emergent translation</strong>: Models learn to translate between any modalities they’ve seen</li>
</ul>
<p>We’re not fully there yet. But for now, thoughtful engineering of fusion mechanisms - like we did today - remains essential.</p>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/apetulante\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>